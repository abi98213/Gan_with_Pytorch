{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import imageio\n",
    "import numpy as np\n",
    "import itertools\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os,time\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "from torch.autograd import Variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0002\n",
    "img_size = 32\n",
    "batch_size = 512\n",
    "num_epochs = 40\n",
    "save_dir = '/home/abdullah/Documents/ai/models/Own implementation of Gan/Conditional Gan/Model results'\n",
    "G_input_dim = 100\n",
    "G_output_dim = 1\n",
    "D_input_dim = 1\n",
    "D_output_dim = 1\n",
    "label_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_var(x):\n",
    "    x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "def denorm(x):\n",
    "    out = (x+1)/2\n",
    "    return out.clamp(0,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(d_losses, g_losses, num_epoch, save=True ,show=False):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0, num_epochs)\n",
    "    ax.set_ylim(0, max(np.max(g_losses), np.max(d_losses))*1.1)\n",
    "    plt.xlabel('Epoch {0}'.format(num_epoch + 1))\n",
    "    plt.ylabel('Loss values')\n",
    "    plt.plot(d_losses, label='Discriminator')\n",
    "    plt.plot(g_losses, label='Generator')\n",
    "    plt.legend()\n",
    "\n",
    "    # save figure\n",
    "    if save:\n",
    "        save_fn = save_dir + 'MNIST_cDCGAN_losses_epoch_{:d}'.format(num_epoch + 1) + '.png'\n",
    "        plt.savefig(save_fn)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "def plot_result(generator, noise, label, num_epoch, save=True, show=False, fig_size=(5, 5)):\n",
    "    generator.eval()\n",
    "\n",
    "    noise = Variable(noise.cuda())\n",
    "    label = Variable(label.cuda())\n",
    "    gen_image = generator(noise, label)\n",
    "    gen_image = denorm(gen_image)\n",
    "\n",
    "    generator.train()\n",
    "\n",
    "    n_rows = np.sqrt(noise.size()[0]).astype(np.int32)\n",
    "    n_cols = np.sqrt(noise.size()[0]).astype(np.int32)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=fig_size)\n",
    "    for ax, img in zip(axes.flatten(), gen_image):\n",
    "        ax.axis('off')\n",
    "        ax.set_adjustable('box-forced')\n",
    "        ax.imshow(img.cpu().data.view(img_size, img_size).numpy(), cmap='gray', aspect='equal')\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    title = 'Epoch {0}'.format(num_epoch+1)\n",
    "    fig.text(0.5, 0.04, title, ha='center')\n",
    "\n",
    "    # save figure\n",
    "    if save:\n",
    "        save_fn = save_dir + 'MNIST_cDCGAN_epoch_{:d}'.format(num_epoch+1) + '.png'\n",
    "        plt.savefig(save_fn)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdullah/miniconda3/lib/python3.6/site-packages/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Scale(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = (0.5,0.5,.5), std = (.5,.5,.5))\n",
    "])\n",
    "\n",
    "\n",
    "mnist_data = datasets.MNIST(root=\"/home/abdullah/Documents/ai/models/Own implementation of Gan/Untitled Folder/DCGAN/mnist\",\n",
    "                         train=True,\n",
    "                         transform=transform,\n",
    "                         download=False)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "discriminator(\n",
       "  (conv1_x): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv1_y): Conv2d(10, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (batch_norm_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (batch_norm_3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class generator(nn.Module):\n",
    "    def __init__(self,d=128):\n",
    "        \n",
    "        super(generator,self).__init__()\n",
    "        \n",
    "        self.deconv1_z = nn.ConvTranspose2d( 100 , d*2 , 4 , 1 , 0)\n",
    "        self.deconv1_y = nn.ConvTranspose2d( 10, d*2 , 4 , 1, 0)\n",
    "        \n",
    "        self.batch_norm_1 = nn.BatchNorm2d(d*2)\n",
    "        \n",
    "        self.deconv2 = nn.ConvTranspose2d(d*4 , d*2, 4, 2, 1)\n",
    "        self.batch_norm_2 = nn.BatchNorm2d(d*2)\n",
    "        \n",
    "        self.deconv3 = nn.ConvTranspose2d(d*2 , d , 4 , 2 , 1)\n",
    "        self.batch_norm_3 = nn.BatchNorm2d(d)\n",
    "        \n",
    "        self.deconv4 = nn.ConvTranspose2d(d , 1 , 4, 2 , 1)\n",
    "        \n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean , std)\n",
    "            \n",
    "    def forward(self , input , label):\n",
    "        x = F.relu(self.batch_norm_1(self.deconv1_z(input)))\n",
    "        y = F.relu(self.batch_norm_1(self.deconv1_y(label)))\n",
    "        \n",
    "        x = torch.cat([x,y],1)\n",
    "        \n",
    "        x = F.relu(self.batch_norm_2(self.deconv2(x)))\n",
    "        \n",
    "        x = F.relu(self.batch_norm_3(self.deconv3(x)))\n",
    "        \n",
    "        x = F.tanh(self.deconv4(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "    def __init__(self, d = 128):\n",
    "        \n",
    "        super( discriminator, self).__init__()\n",
    "        print(\"hello\")\n",
    "        self.conv1_x = nn.Conv2d(1 , d//2 , 4, 2, 1)\n",
    "        self.conv1_y = nn.Conv2d(10 , d//2 , 4, 2, 1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d( d, d*2, 4, 2, 1)\n",
    "        self.batch_norm_2 = nn.BatchNorm2d(d*2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d( d*2, d*4, 4 , 2, 1)\n",
    "        self.batch_norm_3 = nn.BatchNorm2d(d*4)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d( d*4, 1, 4, 1, 0)\n",
    "        \n",
    "    def weight_init(self, mean , std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean , std)\n",
    "            \n",
    "    def forward(self, input  , label):\n",
    "        x = F.leaky_relu(self.conv1_x(input),0.2)\n",
    "        y = F.leaky_relu(self.conv1_y(label),0.2)\n",
    "        \n",
    "        x = torch.cat([x,y],1)\n",
    "        \n",
    "        x = F.leaky_relu(self.batch_norm_2(self.conv2(x)),0.2)\n",
    "        \n",
    "        x = F.leaky_relu(self.batch_norm_3(self.conv3(x)),0.2)\n",
    "        \n",
    "        x = F.sigmoid(self.conv4(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        m.bias.data.zero_()       \n",
    "        \n",
    "        \n",
    "generator = generator()\n",
    "discriminator = discriminator()\n",
    "\n",
    "generator.weight_init(mean = 0 , std = 0.02)\n",
    "discriminator.weight_init(mean = 0 , std = 0.02)\n",
    "\n",
    "generator.cuda()\n",
    "discriminator.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_Gen = optim.Adam(generator.parameters(),lr = lr , betas = (.5, .999))\n",
    "opt_Disc = optim.Adam(discriminator.parameters(),lr = lr , betas = (.5, .999))\n",
    "\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdullah/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:83: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/abdullah/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:84: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/abdullah/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:87: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/40], Step [1/118], D_loss: 6.4806, G_loss: 15.2990\n",
      "Epoch [2/40], Step [2/118], D_loss: 6.5426, G_loss: 5.3879\n",
      "Epoch [2/40], Step [3/118], D_loss: 0.4599, G_loss: 1.1136\n",
      "Epoch [2/40], Step [4/118], D_loss: 1.0587, G_loss: 2.1500\n",
      "Epoch [2/40], Step [5/118], D_loss: 0.4367, G_loss: 2.9613\n",
      "Epoch [2/40], Step [6/118], D_loss: 0.3101, G_loss: 3.1542\n",
      "Epoch [2/40], Step [7/118], D_loss: 0.2477, G_loss: 2.8146\n",
      "Epoch [2/40], Step [8/118], D_loss: 0.3358, G_loss: 3.1463\n",
      "Epoch [2/40], Step [9/118], D_loss: 0.3490, G_loss: 3.7504\n",
      "Epoch [2/40], Step [10/118], D_loss: 0.3992, G_loss: 3.1425\n",
      "Epoch [2/40], Step [11/118], D_loss: 0.4559, G_loss: 4.1095\n",
      "Epoch [2/40], Step [12/118], D_loss: 0.4539, G_loss: 3.0859\n",
      "Epoch [2/40], Step [13/118], D_loss: 0.6488, G_loss: 4.7697\n",
      "Epoch [2/40], Step [14/118], D_loss: 0.6319, G_loss: 2.9902\n",
      "Epoch [2/40], Step [15/118], D_loss: 1.1252, G_loss: 4.6672\n",
      "Epoch [2/40], Step [16/118], D_loss: 1.0291, G_loss: 2.5284\n",
      "Epoch [2/40], Step [17/118], D_loss: 1.1667, G_loss: 3.5917\n",
      "Epoch [2/40], Step [18/118], D_loss: 0.7781, G_loss: 2.5104\n",
      "Epoch [2/40], Step [19/118], D_loss: 0.4638, G_loss: 2.2691\n",
      "Epoch [2/40], Step [20/118], D_loss: 0.5605, G_loss: 3.4466\n",
      "Epoch [2/40], Step [21/118], D_loss: 0.3616, G_loss: 3.6633\n",
      "Epoch [2/40], Step [22/118], D_loss: 0.5518, G_loss: 2.4872\n",
      "Epoch [2/40], Step [23/118], D_loss: 1.1357, G_loss: 5.9205\n",
      "Epoch [2/40], Step [24/118], D_loss: 3.0412, G_loss: 2.3616\n",
      "Epoch [2/40], Step [25/118], D_loss: 0.5009, G_loss: 1.3521\n",
      "Epoch [2/40], Step [26/118], D_loss: 1.4732, G_loss: 3.1261\n",
      "Epoch [2/40], Step [27/118], D_loss: 0.7611, G_loss: 2.8450\n",
      "Epoch [2/40], Step [28/118], D_loss: 0.9225, G_loss: 1.5270\n",
      "Epoch [2/40], Step [29/118], D_loss: 1.1574, G_loss: 1.9153\n",
      "Epoch [2/40], Step [30/118], D_loss: 0.7875, G_loss: 2.0129\n",
      "Epoch [2/40], Step [31/118], D_loss: 0.7333, G_loss: 1.7767\n",
      "Epoch [2/40], Step [32/118], D_loss: 0.9213, G_loss: 2.1548\n",
      "Epoch [2/40], Step [33/118], D_loss: 0.6852, G_loss: 2.0433\n",
      "Epoch [2/40], Step [34/118], D_loss: 0.8694, G_loss: 1.3179\n",
      "Epoch [2/40], Step [35/118], D_loss: 1.5769, G_loss: 3.8211\n",
      "Epoch [2/40], Step [36/118], D_loss: 2.4194, G_loss: 2.2787\n",
      "Epoch [2/40], Step [37/118], D_loss: 1.0807, G_loss: 0.7456\n",
      "Epoch [2/40], Step [38/118], D_loss: 1.1030, G_loss: 1.6624\n",
      "Epoch [2/40], Step [39/118], D_loss: 0.6739, G_loss: 2.0891\n",
      "Epoch [2/40], Step [40/118], D_loss: 0.7244, G_loss: 1.6706\n",
      "Epoch [2/40], Step [41/118], D_loss: 0.8654, G_loss: 1.5979\n",
      "Epoch [2/40], Step [42/118], D_loss: 0.5313, G_loss: 2.0936\n",
      "Epoch [2/40], Step [43/118], D_loss: 0.7310, G_loss: 2.1058\n",
      "Epoch [2/40], Step [44/118], D_loss: 0.6338, G_loss: 2.0338\n",
      "Epoch [2/40], Step [45/118], D_loss: 0.6561, G_loss: 3.6550\n",
      "Epoch [2/40], Step [46/118], D_loss: 1.1397, G_loss: 0.8792\n",
      "Epoch [2/40], Step [47/118], D_loss: 1.5521, G_loss: 4.3578\n",
      "Epoch [2/40], Step [48/118], D_loss: 1.6381, G_loss: 2.4143\n",
      "Epoch [2/40], Step [49/118], D_loss: 0.7087, G_loss: 0.9667\n",
      "Epoch [2/40], Step [50/118], D_loss: 0.9233, G_loss: 2.2697\n",
      "Epoch [2/40], Step [51/118], D_loss: 0.3722, G_loss: 2.8501\n",
      "Epoch [2/40], Step [52/118], D_loss: 0.5143, G_loss: 1.9990\n",
      "Epoch [2/40], Step [53/118], D_loss: 0.6116, G_loss: 2.2469\n",
      "Epoch [2/40], Step [54/118], D_loss: 0.7096, G_loss: 2.2448\n",
      "Epoch [2/40], Step [55/118], D_loss: 0.7080, G_loss: 2.2750\n",
      "Epoch [2/40], Step [56/118], D_loss: 0.7819, G_loss: 1.9751\n",
      "Epoch [2/40], Step [57/118], D_loss: 0.9782, G_loss: 2.7554\n",
      "Epoch [2/40], Step [58/118], D_loss: 0.9423, G_loss: 1.2579\n",
      "Epoch [2/40], Step [59/118], D_loss: 1.2192, G_loss: 3.3522\n",
      "Epoch [2/40], Step [60/118], D_loss: 1.3699, G_loss: 1.6351\n",
      "Epoch [2/40], Step [61/118], D_loss: 0.9107, G_loss: 1.3013\n",
      "Epoch [2/40], Step [62/118], D_loss: 1.2341, G_loss: 2.7611\n",
      "Epoch [2/40], Step [63/118], D_loss: 1.4797, G_loss: 1.2374\n",
      "Epoch [2/40], Step [64/118], D_loss: 1.3210, G_loss: 1.8123\n",
      "Epoch [2/40], Step [65/118], D_loss: 1.3400, G_loss: 1.3924\n",
      "Epoch [2/40], Step [66/118], D_loss: 1.1513, G_loss: 1.2735\n",
      "Epoch [2/40], Step [67/118], D_loss: 1.0195, G_loss: 1.2764\n",
      "Epoch [2/40], Step [68/118], D_loss: 1.1494, G_loss: 1.8605\n",
      "Epoch [2/40], Step [69/118], D_loss: 1.1265, G_loss: 1.0832\n",
      "Epoch [2/40], Step [70/118], D_loss: 0.9833, G_loss: 2.1287\n",
      "Epoch [2/40], Step [71/118], D_loss: 0.7782, G_loss: 1.6393\n",
      "Epoch [2/40], Step [72/118], D_loss: 0.8891, G_loss: 1.5642\n",
      "Epoch [2/40], Step [73/118], D_loss: 0.8312, G_loss: 1.9829\n",
      "Epoch [2/40], Step [74/118], D_loss: 0.9302, G_loss: 1.3473\n",
      "Epoch [2/40], Step [75/118], D_loss: 1.0759, G_loss: 2.3919\n",
      "Epoch [2/40], Step [76/118], D_loss: 1.2287, G_loss: 0.8269\n",
      "Epoch [2/40], Step [77/118], D_loss: 0.9380, G_loss: 2.5346\n",
      "Epoch [2/40], Step [78/118], D_loss: 0.6533, G_loss: 2.0831\n",
      "Epoch [2/40], Step [79/118], D_loss: 0.5040, G_loss: 1.7389\n",
      "Epoch [2/40], Step [80/118], D_loss: 0.7486, G_loss: 2.9744\n",
      "Epoch [2/40], Step [81/118], D_loss: 0.6973, G_loss: 1.9443\n",
      "Epoch [2/40], Step [82/118], D_loss: 0.9197, G_loss: 2.6096\n",
      "Epoch [2/40], Step [83/118], D_loss: 1.1023, G_loss: 1.0386\n",
      "Epoch [2/40], Step [84/118], D_loss: 1.8957, G_loss: 4.0488\n",
      "Epoch [2/40], Step [85/118], D_loss: 2.2054, G_loss: 1.2150\n",
      "Epoch [2/40], Step [86/118], D_loss: 1.2399, G_loss: 1.0362\n",
      "Epoch [2/40], Step [87/118], D_loss: 1.3755, G_loss: 2.1378\n",
      "Epoch [2/40], Step [88/118], D_loss: 0.9690, G_loss: 1.6588\n",
      "Epoch [2/40], Step [89/118], D_loss: 0.8387, G_loss: 1.5224\n",
      "Epoch [2/40], Step [90/118], D_loss: 1.0049, G_loss: 1.9700\n",
      "Epoch [2/40], Step [91/118], D_loss: 0.9066, G_loss: 2.0227\n",
      "Epoch [2/40], Step [92/118], D_loss: 1.0186, G_loss: 1.4616\n",
      "Epoch [2/40], Step [93/118], D_loss: 1.3158, G_loss: 1.8734\n",
      "Epoch [2/40], Step [94/118], D_loss: 1.2187, G_loss: 1.1316\n",
      "Epoch [2/40], Step [95/118], D_loss: 1.5507, G_loss: 1.8637\n",
      "Epoch [2/40], Step [96/118], D_loss: 0.9645, G_loss: 1.5692\n",
      "Epoch [2/40], Step [97/118], D_loss: 0.7802, G_loss: 1.8019\n",
      "Epoch [2/40], Step [98/118], D_loss: 0.6164, G_loss: 2.2909\n",
      "Epoch [2/40], Step [99/118], D_loss: 0.7479, G_loss: 1.6715\n",
      "Epoch [2/40], Step [100/118], D_loss: 0.6655, G_loss: 2.4416\n",
      "Epoch [2/40], Step [101/118], D_loss: 0.6493, G_loss: 2.1492\n",
      "Epoch [2/40], Step [102/118], D_loss: 0.8239, G_loss: 2.3720\n",
      "Epoch [2/40], Step [103/118], D_loss: 0.9952, G_loss: 1.2398\n",
      "Epoch [2/40], Step [104/118], D_loss: 1.5802, G_loss: 4.0133\n",
      "Epoch [2/40], Step [105/118], D_loss: 2.1620, G_loss: 1.1723\n",
      "Epoch [2/40], Step [106/118], D_loss: 0.9832, G_loss: 1.3505\n",
      "Epoch [2/40], Step [107/118], D_loss: 1.0082, G_loss: 2.6778\n",
      "Epoch [2/40], Step [108/118], D_loss: 1.0047, G_loss: 1.4955\n",
      "Epoch [2/40], Step [109/118], D_loss: 1.1122, G_loss: 1.8404\n",
      "Epoch [2/40], Step [110/118], D_loss: 1.0106, G_loss: 1.6554\n",
      "Epoch [2/40], Step [111/118], D_loss: 0.9704, G_loss: 1.8114\n",
      "Epoch [2/40], Step [112/118], D_loss: 0.9372, G_loss: 1.3175\n",
      "Epoch [2/40], Step [113/118], D_loss: 1.0099, G_loss: 2.3639\n",
      "Epoch [2/40], Step [114/118], D_loss: 1.0649, G_loss: 0.9421\n",
      "Epoch [2/40], Step [115/118], D_loss: 1.0963, G_loss: 2.8278\n",
      "Epoch [2/40], Step [116/118], D_loss: 1.2099, G_loss: 1.0872\n",
      "Epoch [2/40], Step [117/118], D_loss: 1.0559, G_loss: 1.5144\n",
      "Epoch [2/40], Step [118/118], D_loss: 0.7758, G_loss: 1.8108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdullah/miniconda3/lib/python3.6/site-packages/matplotlib/axes/_base.py:1400: MatplotlibDeprecationWarning: The 'box-forced' keyword argument is deprecated since 2.2.\n",
      "  \" since 2.2.\", cbook.mplDeprecation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/40], Step [1/118], D_loss: 1.0812, G_loss: 2.1360\n",
      "Epoch [3/40], Step [2/118], D_loss: 1.0399, G_loss: 1.3020\n",
      "Epoch [3/40], Step [3/118], D_loss: 0.8930, G_loss: 2.0244\n",
      "Epoch [3/40], Step [4/118], D_loss: 0.9097, G_loss: 1.5236\n",
      "Epoch [3/40], Step [5/118], D_loss: 0.8094, G_loss: 2.4599\n",
      "Epoch [3/40], Step [6/118], D_loss: 0.6947, G_loss: 1.6502\n",
      "Epoch [3/40], Step [7/118], D_loss: 0.8787, G_loss: 2.9340\n",
      "Epoch [3/40], Step [8/118], D_loss: 0.6783, G_loss: 2.0871\n",
      "Epoch [3/40], Step [9/118], D_loss: 0.6500, G_loss: 2.4134\n",
      "Epoch [3/40], Step [10/118], D_loss: 0.7161, G_loss: 2.9565\n",
      "Epoch [3/40], Step [11/118], D_loss: 0.8153, G_loss: 1.6942\n",
      "Epoch [3/40], Step [12/118], D_loss: 1.0128, G_loss: 3.3344\n",
      "Epoch [3/40], Step [13/118], D_loss: 1.1339, G_loss: 1.3245\n",
      "Epoch [3/40], Step [14/118], D_loss: 1.5627, G_loss: 3.2273\n",
      "Epoch [3/40], Step [15/118], D_loss: 1.9570, G_loss: 0.7863\n",
      "Epoch [3/40], Step [16/118], D_loss: 1.6060, G_loss: 2.7112\n",
      "Epoch [3/40], Step [17/118], D_loss: 1.1824, G_loss: 1.6191\n",
      "Epoch [3/40], Step [18/118], D_loss: 1.0546, G_loss: 1.4712\n",
      "Epoch [3/40], Step [19/118], D_loss: 0.8890, G_loss: 2.1564\n",
      "Epoch [3/40], Step [20/118], D_loss: 0.7344, G_loss: 1.7083\n",
      "Epoch [3/40], Step [21/118], D_loss: 0.7445, G_loss: 1.5087\n",
      "Epoch [3/40], Step [22/118], D_loss: 0.8859, G_loss: 2.6101\n",
      "Epoch [3/40], Step [23/118], D_loss: 1.0013, G_loss: 1.4139\n",
      "Epoch [3/40], Step [24/118], D_loss: 0.7722, G_loss: 2.4120\n",
      "Epoch [3/40], Step [25/118], D_loss: 0.8006, G_loss: 1.5277\n",
      "Epoch [3/40], Step [26/118], D_loss: 0.8625, G_loss: 2.8581\n",
      "Epoch [3/40], Step [27/118], D_loss: 0.8929, G_loss: 1.5471\n",
      "Epoch [3/40], Step [28/118], D_loss: 0.9647, G_loss: 3.1168\n",
      "Epoch [3/40], Step [29/118], D_loss: 0.7745, G_loss: 2.0424\n",
      "Epoch [3/40], Step [30/118], D_loss: 0.5732, G_loss: 2.0462\n",
      "Epoch [3/40], Step [31/118], D_loss: 0.4514, G_loss: 3.2229\n",
      "Epoch [3/40], Step [32/118], D_loss: 0.4250, G_loss: 2.6418\n",
      "Epoch [3/40], Step [33/118], D_loss: 0.4582, G_loss: 2.8463\n",
      "Epoch [3/40], Step [34/118], D_loss: 0.4945, G_loss: 2.9387\n",
      "Epoch [3/40], Step [35/118], D_loss: 0.6940, G_loss: 3.1894\n",
      "Epoch [3/40], Step [36/118], D_loss: 0.8083, G_loss: 1.9130\n",
      "Epoch [3/40], Step [37/118], D_loss: 1.1867, G_loss: 4.4595\n",
      "Epoch [3/40], Step [38/118], D_loss: 1.5268, G_loss: 1.2806\n",
      "Epoch [3/40], Step [39/118], D_loss: 1.4343, G_loss: 3.4467\n",
      "Epoch [3/40], Step [40/118], D_loss: 1.2995, G_loss: 1.4246\n",
      "Epoch [3/40], Step [41/118], D_loss: 0.9367, G_loss: 2.2095\n",
      "Epoch [3/40], Step [42/118], D_loss: 0.7236, G_loss: 2.3913\n",
      "Epoch [3/40], Step [43/118], D_loss: 0.7094, G_loss: 1.7798\n",
      "Epoch [3/40], Step [44/118], D_loss: 0.8598, G_loss: 2.4822\n",
      "Epoch [3/40], Step [45/118], D_loss: 0.8238, G_loss: 1.8074\n",
      "Epoch [3/40], Step [46/118], D_loss: 1.0236, G_loss: 2.9695\n",
      "Epoch [3/40], Step [47/118], D_loss: 1.0463, G_loss: 1.3247\n",
      "Epoch [3/40], Step [48/118], D_loss: 0.9320, G_loss: 3.0830\n",
      "Epoch [3/40], Step [49/118], D_loss: 0.8822, G_loss: 1.8293\n",
      "Epoch [3/40], Step [50/118], D_loss: 0.8743, G_loss: 2.0565\n",
      "Epoch [3/40], Step [51/118], D_loss: 0.8129, G_loss: 2.0279\n",
      "Epoch [3/40], Step [52/118], D_loss: 0.7685, G_loss: 1.8174\n",
      "Epoch [3/40], Step [53/118], D_loss: 0.8003, G_loss: 2.3316\n",
      "Epoch [3/40], Step [54/118], D_loss: 0.8559, G_loss: 1.4290\n",
      "Epoch [3/40], Step [55/118], D_loss: 0.9974, G_loss: 3.1676\n",
      "Epoch [3/40], Step [56/118], D_loss: 1.0992, G_loss: 1.4520\n",
      "Epoch [3/40], Step [57/118], D_loss: 0.9960, G_loss: 2.5993\n",
      "Epoch [3/40], Step [58/118], D_loss: 0.9244, G_loss: 1.6807\n",
      "Epoch [3/40], Step [59/118], D_loss: 0.8376, G_loss: 2.0447\n",
      "Epoch [3/40], Step [60/118], D_loss: 0.8018, G_loss: 2.3007\n",
      "Epoch [3/40], Step [61/118], D_loss: 1.0582, G_loss: 1.9382\n",
      "Epoch [3/40], Step [62/118], D_loss: 0.9256, G_loss: 2.4365\n",
      "Epoch [3/40], Step [63/118], D_loss: 0.8460, G_loss: 1.8410\n",
      "Epoch [3/40], Step [64/118], D_loss: 0.4932, G_loss: 2.5628\n",
      "Epoch [3/40], Step [65/118], D_loss: 0.3668, G_loss: 2.8240\n",
      "Epoch [3/40], Step [66/118], D_loss: 0.2849, G_loss: 2.8339\n",
      "Epoch [3/40], Step [67/118], D_loss: 0.2980, G_loss: 2.6600\n",
      "Epoch [3/40], Step [68/118], D_loss: 0.5145, G_loss: 3.5893\n",
      "Epoch [3/40], Step [69/118], D_loss: 0.5617, G_loss: 2.5866\n",
      "Epoch [3/40], Step [70/118], D_loss: 0.8400, G_loss: 3.7086\n",
      "Epoch [3/40], Step [71/118], D_loss: 1.4384, G_loss: 0.9098\n",
      "Epoch [3/40], Step [72/118], D_loss: 1.2875, G_loss: 3.7765\n",
      "Epoch [3/40], Step [73/118], D_loss: 1.1405, G_loss: 1.6735\n",
      "Epoch [3/40], Step [74/118], D_loss: 0.7088, G_loss: 1.9081\n",
      "Epoch [3/40], Step [75/118], D_loss: 0.6418, G_loss: 2.8844\n",
      "Epoch [3/40], Step [76/118], D_loss: 0.7395, G_loss: 1.8029\n",
      "Epoch [3/40], Step [77/118], D_loss: 0.6169, G_loss: 2.6479\n",
      "Epoch [3/40], Step [78/118], D_loss: 0.5112, G_loss: 2.4396\n",
      "Epoch [3/40], Step [79/118], D_loss: 0.4780, G_loss: 2.4130\n",
      "Epoch [3/40], Step [80/118], D_loss: 0.5612, G_loss: 2.2957\n",
      "Epoch [3/40], Step [81/118], D_loss: 0.6048, G_loss: 2.3699\n",
      "Epoch [3/40], Step [82/118], D_loss: 0.6388, G_loss: 2.2808\n",
      "Epoch [3/40], Step [83/118], D_loss: 0.6737, G_loss: 2.0771\n",
      "Epoch [3/40], Step [84/118], D_loss: 0.7364, G_loss: 3.1462\n",
      "Epoch [3/40], Step [85/118], D_loss: 0.6598, G_loss: 1.8491\n",
      "Epoch [3/40], Step [86/118], D_loss: 0.7757, G_loss: 3.4199\n",
      "Epoch [3/40], Step [87/118], D_loss: 0.6182, G_loss: 2.2829\n",
      "Epoch [3/40], Step [88/118], D_loss: 0.4286, G_loss: 2.8017\n",
      "Epoch [3/40], Step [89/118], D_loss: 0.3487, G_loss: 3.6350\n",
      "Epoch [3/40], Step [90/118], D_loss: 0.5283, G_loss: 2.4779\n",
      "Epoch [3/40], Step [91/118], D_loss: 0.5071, G_loss: 3.5589\n",
      "Epoch [3/40], Step [92/118], D_loss: 0.6118, G_loss: 2.6334\n",
      "Epoch [3/40], Step [93/118], D_loss: 0.6437, G_loss: 3.2732\n",
      "Epoch [3/40], Step [94/118], D_loss: 0.6727, G_loss: 2.0529\n",
      "Epoch [3/40], Step [95/118], D_loss: 0.7900, G_loss: 4.9104\n",
      "Epoch [3/40], Step [96/118], D_loss: 1.2814, G_loss: 0.9822\n",
      "Epoch [3/40], Step [97/118], D_loss: 1.5958, G_loss: 4.9515\n",
      "Epoch [3/40], Step [98/118], D_loss: 1.6331, G_loss: 2.3616\n",
      "Epoch [3/40], Step [99/118], D_loss: 0.5786, G_loss: 0.9867\n",
      "Epoch [3/40], Step [100/118], D_loss: 0.9470, G_loss: 3.6445\n",
      "Epoch [3/40], Step [101/118], D_loss: 0.5776, G_loss: 3.1618\n",
      "Epoch [3/40], Step [102/118], D_loss: 0.4965, G_loss: 1.8231\n",
      "Epoch [3/40], Step [103/118], D_loss: 0.5145, G_loss: 2.3727\n",
      "Epoch [3/40], Step [104/118], D_loss: 0.3974, G_loss: 2.9520\n",
      "Epoch [3/40], Step [105/118], D_loss: 0.4636, G_loss: 2.5118\n",
      "Epoch [3/40], Step [106/118], D_loss: 0.4463, G_loss: 2.4105\n",
      "Epoch [3/40], Step [107/118], D_loss: 0.7894, G_loss: 2.9478\n",
      "Epoch [3/40], Step [108/118], D_loss: 0.7722, G_loss: 2.3464\n",
      "Epoch [3/40], Step [109/118], D_loss: 1.0905, G_loss: 2.6117\n",
      "Epoch [3/40], Step [110/118], D_loss: 1.2621, G_loss: 1.8393\n",
      "Epoch [3/40], Step [111/118], D_loss: 0.8931, G_loss: 1.9856\n",
      "Epoch [3/40], Step [112/118], D_loss: 0.8111, G_loss: 2.3054\n",
      "Epoch [3/40], Step [113/118], D_loss: 0.6497, G_loss: 2.6815\n",
      "Epoch [3/40], Step [114/118], D_loss: 0.8048, G_loss: 2.3334\n",
      "Epoch [3/40], Step [115/118], D_loss: 0.9239, G_loss: 2.4559\n",
      "Epoch [3/40], Step [116/118], D_loss: 0.8067, G_loss: 2.1049\n",
      "Epoch [3/40], Step [117/118], D_loss: 0.6277, G_loss: 2.6746\n",
      "Epoch [3/40], Step [118/118], D_loss: 0.9177, G_loss: 1.4144\n",
      "Epoch [4/40], Step [1/118], D_loss: 0.8254, G_loss: 3.0752\n",
      "Epoch [4/40], Step [2/118], D_loss: 0.6214, G_loss: 2.5086\n",
      "Epoch [4/40], Step [3/118], D_loss: 0.5366, G_loss: 1.9505\n",
      "Epoch [4/40], Step [4/118], D_loss: 0.7752, G_loss: 3.1827\n",
      "Epoch [4/40], Step [5/118], D_loss: 0.7892, G_loss: 1.9092\n",
      "Epoch [4/40], Step [6/118], D_loss: 0.6528, G_loss: 1.7293\n",
      "Epoch [4/40], Step [7/118], D_loss: 0.8075, G_loss: 2.6464\n",
      "Epoch [4/40], Step [8/118], D_loss: 0.6806, G_loss: 1.9212\n",
      "Epoch [4/40], Step [9/118], D_loss: 0.7230, G_loss: 2.4377\n",
      "Epoch [4/40], Step [10/118], D_loss: 0.7686, G_loss: 2.2555\n",
      "Epoch [4/40], Step [11/118], D_loss: 0.6796, G_loss: 1.7596\n",
      "Epoch [4/40], Step [12/118], D_loss: 0.7872, G_loss: 2.9618\n",
      "Epoch [4/40], Step [13/118], D_loss: 0.8714, G_loss: 1.8462\n",
      "Epoch [4/40], Step [14/118], D_loss: 0.6326, G_loss: 2.1172\n",
      "Epoch [4/40], Step [15/118], D_loss: 0.7149, G_loss: 2.4039\n",
      "Epoch [4/40], Step [16/118], D_loss: 0.5013, G_loss: 2.5048\n",
      "Epoch [4/40], Step [17/118], D_loss: 0.6521, G_loss: 2.8500\n",
      "Epoch [4/40], Step [18/118], D_loss: 0.7869, G_loss: 1.6781\n",
      "Epoch [4/40], Step [19/118], D_loss: 0.6685, G_loss: 2.9142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/40], Step [20/118], D_loss: 0.4919, G_loss: 2.8008\n",
      "Epoch [4/40], Step [21/118], D_loss: 0.4994, G_loss: 2.3793\n",
      "Epoch [4/40], Step [22/118], D_loss: 0.4563, G_loss: 2.7784\n",
      "Epoch [4/40], Step [23/118], D_loss: 0.4692, G_loss: 3.0958\n",
      "Epoch [4/40], Step [24/118], D_loss: 0.4610, G_loss: 2.6577\n",
      "Epoch [4/40], Step [25/118], D_loss: 0.5647, G_loss: 3.7573\n",
      "Epoch [4/40], Step [26/118], D_loss: 0.4546, G_loss: 2.9501\n",
      "Epoch [4/40], Step [27/118], D_loss: 0.6738, G_loss: 2.2584\n",
      "Epoch [4/40], Step [28/118], D_loss: 0.7051, G_loss: 3.7476\n",
      "Epoch [4/40], Step [29/118], D_loss: 0.7161, G_loss: 2.1877\n",
      "Epoch [4/40], Step [30/118], D_loss: 0.7855, G_loss: 2.6462\n",
      "Epoch [4/40], Step [31/118], D_loss: 0.8856, G_loss: 2.6869\n",
      "Epoch [4/40], Step [32/118], D_loss: 0.8714, G_loss: 1.9846\n",
      "Epoch [4/40], Step [33/118], D_loss: 0.7836, G_loss: 2.3107\n",
      "Epoch [4/40], Step [34/118], D_loss: 0.7660, G_loss: 2.5844\n",
      "Epoch [4/40], Step [35/118], D_loss: 0.7491, G_loss: 2.4305\n",
      "Epoch [4/40], Step [36/118], D_loss: 0.7002, G_loss: 1.9546\n",
      "Epoch [4/40], Step [37/118], D_loss: 0.9857, G_loss: 2.8078\n",
      "Epoch [4/40], Step [38/118], D_loss: 0.7983, G_loss: 1.9281\n",
      "Epoch [4/40], Step [39/118], D_loss: 0.8971, G_loss: 2.7323\n",
      "Epoch [4/40], Step [40/118], D_loss: 0.6537, G_loss: 2.3143\n",
      "Epoch [4/40], Step [41/118], D_loss: 0.6993, G_loss: 2.1345\n",
      "Epoch [4/40], Step [42/118], D_loss: 0.6041, G_loss: 3.1385\n",
      "Epoch [4/40], Step [43/118], D_loss: 0.7080, G_loss: 2.3342\n",
      "Epoch [4/40], Step [44/118], D_loss: 0.6350, G_loss: 3.1038\n",
      "Epoch [4/40], Step [45/118], D_loss: 0.5871, G_loss: 2.1140\n",
      "Epoch [4/40], Step [46/118], D_loss: 0.5924, G_loss: 2.6205\n",
      "Epoch [4/40], Step [47/118], D_loss: 0.5611, G_loss: 2.6982\n",
      "Epoch [4/40], Step [48/118], D_loss: 0.6358, G_loss: 1.6075\n",
      "Epoch [4/40], Step [49/118], D_loss: 0.7101, G_loss: 3.2020\n",
      "Epoch [4/40], Step [50/118], D_loss: 0.7175, G_loss: 1.8459\n",
      "Epoch [4/40], Step [51/118], D_loss: 0.7018, G_loss: 2.9497\n",
      "Epoch [4/40], Step [52/118], D_loss: 0.8102, G_loss: 1.6012\n",
      "Epoch [4/40], Step [53/118], D_loss: 0.6206, G_loss: 2.8280\n",
      "Epoch [4/40], Step [54/118], D_loss: 0.5320, G_loss: 2.3625\n",
      "Epoch [4/40], Step [55/118], D_loss: 0.6962, G_loss: 1.8863\n",
      "Epoch [4/40], Step [56/118], D_loss: 0.6154, G_loss: 2.7548\n",
      "Epoch [4/40], Step [57/118], D_loss: 0.6841, G_loss: 1.5227\n",
      "Epoch [4/40], Step [58/118], D_loss: 0.6468, G_loss: 3.3208\n",
      "Epoch [4/40], Step [59/118], D_loss: 0.6975, G_loss: 1.5301\n",
      "Epoch [4/40], Step [60/118], D_loss: 0.8899, G_loss: 3.0560\n",
      "Epoch [4/40], Step [61/118], D_loss: 0.8287, G_loss: 1.1425\n",
      "Epoch [4/40], Step [62/118], D_loss: 0.8150, G_loss: 3.4165\n",
      "Epoch [4/40], Step [63/118], D_loss: 0.9231, G_loss: 1.1742\n",
      "Epoch [4/40], Step [64/118], D_loss: 0.9871, G_loss: 3.2041\n",
      "Epoch [4/40], Step [65/118], D_loss: 0.9646, G_loss: 1.3785\n",
      "Epoch [4/40], Step [66/118], D_loss: 0.9552, G_loss: 2.7541\n",
      "Epoch [4/40], Step [67/118], D_loss: 0.7699, G_loss: 1.6236\n",
      "Epoch [4/40], Step [68/118], D_loss: 0.8827, G_loss: 2.2221\n",
      "Epoch [4/40], Step [69/118], D_loss: 0.7525, G_loss: 1.8933\n",
      "Epoch [4/40], Step [70/118], D_loss: 0.8190, G_loss: 1.2675\n",
      "Epoch [4/40], Step [71/118], D_loss: 0.9136, G_loss: 3.0306\n",
      "Epoch [4/40], Step [72/118], D_loss: 0.7694, G_loss: 1.9173\n",
      "Epoch [4/40], Step [73/118], D_loss: 0.5848, G_loss: 1.6705\n",
      "Epoch [4/40], Step [74/118], D_loss: 0.5152, G_loss: 2.6065\n",
      "Epoch [4/40], Step [75/118], D_loss: 0.5321, G_loss: 2.1554\n",
      "Epoch [4/40], Step [76/118], D_loss: 0.5708, G_loss: 2.7288\n",
      "Epoch [4/40], Step [77/118], D_loss: 0.7502, G_loss: 1.6654\n",
      "Epoch [4/40], Step [78/118], D_loss: 0.7482, G_loss: 2.1811\n",
      "Epoch [4/40], Step [79/118], D_loss: 0.8804, G_loss: 1.6599\n",
      "Epoch [4/40], Step [80/118], D_loss: 0.6359, G_loss: 2.2959\n",
      "Epoch [4/40], Step [81/118], D_loss: 0.7451, G_loss: 2.1846\n",
      "Epoch [4/40], Step [82/118], D_loss: 0.8716, G_loss: 1.8753\n",
      "Epoch [4/40], Step [83/118], D_loss: 0.6319, G_loss: 1.7062\n",
      "Epoch [4/40], Step [84/118], D_loss: 0.4771, G_loss: 2.6323\n",
      "Epoch [4/40], Step [85/118], D_loss: 0.6002, G_loss: 2.8711\n",
      "Epoch [4/40], Step [86/118], D_loss: 0.6779, G_loss: 2.1119\n",
      "Epoch [4/40], Step [87/118], D_loss: 0.8431, G_loss: 1.2588\n",
      "Epoch [4/40], Step [88/118], D_loss: 1.1647, G_loss: 4.1378\n",
      "Epoch [4/40], Step [89/118], D_loss: 1.6413, G_loss: 0.8747\n",
      "Epoch [4/40], Step [90/118], D_loss: 1.2150, G_loss: 2.8685\n",
      "Epoch [4/40], Step [91/118], D_loss: 0.7799, G_loss: 1.9903\n",
      "Epoch [4/40], Step [92/118], D_loss: 0.7241, G_loss: 1.7971\n",
      "Epoch [4/40], Step [93/118], D_loss: 0.7896, G_loss: 2.6137\n",
      "Epoch [4/40], Step [94/118], D_loss: 0.7162, G_loss: 1.8086\n",
      "Epoch [4/40], Step [95/118], D_loss: 0.6936, G_loss: 1.7022\n",
      "Epoch [4/40], Step [96/118], D_loss: 0.6822, G_loss: 3.0426\n",
      "Epoch [4/40], Step [97/118], D_loss: 0.8429, G_loss: 1.5082\n",
      "Epoch [4/40], Step [98/118], D_loss: 0.6976, G_loss: 2.0827\n",
      "Epoch [4/40], Step [99/118], D_loss: 0.5289, G_loss: 3.1011\n",
      "Epoch [4/40], Step [100/118], D_loss: 0.7883, G_loss: 1.2922\n",
      "Epoch [4/40], Step [101/118], D_loss: 0.8746, G_loss: 3.3071\n",
      "Epoch [4/40], Step [102/118], D_loss: 0.9844, G_loss: 1.3331\n",
      "Epoch [4/40], Step [103/118], D_loss: 0.9453, G_loss: 2.4778\n",
      "Epoch [4/40], Step [104/118], D_loss: 0.8453, G_loss: 2.1005\n",
      "Epoch [4/40], Step [105/118], D_loss: 0.7921, G_loss: 1.2662\n",
      "Epoch [4/40], Step [106/118], D_loss: 0.7022, G_loss: 2.2382\n",
      "Epoch [4/40], Step [107/118], D_loss: 0.6717, G_loss: 2.2120\n",
      "Epoch [4/40], Step [108/118], D_loss: 0.9054, G_loss: 1.2675\n",
      "Epoch [4/40], Step [109/118], D_loss: 0.6401, G_loss: 2.1880\n",
      "Epoch [4/40], Step [110/118], D_loss: 0.7661, G_loss: 1.9845\n",
      "Epoch [4/40], Step [111/118], D_loss: 0.5094, G_loss: 2.3212\n",
      "Epoch [4/40], Step [112/118], D_loss: 0.4960, G_loss: 2.1810\n",
      "Epoch [4/40], Step [113/118], D_loss: 0.5845, G_loss: 2.2316\n",
      "Epoch [4/40], Step [114/118], D_loss: 0.6082, G_loss: 1.5486\n",
      "Epoch [4/40], Step [115/118], D_loss: 0.5621, G_loss: 3.0239\n",
      "Epoch [4/40], Step [116/118], D_loss: 0.5942, G_loss: 1.8909\n",
      "Epoch [4/40], Step [117/118], D_loss: 0.5263, G_loss: 2.4538\n",
      "Epoch [4/40], Step [118/118], D_loss: 0.4908, G_loss: 2.8874\n",
      "Epoch [5/40], Step [1/118], D_loss: 0.8296, G_loss: 1.1459\n",
      "Epoch [5/40], Step [2/118], D_loss: 0.9454, G_loss: 3.6519\n",
      "Epoch [5/40], Step [3/118], D_loss: 1.3618, G_loss: 0.7933\n",
      "Epoch [5/40], Step [4/118], D_loss: 1.2690, G_loss: 3.3571\n",
      "Epoch [5/40], Step [5/118], D_loss: 1.0156, G_loss: 1.5955\n",
      "Epoch [5/40], Step [6/118], D_loss: 0.6300, G_loss: 1.6708\n",
      "Epoch [5/40], Step [7/118], D_loss: 0.4856, G_loss: 3.2247\n",
      "Epoch [5/40], Step [8/118], D_loss: 0.5584, G_loss: 2.1355\n",
      "Epoch [5/40], Step [9/118], D_loss: 0.5642, G_loss: 2.3093\n",
      "Epoch [5/40], Step [10/118], D_loss: 0.6809, G_loss: 2.5790\n",
      "Epoch [5/40], Step [11/118], D_loss: 0.8424, G_loss: 1.0759\n",
      "Epoch [5/40], Step [12/118], D_loss: 0.9007, G_loss: 3.5480\n",
      "Epoch [5/40], Step [13/118], D_loss: 1.3825, G_loss: 0.8508\n",
      "Epoch [5/40], Step [14/118], D_loss: 1.5620, G_loss: 3.2476\n",
      "Epoch [5/40], Step [15/118], D_loss: 1.2214, G_loss: 1.6085\n",
      "Epoch [5/40], Step [16/118], D_loss: 0.7490, G_loss: 1.1781\n",
      "Epoch [5/40], Step [17/118], D_loss: 0.5545, G_loss: 2.3344\n",
      "Epoch [5/40], Step [18/118], D_loss: 0.4763, G_loss: 2.5495\n",
      "Epoch [5/40], Step [19/118], D_loss: 0.3991, G_loss: 2.2822\n",
      "Epoch [5/40], Step [20/118], D_loss: 0.3405, G_loss: 2.2703\n",
      "Epoch [5/40], Step [21/118], D_loss: 0.5657, G_loss: 2.1624\n",
      "Epoch [5/40], Step [22/118], D_loss: 0.5166, G_loss: 2.0141\n",
      "Epoch [5/40], Step [23/118], D_loss: 0.6474, G_loss: 2.1923\n",
      "Epoch [5/40], Step [24/118], D_loss: 0.6780, G_loss: 1.8462\n",
      "Epoch [5/40], Step [25/118], D_loss: 0.6102, G_loss: 2.0092\n",
      "Epoch [5/40], Step [26/118], D_loss: 0.7542, G_loss: 1.7024\n",
      "Epoch [5/40], Step [27/118], D_loss: 0.6757, G_loss: 2.1075\n",
      "Epoch [5/40], Step [28/118], D_loss: 0.6641, G_loss: 1.7110\n",
      "Epoch [5/40], Step [29/118], D_loss: 0.6703, G_loss: 1.8648\n",
      "Epoch [5/40], Step [30/118], D_loss: 0.5333, G_loss: 1.8497\n",
      "Epoch [5/40], Step [31/118], D_loss: 0.5075, G_loss: 2.9711\n",
      "Epoch [5/40], Step [32/118], D_loss: 0.8933, G_loss: 1.1326\n",
      "Epoch [5/40], Step [33/118], D_loss: 0.8416, G_loss: 3.1565\n",
      "Epoch [5/40], Step [34/118], D_loss: 0.6873, G_loss: 2.0153\n",
      "Epoch [5/40], Step [35/118], D_loss: 0.5098, G_loss: 1.5444\n",
      "Epoch [5/40], Step [36/118], D_loss: 0.6060, G_loss: 2.6780\n",
      "Epoch [5/40], Step [37/118], D_loss: 0.6772, G_loss: 2.0109\n",
      "Epoch [5/40], Step [38/118], D_loss: 0.7893, G_loss: 1.5596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/40], Step [39/118], D_loss: 0.6142, G_loss: 2.2689\n",
      "Epoch [5/40], Step [40/118], D_loss: 0.5589, G_loss: 2.3514\n",
      "Epoch [5/40], Step [41/118], D_loss: 0.6607, G_loss: 1.9384\n",
      "Epoch [5/40], Step [42/118], D_loss: 0.8067, G_loss: 0.9593\n",
      "Epoch [5/40], Step [43/118], D_loss: 1.1687, G_loss: 3.8632\n",
      "Epoch [5/40], Step [44/118], D_loss: 1.5958, G_loss: 1.1232\n",
      "Epoch [5/40], Step [45/118], D_loss: 0.8670, G_loss: 2.3666\n",
      "Epoch [5/40], Step [46/118], D_loss: 0.6953, G_loss: 1.9846\n",
      "Epoch [5/40], Step [47/118], D_loss: 0.6690, G_loss: 1.6608\n",
      "Epoch [5/40], Step [48/118], D_loss: 0.7023, G_loss: 1.5265\n",
      "Epoch [5/40], Step [49/118], D_loss: 0.7490, G_loss: 2.4390\n",
      "Epoch [5/40], Step [50/118], D_loss: 0.9237, G_loss: 1.1107\n",
      "Epoch [5/40], Step [51/118], D_loss: 1.2033, G_loss: 2.9046\n",
      "Epoch [5/40], Step [52/118], D_loss: 1.0058, G_loss: 1.5868\n",
      "Epoch [5/40], Step [53/118], D_loss: 0.6633, G_loss: 1.6392\n",
      "Epoch [5/40], Step [54/118], D_loss: 0.6677, G_loss: 2.2080\n",
      "Epoch [5/40], Step [55/118], D_loss: 0.6758, G_loss: 1.6360\n",
      "Epoch [5/40], Step [56/118], D_loss: 0.7971, G_loss: 1.6268\n",
      "Epoch [5/40], Step [57/118], D_loss: 0.6173, G_loss: 1.7724\n",
      "Epoch [5/40], Step [58/118], D_loss: 0.7127, G_loss: 1.9944\n",
      "Epoch [5/40], Step [59/118], D_loss: 0.8622, G_loss: 1.2956\n",
      "Epoch [5/40], Step [60/118], D_loss: 0.6571, G_loss: 1.8926\n",
      "Epoch [5/40], Step [61/118], D_loss: 0.4971, G_loss: 2.1114\n",
      "Epoch [5/40], Step [62/118], D_loss: 0.8689, G_loss: 2.1576\n",
      "Epoch [5/40], Step [63/118], D_loss: 0.6615, G_loss: 1.7474\n",
      "Epoch [5/40], Step [64/118], D_loss: 0.8369, G_loss: 1.1005\n",
      "Epoch [5/40], Step [65/118], D_loss: 0.8491, G_loss: 3.4588\n",
      "Epoch [5/40], Step [66/118], D_loss: 1.3041, G_loss: 0.7106\n",
      "Epoch [5/40], Step [67/118], D_loss: 0.6383, G_loss: 1.7983\n",
      "Epoch [5/40], Step [68/118], D_loss: 0.7127, G_loss: 3.0136\n",
      "Epoch [5/40], Step [69/118], D_loss: 0.9444, G_loss: 1.3531\n",
      "Epoch [5/40], Step [70/118], D_loss: 0.9612, G_loss: 2.6429\n",
      "Epoch [5/40], Step [71/118], D_loss: 0.7977, G_loss: 1.6222\n",
      "Epoch [5/40], Step [72/118], D_loss: 0.7112, G_loss: 1.5118\n",
      "Epoch [5/40], Step [73/118], D_loss: 0.7200, G_loss: 2.4402\n",
      "Epoch [5/40], Step [74/118], D_loss: 0.7147, G_loss: 1.4100\n",
      "Epoch [5/40], Step [75/118], D_loss: 0.6941, G_loss: 2.3209\n",
      "Epoch [5/40], Step [76/118], D_loss: 0.7472, G_loss: 1.3460\n",
      "Epoch [5/40], Step [77/118], D_loss: 0.9283, G_loss: 3.0345\n",
      "Epoch [5/40], Step [78/118], D_loss: 1.1332, G_loss: 1.1580\n",
      "Epoch [5/40], Step [79/118], D_loss: 0.8269, G_loss: 2.0767\n",
      "Epoch [5/40], Step [80/118], D_loss: 0.4859, G_loss: 2.4454\n",
      "Epoch [5/40], Step [81/118], D_loss: 0.5036, G_loss: 1.7866\n",
      "Epoch [5/40], Step [82/118], D_loss: 0.5699, G_loss: 1.8879\n",
      "Epoch [5/40], Step [83/118], D_loss: 0.7332, G_loss: 1.7287\n",
      "Epoch [5/40], Step [84/118], D_loss: 0.6621, G_loss: 1.7117\n",
      "Epoch [5/40], Step [85/118], D_loss: 0.7164, G_loss: 1.9503\n",
      "Epoch [5/40], Step [86/118], D_loss: 0.6336, G_loss: 1.3410\n",
      "Epoch [5/40], Step [87/118], D_loss: 0.6839, G_loss: 2.8761\n",
      "Epoch [5/40], Step [88/118], D_loss: 0.6103, G_loss: 1.9426\n",
      "Epoch [5/40], Step [89/118], D_loss: 0.6400, G_loss: 1.3974\n",
      "Epoch [5/40], Step [90/118], D_loss: 0.7022, G_loss: 2.5450\n",
      "Epoch [5/40], Step [91/118], D_loss: 0.6389, G_loss: 1.5442\n",
      "Epoch [5/40], Step [92/118], D_loss: 0.5683, G_loss: 1.7942\n",
      "Epoch [5/40], Step [93/118], D_loss: 0.6781, G_loss: 2.8173\n",
      "Epoch [5/40], Step [94/118], D_loss: 0.7898, G_loss: 1.1202\n",
      "Epoch [5/40], Step [95/118], D_loss: 1.0617, G_loss: 3.8101\n",
      "Epoch [5/40], Step [96/118], D_loss: 1.1968, G_loss: 1.2729\n",
      "Epoch [5/40], Step [97/118], D_loss: 0.8045, G_loss: 1.4269\n",
      "Epoch [5/40], Step [98/118], D_loss: 0.8626, G_loss: 2.8606\n",
      "Epoch [5/40], Step [99/118], D_loss: 0.6837, G_loss: 1.9401\n",
      "Epoch [5/40], Step [100/118], D_loss: 0.5774, G_loss: 1.5468\n",
      "Epoch [5/40], Step [101/118], D_loss: 0.7358, G_loss: 2.4123\n",
      "Epoch [5/40], Step [102/118], D_loss: 0.7828, G_loss: 1.3381\n",
      "Epoch [5/40], Step [103/118], D_loss: 0.7603, G_loss: 2.0774\n",
      "Epoch [5/40], Step [104/118], D_loss: 0.5849, G_loss: 1.8525\n",
      "Epoch [5/40], Step [105/118], D_loss: 0.5691, G_loss: 1.4624\n",
      "Epoch [5/40], Step [106/118], D_loss: 0.7585, G_loss: 2.9211\n",
      "Epoch [5/40], Step [107/118], D_loss: 0.8972, G_loss: 1.2351\n",
      "Epoch [5/40], Step [108/118], D_loss: 0.5791, G_loss: 2.0972\n",
      "Epoch [5/40], Step [109/118], D_loss: 0.5442, G_loss: 2.1389\n",
      "Epoch [5/40], Step [110/118], D_loss: 0.5269, G_loss: 1.7930\n",
      "Epoch [5/40], Step [111/118], D_loss: 0.8546, G_loss: 1.8854\n",
      "Epoch [5/40], Step [112/118], D_loss: 0.9465, G_loss: 0.8798\n",
      "Epoch [5/40], Step [113/118], D_loss: 1.2333, G_loss: 3.6640\n",
      "Epoch [5/40], Step [114/118], D_loss: 1.5242, G_loss: 1.3512\n",
      "Epoch [5/40], Step [115/118], D_loss: 0.8074, G_loss: 1.6412\n",
      "Epoch [5/40], Step [116/118], D_loss: 0.6596, G_loss: 2.2458\n",
      "Epoch [5/40], Step [117/118], D_loss: 0.6344, G_loss: 1.5384\n",
      "Epoch [5/40], Step [118/118], D_loss: 0.5150, G_loss: 2.0672\n",
      "Epoch [6/40], Step [1/118], D_loss: 0.5489, G_loss: 1.9925\n",
      "Epoch [6/40], Step [2/118], D_loss: 0.5795, G_loss: 2.0221\n",
      "Epoch [6/40], Step [3/118], D_loss: 0.6613, G_loss: 1.7078\n",
      "Epoch [6/40], Step [4/118], D_loss: 0.7331, G_loss: 1.4423\n",
      "Epoch [6/40], Step [5/118], D_loss: 0.8034, G_loss: 1.4275\n",
      "Epoch [6/40], Step [6/118], D_loss: 0.5333, G_loss: 1.9764\n",
      "Epoch [6/40], Step [7/118], D_loss: 0.6200, G_loss: 1.7788\n",
      "Epoch [6/40], Step [8/118], D_loss: 0.7623, G_loss: 1.6564\n",
      "Epoch [6/40], Step [9/118], D_loss: 0.6705, G_loss: 1.8110\n",
      "Epoch [6/40], Step [10/118], D_loss: 0.6869, G_loss: 1.6516\n",
      "Epoch [6/40], Step [11/118], D_loss: 0.5894, G_loss: 1.4279\n",
      "Epoch [6/40], Step [12/118], D_loss: 0.4748, G_loss: 1.8842\n",
      "Epoch [6/40], Step [13/118], D_loss: 0.5995, G_loss: 1.9023\n",
      "Epoch [6/40], Step [14/118], D_loss: 0.5657, G_loss: 1.9105\n",
      "Epoch [6/40], Step [15/118], D_loss: 0.6705, G_loss: 1.4495\n",
      "Epoch [6/40], Step [16/118], D_loss: 0.6434, G_loss: 1.3692\n",
      "Epoch [6/40], Step [17/118], D_loss: 0.7723, G_loss: 1.7598\n",
      "Epoch [6/40], Step [18/118], D_loss: 0.5057, G_loss: 2.2144\n",
      "Epoch [6/40], Step [19/118], D_loss: 0.7714, G_loss: 1.3778\n",
      "Epoch [6/40], Step [20/118], D_loss: 0.6478, G_loss: 1.3767\n",
      "Epoch [6/40], Step [21/118], D_loss: 0.7169, G_loss: 1.6935\n",
      "Epoch [6/40], Step [22/118], D_loss: 0.7914, G_loss: 1.5095\n",
      "Epoch [6/40], Step [23/118], D_loss: 0.7492, G_loss: 1.5738\n",
      "Epoch [6/40], Step [24/118], D_loss: 0.5475, G_loss: 1.9189\n",
      "Epoch [6/40], Step [25/118], D_loss: 0.7036, G_loss: 1.4506\n",
      "Epoch [6/40], Step [26/118], D_loss: 0.6424, G_loss: 1.4778\n",
      "Epoch [6/40], Step [27/118], D_loss: 0.4616, G_loss: 1.8749\n",
      "Epoch [6/40], Step [28/118], D_loss: 0.6350, G_loss: 1.9470\n",
      "Epoch [6/40], Step [29/118], D_loss: 0.5119, G_loss: 2.1530\n",
      "Epoch [6/40], Step [30/118], D_loss: 0.5040, G_loss: 1.8116\n",
      "Epoch [6/40], Step [31/118], D_loss: 0.6143, G_loss: 1.4632\n",
      "Epoch [6/40], Step [32/118], D_loss: 0.5605, G_loss: 1.7421\n",
      "Epoch [6/40], Step [33/118], D_loss: 0.6115, G_loss: 1.6733\n",
      "Epoch [6/40], Step [34/118], D_loss: 0.5314, G_loss: 2.0399\n",
      "Epoch [6/40], Step [35/118], D_loss: 0.7414, G_loss: 1.5393\n",
      "Epoch [6/40], Step [36/118], D_loss: 0.4036, G_loss: 1.7754\n",
      "Epoch [6/40], Step [37/118], D_loss: 0.5706, G_loss: 1.7970\n",
      "Epoch [6/40], Step [38/118], D_loss: 0.2893, G_loss: 2.5282\n",
      "Epoch [6/40], Step [39/118], D_loss: 0.5929, G_loss: 1.6089\n",
      "Epoch [6/40], Step [40/118], D_loss: 0.7430, G_loss: 1.6359\n",
      "Epoch [6/40], Step [41/118], D_loss: 0.5630, G_loss: 1.9054\n",
      "Epoch [6/40], Step [42/118], D_loss: 1.0015, G_loss: 1.1605\n",
      "Epoch [6/40], Step [43/118], D_loss: 0.7504, G_loss: 1.4340\n",
      "Epoch [6/40], Step [44/118], D_loss: 0.4324, G_loss: 1.8986\n",
      "Epoch [6/40], Step [45/118], D_loss: 0.5151, G_loss: 1.7286\n",
      "Epoch [6/40], Step [46/118], D_loss: 0.5459, G_loss: 1.8029\n",
      "Epoch [6/40], Step [47/118], D_loss: 0.7035, G_loss: 1.6650\n",
      "Epoch [6/40], Step [48/118], D_loss: 0.6312, G_loss: 1.4719\n",
      "Epoch [6/40], Step [49/118], D_loss: 0.5703, G_loss: 1.5709\n",
      "Epoch [6/40], Step [50/118], D_loss: 0.5830, G_loss: 1.7298\n",
      "Epoch [6/40], Step [51/118], D_loss: 0.6410, G_loss: 1.7996\n",
      "Epoch [6/40], Step [52/118], D_loss: 0.5963, G_loss: 1.6104\n",
      "Epoch [6/40], Step [53/118], D_loss: 0.6409, G_loss: 1.5851\n",
      "Epoch [6/40], Step [54/118], D_loss: 0.4844, G_loss: 2.0013\n",
      "Epoch [6/40], Step [55/118], D_loss: 0.4985, G_loss: 1.9580\n",
      "Epoch [6/40], Step [56/118], D_loss: 0.5481, G_loss: 1.5710\n",
      "Epoch [6/40], Step [57/118], D_loss: 0.6707, G_loss: 1.8306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/40], Step [58/118], D_loss: 0.7901, G_loss: 1.2392\n",
      "Epoch [6/40], Step [59/118], D_loss: 1.1103, G_loss: 2.1958\n",
      "Epoch [6/40], Step [60/118], D_loss: 0.9154, G_loss: 1.6169\n",
      "Epoch [6/40], Step [61/118], D_loss: 0.8134, G_loss: 0.9511\n",
      "Epoch [6/40], Step [62/118], D_loss: 1.0452, G_loss: 2.3581\n",
      "Epoch [6/40], Step [63/118], D_loss: 0.8649, G_loss: 2.2274\n",
      "Epoch [6/40], Step [64/118], D_loss: 0.7748, G_loss: 0.7838\n",
      "Epoch [6/40], Step [65/118], D_loss: 0.7539, G_loss: 1.9596\n",
      "Epoch [6/40], Step [66/118], D_loss: 0.6799, G_loss: 1.8668\n",
      "Epoch [6/40], Step [67/118], D_loss: 0.4404, G_loss: 1.8176\n",
      "Epoch [6/40], Step [68/118], D_loss: 0.5101, G_loss: 1.6441\n",
      "Epoch [6/40], Step [69/118], D_loss: 0.5629, G_loss: 1.8040\n",
      "Epoch [6/40], Step [70/118], D_loss: 0.6065, G_loss: 1.7414\n",
      "Epoch [6/40], Step [71/118], D_loss: 0.5810, G_loss: 1.5539\n",
      "Epoch [6/40], Step [72/118], D_loss: 0.5575, G_loss: 1.4663\n",
      "Epoch [6/40], Step [73/118], D_loss: 0.9830, G_loss: 1.7514\n",
      "Epoch [6/40], Step [74/118], D_loss: 0.5553, G_loss: 2.1555\n",
      "Epoch [6/40], Step [75/118], D_loss: 0.6215, G_loss: 1.5394\n",
      "Epoch [6/40], Step [76/118], D_loss: 0.7162, G_loss: 1.0828\n",
      "Epoch [6/40], Step [77/118], D_loss: 0.7112, G_loss: 2.0800\n",
      "Epoch [6/40], Step [78/118], D_loss: 0.7041, G_loss: 1.6806\n",
      "Epoch [6/40], Step [79/118], D_loss: 0.5462, G_loss: 1.3713\n",
      "Epoch [6/40], Step [80/118], D_loss: 0.5678, G_loss: 1.9795\n",
      "Epoch [6/40], Step [81/118], D_loss: 0.6534, G_loss: 2.1180\n",
      "Epoch [6/40], Step [82/118], D_loss: 0.6638, G_loss: 1.5429\n",
      "Epoch [6/40], Step [83/118], D_loss: 0.7265, G_loss: 1.3429\n",
      "Epoch [6/40], Step [84/118], D_loss: 0.6451, G_loss: 1.4082\n",
      "Epoch [6/40], Step [85/118], D_loss: 0.4194, G_loss: 2.1262\n",
      "Epoch [6/40], Step [86/118], D_loss: 0.4253, G_loss: 2.1999\n",
      "Epoch [6/40], Step [87/118], D_loss: 0.6286, G_loss: 1.3853\n",
      "Epoch [6/40], Step [88/118], D_loss: 0.5831, G_loss: 1.3233\n",
      "Epoch [6/40], Step [89/118], D_loss: 0.7718, G_loss: 2.0993\n",
      "Epoch [6/40], Step [90/118], D_loss: 0.7487, G_loss: 1.5484\n",
      "Epoch [6/40], Step [91/118], D_loss: 0.6450, G_loss: 1.3996\n",
      "Epoch [6/40], Step [92/118], D_loss: 0.6909, G_loss: 1.6249\n",
      "Epoch [6/40], Step [93/118], D_loss: 0.5734, G_loss: 1.4976\n",
      "Epoch [6/40], Step [94/118], D_loss: 0.6130, G_loss: 2.0592\n",
      "Epoch [6/40], Step [95/118], D_loss: 0.7190, G_loss: 1.4656\n",
      "Epoch [6/40], Step [96/118], D_loss: 0.5376, G_loss: 1.8387\n",
      "Epoch [6/40], Step [97/118], D_loss: 0.7116, G_loss: 1.7665\n",
      "Epoch [6/40], Step [98/118], D_loss: 0.6283, G_loss: 1.4930\n",
      "Epoch [6/40], Step [99/118], D_loss: 0.4589, G_loss: 1.7488\n",
      "Epoch [6/40], Step [100/118], D_loss: 0.5061, G_loss: 2.0359\n",
      "Epoch [6/40], Step [101/118], D_loss: 0.4894, G_loss: 1.9129\n",
      "Epoch [6/40], Step [102/118], D_loss: 0.4815, G_loss: 1.6911\n",
      "Epoch [6/40], Step [103/118], D_loss: 0.4736, G_loss: 1.7649\n",
      "Epoch [6/40], Step [104/118], D_loss: 0.5148, G_loss: 1.7117\n",
      "Epoch [6/40], Step [105/118], D_loss: 0.5801, G_loss: 2.2761\n",
      "Epoch [6/40], Step [106/118], D_loss: 0.7466, G_loss: 1.2184\n",
      "Epoch [6/40], Step [107/118], D_loss: 0.5159, G_loss: 1.5593\n",
      "Epoch [6/40], Step [108/118], D_loss: 0.4570, G_loss: 2.4903\n",
      "Epoch [6/40], Step [109/118], D_loss: 0.4750, G_loss: 1.9537\n",
      "Epoch [6/40], Step [110/118], D_loss: 0.8746, G_loss: 1.1869\n",
      "Epoch [6/40], Step [111/118], D_loss: 0.6959, G_loss: 1.4991\n",
      "Epoch [6/40], Step [112/118], D_loss: 0.7948, G_loss: 1.4310\n",
      "Epoch [6/40], Step [113/118], D_loss: 0.6181, G_loss: 2.1544\n",
      "Epoch [6/40], Step [114/118], D_loss: 0.9826, G_loss: 0.7220\n",
      "Epoch [6/40], Step [115/118], D_loss: 0.8182, G_loss: 2.2017\n",
      "Epoch [6/40], Step [116/118], D_loss: 0.6149, G_loss: 2.0268\n",
      "Epoch [6/40], Step [117/118], D_loss: 0.7566, G_loss: 0.8910\n",
      "Epoch [6/40], Step [118/118], D_loss: 0.6953, G_loss: 1.6432\n",
      "Epoch [7/40], Step [1/118], D_loss: 0.6689, G_loss: 2.4839\n",
      "Epoch [7/40], Step [2/118], D_loss: 0.5659, G_loss: 1.9013\n",
      "Epoch [7/40], Step [3/118], D_loss: 0.6131, G_loss: 1.1103\n",
      "Epoch [7/40], Step [4/118], D_loss: 0.8575, G_loss: 2.4345\n",
      "Epoch [7/40], Step [5/118], D_loss: 0.8345, G_loss: 1.3962\n",
      "Epoch [7/40], Step [6/118], D_loss: 0.3636, G_loss: 2.0056\n",
      "Epoch [7/40], Step [7/118], D_loss: 0.5191, G_loss: 2.3277\n",
      "Epoch [7/40], Step [8/118], D_loss: 0.6722, G_loss: 1.6128\n",
      "Epoch [7/40], Step [9/118], D_loss: 0.9328, G_loss: 2.1575\n",
      "Epoch [7/40], Step [10/118], D_loss: 0.8296, G_loss: 1.3013\n",
      "Epoch [7/40], Step [11/118], D_loss: 0.6418, G_loss: 1.4253\n",
      "Epoch [7/40], Step [12/118], D_loss: 0.5949, G_loss: 2.1680\n",
      "Epoch [7/40], Step [13/118], D_loss: 0.6201, G_loss: 1.8817\n",
      "Epoch [7/40], Step [14/118], D_loss: 0.5077, G_loss: 1.4611\n",
      "Epoch [7/40], Step [15/118], D_loss: 0.3702, G_loss: 1.6784\n",
      "Epoch [7/40], Step [16/118], D_loss: 0.7494, G_loss: 2.1420\n",
      "Epoch [7/40], Step [17/118], D_loss: 0.6052, G_loss: 1.6167\n",
      "Epoch [7/40], Step [18/118], D_loss: 0.3557, G_loss: 2.0231\n",
      "Epoch [7/40], Step [19/118], D_loss: 0.5559, G_loss: 2.0752\n",
      "Epoch [7/40], Step [20/118], D_loss: 0.5558, G_loss: 1.5147\n",
      "Epoch [7/40], Step [21/118], D_loss: 0.5075, G_loss: 2.4020\n",
      "Epoch [7/40], Step [22/118], D_loss: 0.4343, G_loss: 2.2566\n",
      "Epoch [7/40], Step [23/118], D_loss: 0.8372, G_loss: 1.0958\n",
      "Epoch [7/40], Step [24/118], D_loss: 1.0675, G_loss: 1.2393\n",
      "Epoch [7/40], Step [25/118], D_loss: 0.6873, G_loss: 2.1116\n",
      "Epoch [7/40], Step [26/118], D_loss: 0.8022, G_loss: 0.9815\n",
      "Epoch [7/40], Step [27/118], D_loss: 0.6237, G_loss: 2.3634\n",
      "Epoch [7/40], Step [28/118], D_loss: 0.9013, G_loss: 0.9523\n",
      "Epoch [7/40], Step [29/118], D_loss: 0.6911, G_loss: 1.5393\n",
      "Epoch [7/40], Step [30/118], D_loss: 0.6319, G_loss: 2.1436\n",
      "Epoch [7/40], Step [31/118], D_loss: 0.6456, G_loss: 1.2972\n",
      "Epoch [7/40], Step [32/118], D_loss: 0.7120, G_loss: 2.2917\n",
      "Epoch [7/40], Step [33/118], D_loss: 0.6965, G_loss: 1.3883\n",
      "Epoch [7/40], Step [34/118], D_loss: 0.3914, G_loss: 1.6163\n",
      "Epoch [7/40], Step [35/118], D_loss: 0.4361, G_loss: 2.2990\n",
      "Epoch [7/40], Step [36/118], D_loss: 1.0222, G_loss: 0.8430\n",
      "Epoch [7/40], Step [37/118], D_loss: 0.9186, G_loss: 2.5462\n",
      "Epoch [7/40], Step [38/118], D_loss: 1.5487, G_loss: 0.4241\n",
      "Epoch [7/40], Step [39/118], D_loss: 1.6723, G_loss: 2.2432\n",
      "Epoch [7/40], Step [40/118], D_loss: 0.9920, G_loss: 1.4582\n",
      "Epoch [7/40], Step [41/118], D_loss: 0.4607, G_loss: 1.5407\n",
      "Epoch [7/40], Step [42/118], D_loss: 0.4683, G_loss: 2.0850\n",
      "Epoch [7/40], Step [43/118], D_loss: 0.7965, G_loss: 1.3481\n",
      "Epoch [7/40], Step [44/118], D_loss: 0.5299, G_loss: 1.7823\n",
      "Epoch [7/40], Step [45/118], D_loss: 0.6267, G_loss: 1.8251\n",
      "Epoch [7/40], Step [46/118], D_loss: 0.7410, G_loss: 1.6082\n",
      "Epoch [7/40], Step [47/118], D_loss: 1.0897, G_loss: 0.5195\n",
      "Epoch [7/40], Step [48/118], D_loss: 1.5641, G_loss: 2.6545\n",
      "Epoch [7/40], Step [49/118], D_loss: 0.7557, G_loss: 2.3732\n",
      "Epoch [7/40], Step [50/118], D_loss: 0.7339, G_loss: 0.7380\n",
      "Epoch [7/40], Step [51/118], D_loss: 0.7536, G_loss: 2.1524\n",
      "Epoch [7/40], Step [52/118], D_loss: 0.7959, G_loss: 1.4985\n",
      "Epoch [7/40], Step [53/118], D_loss: 0.5995, G_loss: 2.1145\n",
      "Epoch [7/40], Step [54/118], D_loss: 0.7053, G_loss: 1.5879\n",
      "Epoch [7/40], Step [55/118], D_loss: 0.5893, G_loss: 1.4873\n",
      "Epoch [7/40], Step [56/118], D_loss: 0.6446, G_loss: 1.2061\n",
      "Epoch [7/40], Step [57/118], D_loss: 0.9960, G_loss: 2.3625\n",
      "Epoch [7/40], Step [58/118], D_loss: 1.0840, G_loss: 1.0405\n",
      "Epoch [7/40], Step [59/118], D_loss: 0.7926, G_loss: 1.4328\n",
      "Epoch [7/40], Step [60/118], D_loss: 0.5928, G_loss: 2.0588\n",
      "Epoch [7/40], Step [61/118], D_loss: 0.7485, G_loss: 1.2730\n",
      "Epoch [7/40], Step [62/118], D_loss: 0.7516, G_loss: 1.3569\n",
      "Epoch [7/40], Step [63/118], D_loss: 1.0861, G_loss: 1.3254\n",
      "Epoch [7/40], Step [64/118], D_loss: 0.7116, G_loss: 1.5972\n",
      "Epoch [7/40], Step [65/118], D_loss: 0.7317, G_loss: 1.2197\n",
      "Epoch [7/40], Step [66/118], D_loss: 0.8875, G_loss: 1.0019\n",
      "Epoch [7/40], Step [67/118], D_loss: 0.7161, G_loss: 1.4066\n",
      "Epoch [7/40], Step [68/118], D_loss: 0.7042, G_loss: 2.1885\n",
      "Epoch [7/40], Step [69/118], D_loss: 0.8529, G_loss: 1.0393\n",
      "Epoch [7/40], Step [70/118], D_loss: 0.6147, G_loss: 2.0878\n",
      "Epoch [7/40], Step [71/118], D_loss: 0.7084, G_loss: 1.3121\n",
      "Epoch [7/40], Step [72/118], D_loss: 0.8698, G_loss: 1.9755\n",
      "Epoch [7/40], Step [73/118], D_loss: 0.8373, G_loss: 1.0080\n",
      "Epoch [7/40], Step [74/118], D_loss: 0.8071, G_loss: 1.5765\n",
      "Epoch [7/40], Step [75/118], D_loss: 0.7098, G_loss: 1.5072\n",
      "Epoch [7/40], Step [76/118], D_loss: 1.0572, G_loss: 1.1806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/40], Step [77/118], D_loss: 0.5776, G_loss: 2.6029\n",
      "Epoch [7/40], Step [78/118], D_loss: 1.7973, G_loss: 0.2526\n",
      "Epoch [7/40], Step [79/118], D_loss: 2.2206, G_loss: 2.1770\n",
      "Epoch [7/40], Step [80/118], D_loss: 0.8155, G_loss: 2.3164\n",
      "Epoch [7/40], Step [81/118], D_loss: 0.8894, G_loss: 0.8591\n",
      "Epoch [7/40], Step [82/118], D_loss: 0.7272, G_loss: 1.5770\n",
      "Epoch [7/40], Step [83/118], D_loss: 0.4155, G_loss: 2.1708\n",
      "Epoch [7/40], Step [84/118], D_loss: 0.6515, G_loss: 1.5666\n",
      "Epoch [7/40], Step [85/118], D_loss: 0.7698, G_loss: 0.9638\n",
      "Epoch [7/40], Step [86/118], D_loss: 0.7981, G_loss: 1.7474\n",
      "Epoch [7/40], Step [87/118], D_loss: 0.9084, G_loss: 1.1274\n",
      "Epoch [7/40], Step [88/118], D_loss: 0.7744, G_loss: 2.0830\n",
      "Epoch [7/40], Step [89/118], D_loss: 0.4775, G_loss: 2.1061\n",
      "Epoch [7/40], Step [90/118], D_loss: 0.7272, G_loss: 1.1153\n",
      "Epoch [7/40], Step [91/118], D_loss: 0.8866, G_loss: 1.9106\n",
      "Epoch [7/40], Step [92/118], D_loss: 0.6789, G_loss: 1.4632\n",
      "Epoch [7/40], Step [93/118], D_loss: 0.6787, G_loss: 0.8544\n",
      "Epoch [7/40], Step [94/118], D_loss: 0.7448, G_loss: 2.1272\n",
      "Epoch [7/40], Step [95/118], D_loss: 0.4099, G_loss: 2.5843\n",
      "Epoch [7/40], Step [96/118], D_loss: 0.5101, G_loss: 1.6336\n",
      "Epoch [7/40], Step [97/118], D_loss: 0.8902, G_loss: 0.9117\n",
      "Epoch [7/40], Step [98/118], D_loss: 1.3340, G_loss: 2.6065\n",
      "Epoch [7/40], Step [99/118], D_loss: 1.4622, G_loss: 0.9297\n",
      "Epoch [7/40], Step [100/118], D_loss: 0.7345, G_loss: 1.0867\n",
      "Epoch [7/40], Step [101/118], D_loss: 1.0332, G_loss: 2.2250\n",
      "Epoch [7/40], Step [102/118], D_loss: 0.9398, G_loss: 1.2928\n",
      "Epoch [7/40], Step [103/118], D_loss: 0.7006, G_loss: 1.2218\n",
      "Epoch [7/40], Step [104/118], D_loss: 0.5527, G_loss: 2.4378\n",
      "Epoch [7/40], Step [105/118], D_loss: 0.9118, G_loss: 1.2150\n",
      "Epoch [7/40], Step [106/118], D_loss: 0.4657, G_loss: 1.3866\n",
      "Epoch [7/40], Step [107/118], D_loss: 0.6707, G_loss: 1.7796\n",
      "Epoch [7/40], Step [108/118], D_loss: 1.0428, G_loss: 1.2226\n",
      "Epoch [7/40], Step [109/118], D_loss: 0.8822, G_loss: 0.8785\n",
      "Epoch [7/40], Step [110/118], D_loss: 0.8167, G_loss: 2.2150\n",
      "Epoch [7/40], Step [111/118], D_loss: 0.6763, G_loss: 1.5905\n",
      "Epoch [7/40], Step [112/118], D_loss: 0.7779, G_loss: 1.0476\n",
      "Epoch [7/40], Step [113/118], D_loss: 0.6266, G_loss: 2.7205\n",
      "Epoch [7/40], Step [114/118], D_loss: 0.7488, G_loss: 1.5107\n",
      "Epoch [7/40], Step [115/118], D_loss: 1.0040, G_loss: 1.1910\n",
      "Epoch [7/40], Step [116/118], D_loss: 0.6928, G_loss: 1.3689\n",
      "Epoch [7/40], Step [117/118], D_loss: 0.6219, G_loss: 2.2839\n",
      "Epoch [7/40], Step [118/118], D_loss: 0.7735, G_loss: 1.2962\n",
      "Epoch [8/40], Step [1/118], D_loss: 0.4275, G_loss: 1.6951\n",
      "Epoch [8/40], Step [2/118], D_loss: 0.6040, G_loss: 1.6273\n",
      "Epoch [8/40], Step [3/118], D_loss: 0.8470, G_loss: 2.3461\n",
      "Epoch [8/40], Step [4/118], D_loss: 0.7821, G_loss: 1.3151\n",
      "Epoch [8/40], Step [5/118], D_loss: 0.6842, G_loss: 1.6746\n",
      "Epoch [8/40], Step [6/118], D_loss: 0.5983, G_loss: 2.2580\n",
      "Epoch [8/40], Step [7/118], D_loss: 1.1654, G_loss: 0.7028\n",
      "Epoch [8/40], Step [8/118], D_loss: 0.6018, G_loss: 1.7395\n",
      "Epoch [8/40], Step [9/118], D_loss: 0.8823, G_loss: 1.3570\n",
      "Epoch [8/40], Step [10/118], D_loss: 0.7271, G_loss: 1.1489\n",
      "Epoch [8/40], Step [11/118], D_loss: 0.8028, G_loss: 1.8343\n",
      "Epoch [8/40], Step [12/118], D_loss: 0.7672, G_loss: 2.1487\n",
      "Epoch [8/40], Step [13/118], D_loss: 0.6192, G_loss: 1.4421\n",
      "Epoch [8/40], Step [14/118], D_loss: 0.3155, G_loss: 2.0904\n",
      "Epoch [8/40], Step [15/118], D_loss: 0.6095, G_loss: 2.0025\n",
      "Epoch [8/40], Step [16/118], D_loss: 1.1486, G_loss: 0.5592\n",
      "Epoch [8/40], Step [17/118], D_loss: 1.7714, G_loss: 2.7809\n",
      "Epoch [8/40], Step [18/118], D_loss: 1.4588, G_loss: 1.1641\n",
      "Epoch [8/40], Step [19/118], D_loss: 0.5853, G_loss: 1.2609\n",
      "Epoch [8/40], Step [20/118], D_loss: 1.0748, G_loss: 1.9233\n",
      "Epoch [8/40], Step [21/118], D_loss: 1.2465, G_loss: 0.6115\n",
      "Epoch [8/40], Step [22/118], D_loss: 1.2320, G_loss: 2.1230\n",
      "Epoch [8/40], Step [23/118], D_loss: 0.5804, G_loss: 2.3096\n",
      "Epoch [8/40], Step [24/118], D_loss: 0.3956, G_loss: 1.9554\n",
      "Epoch [8/40], Step [25/118], D_loss: 0.6074, G_loss: 1.7195\n",
      "Epoch [8/40], Step [26/118], D_loss: 0.6103, G_loss: 1.8067\n",
      "Epoch [8/40], Step [27/118], D_loss: 0.4664, G_loss: 2.1990\n",
      "Epoch [8/40], Step [28/118], D_loss: 1.0379, G_loss: 0.6538\n",
      "Epoch [8/40], Step [29/118], D_loss: 0.5587, G_loss: 1.8640\n",
      "Epoch [8/40], Step [30/118], D_loss: 0.4453, G_loss: 2.5859\n",
      "Epoch [8/40], Step [31/118], D_loss: 0.9094, G_loss: 0.8832\n",
      "Epoch [8/40], Step [32/118], D_loss: 1.0611, G_loss: 2.2523\n",
      "Epoch [8/40], Step [33/118], D_loss: 1.4240, G_loss: 0.5962\n",
      "Epoch [8/40], Step [34/118], D_loss: 1.0439, G_loss: 1.8732\n",
      "Epoch [8/40], Step [35/118], D_loss: 0.8263, G_loss: 1.4823\n",
      "Epoch [8/40], Step [36/118], D_loss: 0.8830, G_loss: 0.7215\n",
      "Epoch [8/40], Step [37/118], D_loss: 0.7476, G_loss: 2.3404\n",
      "Epoch [8/40], Step [38/118], D_loss: 0.6002, G_loss: 1.9548\n",
      "Epoch [8/40], Step [39/118], D_loss: 0.7806, G_loss: 1.1545\n",
      "Epoch [8/40], Step [40/118], D_loss: 0.6632, G_loss: 1.6546\n",
      "Epoch [8/40], Step [41/118], D_loss: 0.7479, G_loss: 1.3141\n",
      "Epoch [8/40], Step [42/118], D_loss: 0.4536, G_loss: 1.5433\n",
      "Epoch [8/40], Step [43/118], D_loss: 0.8332, G_loss: 2.4407\n",
      "Epoch [8/40], Step [44/118], D_loss: 1.0781, G_loss: 0.9299\n",
      "Epoch [8/40], Step [45/118], D_loss: 0.5985, G_loss: 1.3550\n",
      "Epoch [8/40], Step [46/118], D_loss: 1.0155, G_loss: 1.8055\n",
      "Epoch [8/40], Step [47/118], D_loss: 0.7250, G_loss: 1.3437\n",
      "Epoch [8/40], Step [48/118], D_loss: 0.7354, G_loss: 1.2354\n",
      "Epoch [8/40], Step [49/118], D_loss: 0.5447, G_loss: 1.9186\n",
      "Epoch [8/40], Step [50/118], D_loss: 0.5061, G_loss: 2.4533\n",
      "Epoch [8/40], Step [51/118], D_loss: 1.1730, G_loss: 0.5697\n",
      "Epoch [8/40], Step [52/118], D_loss: 1.6314, G_loss: 2.6581\n",
      "Epoch [8/40], Step [53/118], D_loss: 2.1789, G_loss: 1.0559\n",
      "Epoch [8/40], Step [54/118], D_loss: 0.5703, G_loss: 1.2693\n",
      "Epoch [8/40], Step [55/118], D_loss: 1.5293, G_loss: 2.3724\n",
      "Epoch [8/40], Step [56/118], D_loss: 0.8185, G_loss: 2.1617\n",
      "Epoch [8/40], Step [57/118], D_loss: 1.2191, G_loss: 0.3160\n",
      "Epoch [8/40], Step [58/118], D_loss: 1.8521, G_loss: 1.6194\n",
      "Epoch [8/40], Step [59/118], D_loss: 0.9333, G_loss: 1.9011\n",
      "Epoch [8/40], Step [60/118], D_loss: 0.7567, G_loss: 1.4325\n",
      "Epoch [8/40], Step [61/118], D_loss: 0.7949, G_loss: 0.9299\n",
      "Epoch [8/40], Step [62/118], D_loss: 1.1141, G_loss: 1.6401\n",
      "Epoch [8/40], Step [63/118], D_loss: 0.7234, G_loss: 1.9220\n",
      "Epoch [8/40], Step [64/118], D_loss: 1.3044, G_loss: 0.6066\n",
      "Epoch [8/40], Step [65/118], D_loss: 0.8441, G_loss: 1.3623\n",
      "Epoch [8/40], Step [66/118], D_loss: 0.6346, G_loss: 2.6047\n",
      "Epoch [8/40], Step [67/118], D_loss: 0.9808, G_loss: 1.4112\n",
      "Epoch [8/40], Step [68/118], D_loss: 1.0751, G_loss: 1.5059\n",
      "Epoch [8/40], Step [69/118], D_loss: 1.5922, G_loss: 0.5626\n",
      "Epoch [8/40], Step [70/118], D_loss: 0.6464, G_loss: 1.4040\n",
      "Epoch [8/40], Step [71/118], D_loss: 0.7234, G_loss: 1.6000\n",
      "Epoch [8/40], Step [72/118], D_loss: 0.5362, G_loss: 1.8590\n",
      "Epoch [8/40], Step [73/118], D_loss: 0.6638, G_loss: 1.2519\n",
      "Epoch [8/40], Step [74/118], D_loss: 1.0920, G_loss: 2.0222\n",
      "Epoch [8/40], Step [75/118], D_loss: 0.8375, G_loss: 1.5989\n",
      "Epoch [8/40], Step [76/118], D_loss: 0.6613, G_loss: 1.1483\n",
      "Epoch [8/40], Step [77/118], D_loss: 0.8271, G_loss: 1.8118\n",
      "Epoch [8/40], Step [78/118], D_loss: 0.5091, G_loss: 1.9821\n",
      "Epoch [8/40], Step [79/118], D_loss: 0.6314, G_loss: 1.1490\n",
      "Epoch [8/40], Step [80/118], D_loss: 0.9975, G_loss: 0.9933\n",
      "Epoch [8/40], Step [81/118], D_loss: 1.3714, G_loss: 1.8498\n",
      "Epoch [8/40], Step [82/118], D_loss: 0.7626, G_loss: 1.7114\n",
      "Epoch [8/40], Step [83/118], D_loss: 0.9718, G_loss: 1.2141\n",
      "Epoch [8/40], Step [84/118], D_loss: 0.4907, G_loss: 1.7183\n",
      "Epoch [8/40], Step [85/118], D_loss: 0.5009, G_loss: 1.9790\n",
      "Epoch [8/40], Step [86/118], D_loss: 1.2706, G_loss: 0.5608\n",
      "Epoch [8/40], Step [87/118], D_loss: 0.8284, G_loss: 1.8200\n",
      "Epoch [8/40], Step [88/118], D_loss: 0.5764, G_loss: 2.3556\n",
      "Epoch [8/40], Step [89/118], D_loss: 0.8289, G_loss: 1.0492\n",
      "Epoch [8/40], Step [90/118], D_loss: 0.8928, G_loss: 1.1591\n",
      "Epoch [8/40], Step [91/118], D_loss: 0.7316, G_loss: 2.1808\n",
      "Epoch [8/40], Step [92/118], D_loss: 0.8904, G_loss: 1.0369\n",
      "Epoch [8/40], Step [93/118], D_loss: 0.9289, G_loss: 1.5935\n",
      "Epoch [8/40], Step [94/118], D_loss: 1.2139, G_loss: 1.1144\n",
      "Epoch [8/40], Step [95/118], D_loss: 0.6418, G_loss: 1.2158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/40], Step [96/118], D_loss: 0.7173, G_loss: 2.0225\n",
      "Epoch [8/40], Step [97/118], D_loss: 0.9053, G_loss: 1.2410\n",
      "Epoch [8/40], Step [98/118], D_loss: 0.7714, G_loss: 1.4943\n",
      "Epoch [8/40], Step [99/118], D_loss: 0.9515, G_loss: 0.9180\n",
      "Epoch [8/40], Step [100/118], D_loss: 0.7604, G_loss: 1.3814\n",
      "Epoch [8/40], Step [101/118], D_loss: 0.8945, G_loss: 0.8068\n",
      "Epoch [8/40], Step [102/118], D_loss: 1.0877, G_loss: 2.6013\n",
      "Epoch [8/40], Step [103/118], D_loss: 1.1670, G_loss: 1.0643\n",
      "Epoch [8/40], Step [104/118], D_loss: 1.4111, G_loss: 0.6531\n",
      "Epoch [8/40], Step [105/118], D_loss: 0.9115, G_loss: 1.4142\n",
      "Epoch [8/40], Step [106/118], D_loss: 0.5762, G_loss: 1.5464\n",
      "Epoch [8/40], Step [107/118], D_loss: 0.7367, G_loss: 1.4747\n",
      "Epoch [8/40], Step [108/118], D_loss: 0.5205, G_loss: 1.6268\n",
      "Epoch [8/40], Step [109/118], D_loss: 0.6951, G_loss: 2.1666\n",
      "Epoch [8/40], Step [110/118], D_loss: 0.8833, G_loss: 0.9648\n",
      "Epoch [8/40], Step [111/118], D_loss: 0.7973, G_loss: 1.4949\n",
      "Epoch [8/40], Step [112/118], D_loss: 0.8129, G_loss: 1.5696\n",
      "Epoch [8/40], Step [113/118], D_loss: 0.7628, G_loss: 1.2679\n",
      "Epoch [8/40], Step [114/118], D_loss: 0.7756, G_loss: 1.5820\n",
      "Epoch [8/40], Step [115/118], D_loss: 0.7457, G_loss: 1.7438\n",
      "Epoch [8/40], Step [116/118], D_loss: 0.6180, G_loss: 1.4645\n",
      "Epoch [8/40], Step [117/118], D_loss: 0.9330, G_loss: 1.5716\n",
      "Epoch [8/40], Step [118/118], D_loss: 1.2721, G_loss: 0.5149\n",
      "Epoch [9/40], Step [1/118], D_loss: 1.2288, G_loss: 2.8114\n",
      "Epoch [9/40], Step [2/118], D_loss: 0.8403, G_loss: 1.7527\n",
      "Epoch [9/40], Step [3/118], D_loss: 0.7744, G_loss: 0.5408\n",
      "Epoch [9/40], Step [4/118], D_loss: 1.7082, G_loss: 2.3292\n",
      "Epoch [9/40], Step [5/118], D_loss: 1.3322, G_loss: 0.9629\n",
      "Epoch [9/40], Step [6/118], D_loss: 0.6501, G_loss: 0.7017\n",
      "Epoch [9/40], Step [7/118], D_loss: 1.5804, G_loss: 2.4168\n",
      "Epoch [9/40], Step [8/118], D_loss: 1.6080, G_loss: 0.8877\n",
      "Epoch [9/40], Step [9/118], D_loss: 0.5804, G_loss: 1.1679\n",
      "Epoch [9/40], Step [10/118], D_loss: 0.9507, G_loss: 1.2328\n",
      "Epoch [9/40], Step [11/118], D_loss: 0.5298, G_loss: 2.2013\n",
      "Epoch [9/40], Step [12/118], D_loss: 0.7789, G_loss: 1.2187\n",
      "Epoch [9/40], Step [13/118], D_loss: 0.7433, G_loss: 1.7181\n",
      "Epoch [9/40], Step [14/118], D_loss: 0.6224, G_loss: 1.9291\n",
      "Epoch [9/40], Step [15/118], D_loss: 1.2975, G_loss: 0.4290\n",
      "Epoch [9/40], Step [16/118], D_loss: 1.5407, G_loss: 2.0437\n",
      "Epoch [9/40], Step [17/118], D_loss: 1.1865, G_loss: 1.1189\n",
      "Epoch [9/40], Step [18/118], D_loss: 0.9207, G_loss: 0.7333\n",
      "Epoch [9/40], Step [19/118], D_loss: 1.1267, G_loss: 1.8887\n",
      "Epoch [9/40], Step [20/118], D_loss: 0.9696, G_loss: 1.2674\n",
      "Epoch [9/40], Step [21/118], D_loss: 0.9659, G_loss: 1.8230\n",
      "Epoch [9/40], Step [22/118], D_loss: 0.6962, G_loss: 1.7116\n",
      "Epoch [9/40], Step [23/118], D_loss: 0.9365, G_loss: 0.7409\n",
      "Epoch [9/40], Step [24/118], D_loss: 1.1228, G_loss: 1.0467\n",
      "Epoch [9/40], Step [25/118], D_loss: 1.1140, G_loss: 2.5100\n",
      "Epoch [9/40], Step [26/118], D_loss: 2.3946, G_loss: 0.9183\n",
      "Epoch [9/40], Step [27/118], D_loss: 1.1740, G_loss: 0.7736\n",
      "Epoch [9/40], Step [28/118], D_loss: 1.9849, G_loss: 2.1379\n",
      "Epoch [9/40], Step [29/118], D_loss: 1.5659, G_loss: 1.5401\n",
      "Epoch [9/40], Step [30/118], D_loss: 0.5123, G_loss: 1.2115\n",
      "Epoch [9/40], Step [31/118], D_loss: 1.0929, G_loss: 1.5128\n",
      "Epoch [9/40], Step [32/118], D_loss: 0.9913, G_loss: 1.2195\n",
      "Epoch [9/40], Step [33/118], D_loss: 1.4675, G_loss: 1.2531\n",
      "Epoch [9/40], Step [34/118], D_loss: 0.5519, G_loss: 2.3226\n",
      "Epoch [9/40], Step [35/118], D_loss: 0.6179, G_loss: 1.2192\n",
      "Epoch [9/40], Step [36/118], D_loss: 0.5851, G_loss: 1.5332\n",
      "Epoch [9/40], Step [37/118], D_loss: 0.5044, G_loss: 1.7315\n",
      "Epoch [9/40], Step [38/118], D_loss: 0.7669, G_loss: 1.5350\n",
      "Epoch [9/40], Step [39/118], D_loss: 0.5788, G_loss: 2.1953\n",
      "Epoch [9/40], Step [40/118], D_loss: 1.2853, G_loss: 0.8658\n",
      "Epoch [9/40], Step [41/118], D_loss: 1.3380, G_loss: 1.5323\n",
      "Epoch [9/40], Step [42/118], D_loss: 0.9291, G_loss: 1.4696\n",
      "Epoch [9/40], Step [43/118], D_loss: 0.5304, G_loss: 1.6549\n",
      "Epoch [9/40], Step [44/118], D_loss: 0.5357, G_loss: 1.6456\n",
      "Epoch [9/40], Step [45/118], D_loss: 0.7908, G_loss: 1.1250\n",
      "Epoch [9/40], Step [46/118], D_loss: 1.4719, G_loss: 2.2057\n",
      "Epoch [9/40], Step [47/118], D_loss: 1.3703, G_loss: 1.7861\n",
      "Epoch [9/40], Step [48/118], D_loss: 0.7130, G_loss: 0.6732\n",
      "Epoch [9/40], Step [49/118], D_loss: 1.2911, G_loss: 1.7837\n",
      "Epoch [9/40], Step [50/118], D_loss: 1.0594, G_loss: 1.3774\n",
      "Epoch [9/40], Step [51/118], D_loss: 0.7361, G_loss: 1.3781\n",
      "Epoch [9/40], Step [52/118], D_loss: 1.1980, G_loss: 0.9437\n",
      "Epoch [9/40], Step [53/118], D_loss: 0.7437, G_loss: 1.1685\n",
      "Epoch [9/40], Step [54/118], D_loss: 0.5737, G_loss: 1.5727\n",
      "Epoch [9/40], Step [55/118], D_loss: 0.7733, G_loss: 1.3592\n",
      "Epoch [9/40], Step [56/118], D_loss: 0.8945, G_loss: 1.0925\n",
      "Epoch [9/40], Step [57/118], D_loss: 1.0562, G_loss: 1.6716\n",
      "Epoch [9/40], Step [58/118], D_loss: 0.9709, G_loss: 1.1148\n",
      "Epoch [9/40], Step [59/118], D_loss: 0.9988, G_loss: 1.4931\n",
      "Epoch [9/40], Step [60/118], D_loss: 1.0305, G_loss: 0.9975\n",
      "Epoch [9/40], Step [61/118], D_loss: 0.6822, G_loss: 1.2607\n",
      "Epoch [9/40], Step [62/118], D_loss: 0.5591, G_loss: 2.2654\n",
      "Epoch [9/40], Step [63/118], D_loss: 0.8289, G_loss: 1.8280\n",
      "Epoch [9/40], Step [64/118], D_loss: 1.5212, G_loss: 0.6003\n",
      "Epoch [9/40], Step [65/118], D_loss: 0.9333, G_loss: 1.2879\n",
      "Epoch [9/40], Step [66/118], D_loss: 0.6794, G_loss: 2.5159\n",
      "Epoch [9/40], Step [67/118], D_loss: 0.8970, G_loss: 1.5069\n",
      "Epoch [9/40], Step [68/118], D_loss: 0.4849, G_loss: 1.2602\n",
      "Epoch [9/40], Step [69/118], D_loss: 0.8236, G_loss: 1.3402\n",
      "Epoch [9/40], Step [70/118], D_loss: 0.5901, G_loss: 1.9038\n",
      "Epoch [9/40], Step [71/118], D_loss: 0.8628, G_loss: 1.4465\n",
      "Epoch [9/40], Step [72/118], D_loss: 0.9525, G_loss: 0.7731\n",
      "Epoch [9/40], Step [73/118], D_loss: 0.9951, G_loss: 1.7658\n",
      "Epoch [9/40], Step [74/118], D_loss: 1.1412, G_loss: 1.1186\n",
      "Epoch [9/40], Step [75/118], D_loss: 0.6406, G_loss: 1.1451\n",
      "Epoch [9/40], Step [76/118], D_loss: 1.3011, G_loss: 1.4181\n",
      "Epoch [9/40], Step [77/118], D_loss: 0.6983, G_loss: 1.6868\n",
      "Epoch [9/40], Step [78/118], D_loss: 0.5502, G_loss: 1.3680\n",
      "Epoch [9/40], Step [79/118], D_loss: 0.8017, G_loss: 2.0755\n",
      "Epoch [9/40], Step [80/118], D_loss: 0.4615, G_loss: 2.4474\n",
      "Epoch [9/40], Step [81/118], D_loss: 0.4918, G_loss: 1.3965\n",
      "Epoch [9/40], Step [82/118], D_loss: 1.1667, G_loss: 0.8159\n",
      "Epoch [9/40], Step [83/118], D_loss: 1.1915, G_loss: 1.9641\n",
      "Epoch [9/40], Step [84/118], D_loss: 1.5797, G_loss: 0.6663\n",
      "Epoch [9/40], Step [85/118], D_loss: 0.6007, G_loss: 1.2474\n",
      "Epoch [9/40], Step [86/118], D_loss: 1.0698, G_loss: 2.2591\n",
      "Epoch [9/40], Step [87/118], D_loss: 1.3388, G_loss: 0.9440\n",
      "Epoch [9/40], Step [88/118], D_loss: 0.9762, G_loss: 0.8568\n",
      "Epoch [9/40], Step [89/118], D_loss: 1.1402, G_loss: 1.8431\n",
      "Epoch [9/40], Step [90/118], D_loss: 1.1582, G_loss: 1.0311\n",
      "Epoch [9/40], Step [91/118], D_loss: 1.2140, G_loss: 0.4294\n",
      "Epoch [9/40], Step [92/118], D_loss: 1.8607, G_loss: 1.6951\n",
      "Epoch [9/40], Step [93/118], D_loss: 1.2588, G_loss: 1.6203\n",
      "Epoch [9/40], Step [94/118], D_loss: 0.6988, G_loss: 1.5816\n",
      "Epoch [9/40], Step [95/118], D_loss: 0.8609, G_loss: 0.7545\n",
      "Epoch [9/40], Step [96/118], D_loss: 0.7481, G_loss: 1.1548\n",
      "Epoch [9/40], Step [97/118], D_loss: 1.6241, G_loss: 1.9282\n",
      "Epoch [9/40], Step [98/118], D_loss: 0.4789, G_loss: 3.0417\n",
      "Epoch [9/40], Step [99/118], D_loss: 1.2141, G_loss: 0.8238\n",
      "Epoch [9/40], Step [100/118], D_loss: 1.7225, G_loss: 1.1530\n",
      "Epoch [9/40], Step [101/118], D_loss: 0.4923, G_loss: 2.5724\n",
      "Epoch [9/40], Step [102/118], D_loss: 1.1804, G_loss: 0.9581\n",
      "Epoch [9/40], Step [103/118], D_loss: 1.1648, G_loss: 1.4269\n",
      "Epoch [9/40], Step [104/118], D_loss: 0.8385, G_loss: 1.5659\n",
      "Epoch [9/40], Step [105/118], D_loss: 0.6352, G_loss: 1.3657\n",
      "Epoch [9/40], Step [106/118], D_loss: 0.5618, G_loss: 1.3754\n",
      "Epoch [9/40], Step [107/118], D_loss: 0.4471, G_loss: 1.6795\n",
      "Epoch [9/40], Step [108/118], D_loss: 1.2709, G_loss: 1.8970\n",
      "Epoch [9/40], Step [109/118], D_loss: 1.4799, G_loss: 1.0288\n",
      "Epoch [9/40], Step [110/118], D_loss: 0.5316, G_loss: 0.9676\n",
      "Epoch [9/40], Step [111/118], D_loss: 1.1347, G_loss: 1.6423\n",
      "Epoch [9/40], Step [112/118], D_loss: 0.9879, G_loss: 1.6321\n",
      "Epoch [9/40], Step [113/118], D_loss: 0.6415, G_loss: 1.6666\n",
      "Epoch [9/40], Step [114/118], D_loss: 0.6467, G_loss: 1.2980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/40], Step [115/118], D_loss: 0.5399, G_loss: 1.7729\n",
      "Epoch [9/40], Step [116/118], D_loss: 1.0047, G_loss: 1.0498\n",
      "Epoch [9/40], Step [117/118], D_loss: 1.1391, G_loss: 0.6896\n",
      "Epoch [9/40], Step [118/118], D_loss: 0.5264, G_loss: 2.1973\n",
      "Epoch [10/40], Step [1/118], D_loss: 0.6156, G_loss: 1.8127\n",
      "Epoch [10/40], Step [2/118], D_loss: 1.1199, G_loss: 1.0570\n",
      "Epoch [10/40], Step [3/118], D_loss: 1.5525, G_loss: 1.8648\n",
      "Epoch [10/40], Step [4/118], D_loss: 2.5941, G_loss: 0.7840\n",
      "Epoch [10/40], Step [5/118], D_loss: 0.3665, G_loss: 1.2602\n",
      "Epoch [10/40], Step [6/118], D_loss: 1.5416, G_loss: 2.0323\n",
      "Epoch [10/40], Step [7/118], D_loss: 1.3676, G_loss: 1.6880\n",
      "Epoch [10/40], Step [8/118], D_loss: 0.8374, G_loss: 0.6515\n",
      "Epoch [10/40], Step [9/118], D_loss: 1.1980, G_loss: 1.1284\n",
      "Epoch [10/40], Step [10/118], D_loss: 0.7552, G_loss: 1.7245\n",
      "Epoch [10/40], Step [11/118], D_loss: 0.7532, G_loss: 1.7916\n",
      "Epoch [10/40], Step [12/118], D_loss: 0.8237, G_loss: 1.3631\n",
      "Epoch [10/40], Step [13/118], D_loss: 0.9058, G_loss: 0.8243\n",
      "Epoch [10/40], Step [14/118], D_loss: 0.9154, G_loss: 1.8705\n",
      "Epoch [10/40], Step [15/118], D_loss: 0.8522, G_loss: 1.6927\n",
      "Epoch [10/40], Step [16/118], D_loss: 0.4567, G_loss: 1.4076\n",
      "Epoch [10/40], Step [17/118], D_loss: 1.3154, G_loss: 1.9435\n",
      "Epoch [10/40], Step [18/118], D_loss: 1.4418, G_loss: 1.2236\n",
      "Epoch [10/40], Step [19/118], D_loss: 0.9458, G_loss: 0.8718\n",
      "Epoch [10/40], Step [20/118], D_loss: 0.6329, G_loss: 1.4005\n",
      "Epoch [10/40], Step [21/118], D_loss: 0.5580, G_loss: 1.8552\n",
      "Epoch [10/40], Step [22/118], D_loss: 0.5431, G_loss: 1.9324\n",
      "Epoch [10/40], Step [23/118], D_loss: 0.9109, G_loss: 1.0118\n",
      "Epoch [10/40], Step [24/118], D_loss: 0.9925, G_loss: 1.2421\n",
      "Epoch [10/40], Step [25/118], D_loss: 0.6192, G_loss: 2.1822\n",
      "Epoch [10/40], Step [26/118], D_loss: 0.9911, G_loss: 1.2531\n",
      "Epoch [10/40], Step [27/118], D_loss: 0.2849, G_loss: 1.5374\n",
      "Epoch [10/40], Step [28/118], D_loss: 0.7042, G_loss: 1.8854\n",
      "Epoch [10/40], Step [29/118], D_loss: 0.7965, G_loss: 2.1914\n",
      "Epoch [10/40], Step [30/118], D_loss: 1.3786, G_loss: 1.0736\n",
      "Epoch [10/40], Step [31/118], D_loss: 0.8534, G_loss: 1.2132\n",
      "Epoch [10/40], Step [32/118], D_loss: 1.2644, G_loss: 2.3807\n",
      "Epoch [10/40], Step [33/118], D_loss: 1.5376, G_loss: 1.7409\n",
      "Epoch [10/40], Step [34/118], D_loss: 0.8880, G_loss: 0.7414\n",
      "Epoch [10/40], Step [35/118], D_loss: 0.6366, G_loss: 1.4923\n",
      "Epoch [10/40], Step [36/118], D_loss: 0.9985, G_loss: 0.9560\n",
      "Epoch [10/40], Step [37/118], D_loss: 1.6554, G_loss: 1.4892\n",
      "Epoch [10/40], Step [38/118], D_loss: 1.1191, G_loss: 1.3801\n",
      "Epoch [10/40], Step [39/118], D_loss: 0.7657, G_loss: 0.9423\n",
      "Epoch [10/40], Step [40/118], D_loss: 0.8933, G_loss: 1.6972\n",
      "Epoch [10/40], Step [41/118], D_loss: 1.1002, G_loss: 1.2082\n",
      "Epoch [10/40], Step [42/118], D_loss: 0.7378, G_loss: 1.0983\n",
      "Epoch [10/40], Step [43/118], D_loss: 0.7678, G_loss: 1.4395\n",
      "Epoch [10/40], Step [44/118], D_loss: 0.6647, G_loss: 1.7082\n",
      "Epoch [10/40], Step [45/118], D_loss: 0.5042, G_loss: 1.7528\n",
      "Epoch [10/40], Step [46/118], D_loss: 0.5130, G_loss: 1.8808\n",
      "Epoch [10/40], Step [47/118], D_loss: 0.7114, G_loss: 1.3719\n",
      "Epoch [10/40], Step [48/118], D_loss: 0.7900, G_loss: 1.6706\n",
      "Epoch [10/40], Step [49/118], D_loss: 1.2200, G_loss: 0.8400\n",
      "Epoch [10/40], Step [50/118], D_loss: 1.0486, G_loss: 0.8446\n",
      "Epoch [10/40], Step [51/118], D_loss: 1.0815, G_loss: 1.2915\n",
      "Epoch [10/40], Step [52/118], D_loss: 0.7114, G_loss: 1.7122\n",
      "Epoch [10/40], Step [53/118], D_loss: 0.5292, G_loss: 1.8344\n",
      "Epoch [10/40], Step [54/118], D_loss: 1.3656, G_loss: 0.8039\n",
      "Epoch [10/40], Step [55/118], D_loss: 1.0517, G_loss: 1.4528\n",
      "Epoch [10/40], Step [56/118], D_loss: 1.2573, G_loss: 0.7683\n",
      "Epoch [10/40], Step [57/118], D_loss: 1.5231, G_loss: 1.2601\n",
      "Epoch [10/40], Step [58/118], D_loss: 0.5242, G_loss: 2.1446\n",
      "Epoch [10/40], Step [59/118], D_loss: 1.1873, G_loss: 0.3987\n",
      "Epoch [10/40], Step [60/118], D_loss: 1.5069, G_loss: 1.7121\n",
      "Epoch [10/40], Step [61/118], D_loss: 0.3994, G_loss: 2.9160\n",
      "Epoch [10/40], Step [62/118], D_loss: 0.8417, G_loss: 1.5105\n",
      "Epoch [10/40], Step [63/118], D_loss: 1.0829, G_loss: 0.6336\n",
      "Epoch [10/40], Step [64/118], D_loss: 0.9502, G_loss: 2.1280\n",
      "Epoch [10/40], Step [65/118], D_loss: 1.0735, G_loss: 1.2571\n",
      "Epoch [10/40], Step [66/118], D_loss: 0.4373, G_loss: 1.5411\n",
      "Epoch [10/40], Step [67/118], D_loss: 0.6142, G_loss: 1.6885\n",
      "Epoch [10/40], Step [68/118], D_loss: 1.4300, G_loss: 0.6779\n",
      "Epoch [10/40], Step [69/118], D_loss: 1.2436, G_loss: 1.5730\n",
      "Epoch [10/40], Step [70/118], D_loss: 1.5759, G_loss: 0.7417\n",
      "Epoch [10/40], Step [71/118], D_loss: 0.4285, G_loss: 1.6145\n",
      "Epoch [10/40], Step [72/118], D_loss: 0.9203, G_loss: 1.6342\n",
      "Epoch [10/40], Step [73/118], D_loss: 1.6007, G_loss: 0.6091\n",
      "Epoch [10/40], Step [74/118], D_loss: 2.1412, G_loss: 1.3103\n",
      "Epoch [10/40], Step [75/118], D_loss: 0.2595, G_loss: 3.6504\n",
      "Epoch [10/40], Step [76/118], D_loss: 1.4099, G_loss: 0.7632\n",
      "Epoch [10/40], Step [77/118], D_loss: 0.8942, G_loss: 0.5772\n",
      "Epoch [10/40], Step [78/118], D_loss: 1.2564, G_loss: 0.5494\n",
      "Epoch [10/40], Step [79/118], D_loss: 1.6837, G_loss: 1.5568\n",
      "Epoch [10/40], Step [80/118], D_loss: 0.7102, G_loss: 2.1832\n",
      "Epoch [10/40], Step [81/118], D_loss: 0.6731, G_loss: 1.5166\n",
      "Epoch [10/40], Step [82/118], D_loss: 0.6760, G_loss: 1.6297\n",
      "Epoch [10/40], Step [83/118], D_loss: 1.0627, G_loss: 1.0508\n",
      "Epoch [10/40], Step [84/118], D_loss: 0.9822, G_loss: 1.6188\n",
      "Epoch [10/40], Step [85/118], D_loss: 0.9943, G_loss: 2.3450\n",
      "Epoch [10/40], Step [86/118], D_loss: 1.4473, G_loss: 1.1950\n",
      "Epoch [10/40], Step [87/118], D_loss: 1.7942, G_loss: 0.5593\n",
      "Epoch [10/40], Step [88/118], D_loss: 0.9911, G_loss: 1.2617\n",
      "Epoch [10/40], Step [89/118], D_loss: 0.5132, G_loss: 1.8400\n",
      "Epoch [10/40], Step [90/118], D_loss: 0.9942, G_loss: 0.9314\n",
      "Epoch [10/40], Step [91/118], D_loss: 0.3576, G_loss: 1.5994\n",
      "Epoch [10/40], Step [92/118], D_loss: 1.9046, G_loss: 1.6801\n",
      "Epoch [10/40], Step [93/118], D_loss: 1.1264, G_loss: 1.7614\n",
      "Epoch [10/40], Step [94/118], D_loss: 0.6815, G_loss: 1.0532\n",
      "Epoch [10/40], Step [95/118], D_loss: 1.2844, G_loss: 1.2128\n",
      "Epoch [10/40], Step [96/118], D_loss: 1.0658, G_loss: 1.7758\n",
      "Epoch [10/40], Step [97/118], D_loss: 0.6330, G_loss: 2.2056\n",
      "Epoch [10/40], Step [98/118], D_loss: 0.6065, G_loss: 1.1012\n",
      "Epoch [10/40], Step [99/118], D_loss: 0.6752, G_loss: 1.2833\n",
      "Epoch [10/40], Step [100/118], D_loss: 0.5500, G_loss: 1.9089\n",
      "Epoch [10/40], Step [101/118], D_loss: 0.9626, G_loss: 1.3134\n",
      "Epoch [10/40], Step [102/118], D_loss: 1.0186, G_loss: 1.2530\n",
      "Epoch [10/40], Step [103/118], D_loss: 0.7780, G_loss: 1.5148\n",
      "Epoch [10/40], Step [104/118], D_loss: 0.4994, G_loss: 2.0068\n",
      "Epoch [10/40], Step [105/118], D_loss: 2.4671, G_loss: 0.3071\n",
      "Epoch [10/40], Step [106/118], D_loss: 0.7691, G_loss: 1.5076\n",
      "Epoch [10/40], Step [107/118], D_loss: 0.7906, G_loss: 1.2193\n",
      "Epoch [10/40], Step [108/118], D_loss: 1.0409, G_loss: 1.7696\n",
      "Epoch [10/40], Step [109/118], D_loss: 0.4316, G_loss: 2.6258\n",
      "Epoch [10/40], Step [110/118], D_loss: 0.8005, G_loss: 1.5021\n",
      "Epoch [10/40], Step [111/118], D_loss: 1.1752, G_loss: 0.5122\n",
      "Epoch [10/40], Step [112/118], D_loss: 0.7842, G_loss: 1.4553\n",
      "Epoch [10/40], Step [113/118], D_loss: 0.3825, G_loss: 2.2413\n",
      "Epoch [10/40], Step [114/118], D_loss: 0.6078, G_loss: 1.6080\n",
      "Epoch [10/40], Step [115/118], D_loss: 0.8276, G_loss: 1.0271\n",
      "Epoch [10/40], Step [116/118], D_loss: 1.7684, G_loss: 1.6803\n",
      "Epoch [10/40], Step [117/118], D_loss: 0.9533, G_loss: 1.8420\n",
      "Epoch [10/40], Step [118/118], D_loss: 0.6419, G_loss: 1.1022\n",
      "Epoch [11/40], Step [1/118], D_loss: 0.6450, G_loss: 1.2082\n",
      "Epoch [11/40], Step [2/118], D_loss: 0.7969, G_loss: 1.2301\n",
      "Epoch [11/40], Step [3/118], D_loss: 1.1993, G_loss: 0.9708\n",
      "Epoch [11/40], Step [4/118], D_loss: 1.0965, G_loss: 1.0932\n",
      "Epoch [11/40], Step [5/118], D_loss: 0.5764, G_loss: 1.4495\n",
      "Epoch [11/40], Step [6/118], D_loss: 1.2816, G_loss: 1.0214\n",
      "Epoch [11/40], Step [7/118], D_loss: 0.5999, G_loss: 1.9433\n",
      "Epoch [11/40], Step [8/118], D_loss: 0.8887, G_loss: 1.0214\n",
      "Epoch [11/40], Step [9/118], D_loss: 0.7382, G_loss: 1.0627\n",
      "Epoch [11/40], Step [10/118], D_loss: 0.4382, G_loss: 1.6687\n",
      "Epoch [11/40], Step [11/118], D_loss: 0.9154, G_loss: 1.1954\n",
      "Epoch [11/40], Step [12/118], D_loss: 0.6257, G_loss: 2.0633\n",
      "Epoch [11/40], Step [13/118], D_loss: 0.4975, G_loss: 1.4753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/40], Step [14/118], D_loss: 1.1209, G_loss: 1.0023\n",
      "Epoch [11/40], Step [15/118], D_loss: 0.7448, G_loss: 1.5844\n",
      "Epoch [11/40], Step [16/118], D_loss: 0.6672, G_loss: 1.2492\n",
      "Epoch [11/40], Step [17/118], D_loss: 0.6890, G_loss: 1.3819\n",
      "Epoch [11/40], Step [18/118], D_loss: 0.7343, G_loss: 1.3984\n",
      "Epoch [11/40], Step [19/118], D_loss: 1.2231, G_loss: 1.3054\n",
      "Epoch [11/40], Step [20/118], D_loss: 0.6892, G_loss: 1.3719\n",
      "Epoch [11/40], Step [21/118], D_loss: 1.5065, G_loss: 0.6279\n",
      "Epoch [11/40], Step [22/118], D_loss: 0.8566, G_loss: 1.5988\n",
      "Epoch [11/40], Step [23/118], D_loss: 0.8517, G_loss: 1.6824\n",
      "Epoch [11/40], Step [24/118], D_loss: 1.7664, G_loss: 0.6025\n",
      "Epoch [11/40], Step [25/118], D_loss: 0.8382, G_loss: 1.1415\n",
      "Epoch [11/40], Step [26/118], D_loss: 0.7281, G_loss: 1.1212\n",
      "Epoch [11/40], Step [27/118], D_loss: 0.4367, G_loss: 1.8761\n",
      "Epoch [11/40], Step [28/118], D_loss: 0.4155, G_loss: 1.7116\n",
      "Epoch [11/40], Step [29/118], D_loss: 0.4980, G_loss: 1.6597\n",
      "Epoch [11/40], Step [30/118], D_loss: 0.6995, G_loss: 1.3314\n",
      "Epoch [11/40], Step [31/118], D_loss: 1.1392, G_loss: 1.0153\n",
      "Epoch [11/40], Step [32/118], D_loss: 0.7758, G_loss: 1.2217\n",
      "Epoch [11/40], Step [33/118], D_loss: 0.6765, G_loss: 1.5194\n",
      "Epoch [11/40], Step [34/118], D_loss: 0.5711, G_loss: 1.4523\n",
      "Epoch [11/40], Step [35/118], D_loss: 0.3362, G_loss: 2.8448\n",
      "Epoch [11/40], Step [36/118], D_loss: 0.5264, G_loss: 1.4596\n",
      "Epoch [11/40], Step [37/118], D_loss: 0.7826, G_loss: 1.4647\n",
      "Epoch [11/40], Step [38/118], D_loss: 0.3839, G_loss: 1.5692\n",
      "Epoch [11/40], Step [39/118], D_loss: 1.4763, G_loss: 0.8741\n",
      "Epoch [11/40], Step [40/118], D_loss: 0.5235, G_loss: 1.9995\n",
      "Epoch [11/40], Step [41/118], D_loss: 1.4149, G_loss: 1.2782\n",
      "Epoch [11/40], Step [42/118], D_loss: 1.2661, G_loss: 0.5115\n",
      "Epoch [11/40], Step [43/118], D_loss: 0.8017, G_loss: 1.0236\n",
      "Epoch [11/40], Step [44/118], D_loss: 1.6470, G_loss: 1.1301\n",
      "Epoch [11/40], Step [45/118], D_loss: 1.3774, G_loss: 1.4168\n",
      "Epoch [11/40], Step [46/118], D_loss: 1.5295, G_loss: 1.4236\n",
      "Epoch [11/40], Step [47/118], D_loss: 1.0342, G_loss: 0.7913\n",
      "Epoch [11/40], Step [48/118], D_loss: 1.6720, G_loss: 0.5866\n",
      "Epoch [11/40], Step [49/118], D_loss: 0.5165, G_loss: 1.9371\n",
      "Epoch [11/40], Step [50/118], D_loss: 0.7733, G_loss: 1.5638\n",
      "Epoch [11/40], Step [51/118], D_loss: 0.5312, G_loss: 1.7543\n",
      "Epoch [11/40], Step [52/118], D_loss: 2.3638, G_loss: 0.9339\n",
      "Epoch [11/40], Step [53/118], D_loss: 1.0545, G_loss: 0.7138\n",
      "Epoch [11/40], Step [54/118], D_loss: 1.0727, G_loss: 1.1484\n",
      "Epoch [11/40], Step [55/118], D_loss: 0.9644, G_loss: 1.2253\n",
      "Epoch [11/40], Step [56/118], D_loss: 0.4578, G_loss: 1.8093\n",
      "Epoch [11/40], Step [57/118], D_loss: 0.8531, G_loss: 1.3658\n",
      "Epoch [11/40], Step [58/118], D_loss: 0.5783, G_loss: 1.8129\n",
      "Epoch [11/40], Step [59/118], D_loss: 1.0663, G_loss: 1.3027\n",
      "Epoch [11/40], Step [60/118], D_loss: 0.5253, G_loss: 1.3803\n",
      "Epoch [11/40], Step [61/118], D_loss: 0.8995, G_loss: 0.9887\n",
      "Epoch [11/40], Step [62/118], D_loss: 1.1476, G_loss: 0.7826\n",
      "Epoch [11/40], Step [63/118], D_loss: 1.1199, G_loss: 1.1806\n",
      "Epoch [11/40], Step [64/118], D_loss: 0.9442, G_loss: 1.6158\n",
      "Epoch [11/40], Step [65/118], D_loss: 0.4614, G_loss: 1.4615\n",
      "Epoch [11/40], Step [66/118], D_loss: 0.4495, G_loss: 1.7775\n",
      "Epoch [11/40], Step [67/118], D_loss: 1.4492, G_loss: 1.0349\n",
      "Epoch [11/40], Step [68/118], D_loss: 1.7984, G_loss: 1.2066\n",
      "Epoch [11/40], Step [69/118], D_loss: 0.4922, G_loss: 1.2953\n",
      "Epoch [11/40], Step [70/118], D_loss: 0.5839, G_loss: 1.4666\n",
      "Epoch [11/40], Step [71/118], D_loss: 0.2306, G_loss: 3.0547\n",
      "Epoch [11/40], Step [72/118], D_loss: 0.2834, G_loss: 1.9931\n",
      "Epoch [11/40], Step [73/118], D_loss: 1.0582, G_loss: 0.9696\n",
      "Epoch [11/40], Step [74/118], D_loss: 0.5981, G_loss: 1.3760\n",
      "Epoch [11/40], Step [75/118], D_loss: 0.5605, G_loss: 1.7056\n",
      "Epoch [11/40], Step [76/118], D_loss: 0.4585, G_loss: 2.0818\n",
      "Epoch [11/40], Step [77/118], D_loss: 0.2964, G_loss: 1.9983\n",
      "Epoch [11/40], Step [78/118], D_loss: 0.9503, G_loss: 1.1553\n",
      "Epoch [11/40], Step [79/118], D_loss: 0.7760, G_loss: 1.8203\n",
      "Epoch [11/40], Step [80/118], D_loss: 1.0441, G_loss: 0.8169\n",
      "Epoch [11/40], Step [81/118], D_loss: 1.0956, G_loss: 1.1308\n",
      "Epoch [11/40], Step [82/118], D_loss: 1.3860, G_loss: 1.2802\n",
      "Epoch [11/40], Step [83/118], D_loss: 1.0118, G_loss: 0.8203\n",
      "Epoch [11/40], Step [84/118], D_loss: 1.7965, G_loss: 0.8292\n",
      "Epoch [11/40], Step [85/118], D_loss: 1.7861, G_loss: 0.9973\n",
      "Epoch [11/40], Step [86/118], D_loss: 1.7108, G_loss: 0.8072\n",
      "Epoch [11/40], Step [87/118], D_loss: 0.7417, G_loss: 1.1541\n",
      "Epoch [11/40], Step [88/118], D_loss: 0.7367, G_loss: 1.5962\n",
      "Epoch [11/40], Step [89/118], D_loss: 0.8341, G_loss: 1.6098\n",
      "Epoch [11/40], Step [90/118], D_loss: 0.4894, G_loss: 2.3173\n",
      "Epoch [11/40], Step [91/118], D_loss: 0.9763, G_loss: 0.9779\n",
      "Epoch [11/40], Step [92/118], D_loss: 1.0646, G_loss: 0.8241\n",
      "Epoch [11/40], Step [93/118], D_loss: 0.7389, G_loss: 1.5990\n",
      "Epoch [11/40], Step [94/118], D_loss: 0.7209, G_loss: 1.7250\n",
      "Epoch [11/40], Step [95/118], D_loss: 1.6165, G_loss: 0.7243\n",
      "Epoch [11/40], Step [96/118], D_loss: 1.0632, G_loss: 1.1055\n",
      "Epoch [11/40], Step [97/118], D_loss: 0.5074, G_loss: 2.4198\n",
      "Epoch [11/40], Step [98/118], D_loss: 1.9527, G_loss: 0.4763\n",
      "Epoch [11/40], Step [99/118], D_loss: 0.9878, G_loss: 0.9736\n",
      "Epoch [11/40], Step [100/118], D_loss: 0.7126, G_loss: 1.6284\n",
      "Epoch [11/40], Step [101/118], D_loss: 0.9617, G_loss: 1.4981\n",
      "Epoch [11/40], Step [102/118], D_loss: 1.0710, G_loss: 0.8140\n",
      "Epoch [11/40], Step [103/118], D_loss: 0.8571, G_loss: 1.0663\n",
      "Epoch [11/40], Step [104/118], D_loss: 0.8919, G_loss: 1.0811\n",
      "Epoch [11/40], Step [105/118], D_loss: 0.7241, G_loss: 1.1740\n",
      "Epoch [11/40], Step [106/118], D_loss: 0.7283, G_loss: 1.2738\n",
      "Epoch [11/40], Step [107/118], D_loss: 0.4124, G_loss: 2.2006\n",
      "Epoch [11/40], Step [108/118], D_loss: 0.7979, G_loss: 1.3518\n",
      "Epoch [11/40], Step [109/118], D_loss: 0.7679, G_loss: 1.7295\n",
      "Epoch [11/40], Step [110/118], D_loss: 1.1228, G_loss: 1.8933\n",
      "Epoch [11/40], Step [111/118], D_loss: 0.8182, G_loss: 0.9182\n",
      "Epoch [11/40], Step [112/118], D_loss: 0.7314, G_loss: 1.1324\n",
      "Epoch [11/40], Step [113/118], D_loss: 0.5184, G_loss: 1.5521\n",
      "Epoch [11/40], Step [114/118], D_loss: 0.9872, G_loss: 0.9088\n",
      "Epoch [11/40], Step [115/118], D_loss: 0.4348, G_loss: 1.8625\n",
      "Epoch [11/40], Step [116/118], D_loss: 1.0475, G_loss: 1.2063\n",
      "Epoch [11/40], Step [117/118], D_loss: 0.5854, G_loss: 2.0995\n",
      "Epoch [11/40], Step [118/118], D_loss: 0.5614, G_loss: 1.6868\n",
      "Epoch [12/40], Step [1/118], D_loss: 1.0302, G_loss: 1.1938\n",
      "Epoch [12/40], Step [2/118], D_loss: 0.2110, G_loss: 2.5117\n",
      "Epoch [12/40], Step [3/118], D_loss: 0.5785, G_loss: 1.6263\n",
      "Epoch [12/40], Step [4/118], D_loss: 1.0230, G_loss: 0.8568\n",
      "Epoch [12/40], Step [5/118], D_loss: 0.6363, G_loss: 1.2031\n",
      "Epoch [12/40], Step [6/118], D_loss: 1.3503, G_loss: 0.6737\n",
      "Epoch [12/40], Step [7/118], D_loss: 0.7419, G_loss: 1.1736\n",
      "Epoch [12/40], Step [8/118], D_loss: 0.8881, G_loss: 1.5603\n",
      "Epoch [12/40], Step [9/118], D_loss: 1.3304, G_loss: 1.1629\n",
      "Epoch [12/40], Step [10/118], D_loss: 0.6486, G_loss: 1.1934\n",
      "Epoch [12/40], Step [11/118], D_loss: 0.2944, G_loss: 1.7033\n",
      "Epoch [12/40], Step [12/118], D_loss: 1.2085, G_loss: 1.4448\n",
      "Epoch [12/40], Step [13/118], D_loss: 0.9810, G_loss: 1.8300\n",
      "Epoch [12/40], Step [14/118], D_loss: 1.3637, G_loss: 1.0407\n",
      "Epoch [12/40], Step [15/118], D_loss: 1.4760, G_loss: 0.5848\n",
      "Epoch [12/40], Step [16/118], D_loss: 0.9791, G_loss: 1.6544\n",
      "Epoch [12/40], Step [17/118], D_loss: 0.7421, G_loss: 1.9283\n",
      "Epoch [12/40], Step [18/118], D_loss: 1.1577, G_loss: 2.0089\n",
      "Epoch [12/40], Step [19/118], D_loss: 0.9640, G_loss: 0.7223\n",
      "Epoch [12/40], Step [20/118], D_loss: 0.7393, G_loss: 1.3037\n",
      "Epoch [12/40], Step [21/118], D_loss: 1.2357, G_loss: 1.3275\n",
      "Epoch [12/40], Step [22/118], D_loss: 0.4159, G_loss: 1.6590\n",
      "Epoch [12/40], Step [23/118], D_loss: 1.8524, G_loss: 0.7102\n",
      "Epoch [12/40], Step [24/118], D_loss: 0.6627, G_loss: 1.9318\n",
      "Epoch [12/40], Step [25/118], D_loss: 0.8583, G_loss: 1.3923\n",
      "Epoch [12/40], Step [26/118], D_loss: 0.3015, G_loss: 1.9101\n",
      "Epoch [12/40], Step [27/118], D_loss: 0.8483, G_loss: 1.3643\n",
      "Epoch [12/40], Step [28/118], D_loss: 0.2846, G_loss: 1.6494\n",
      "Epoch [12/40], Step [29/118], D_loss: 0.9028, G_loss: 1.2185\n",
      "Epoch [12/40], Step [30/118], D_loss: 0.4369, G_loss: 2.0079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/40], Step [31/118], D_loss: 0.9823, G_loss: 1.1280\n",
      "Epoch [12/40], Step [32/118], D_loss: 1.3743, G_loss: 0.9319\n",
      "Epoch [12/40], Step [33/118], D_loss: 1.6001, G_loss: 1.3721\n",
      "Epoch [12/40], Step [34/118], D_loss: 0.3171, G_loss: 1.4310\n",
      "Epoch [12/40], Step [35/118], D_loss: 0.9019, G_loss: 1.0803\n",
      "Epoch [12/40], Step [36/118], D_loss: 0.4722, G_loss: 1.8121\n",
      "Epoch [12/40], Step [37/118], D_loss: 0.9069, G_loss: 1.1999\n",
      "Epoch [12/40], Step [38/118], D_loss: 1.0772, G_loss: 0.9665\n",
      "Epoch [12/40], Step [39/118], D_loss: 0.2034, G_loss: 2.1337\n",
      "Epoch [12/40], Step [40/118], D_loss: 1.0471, G_loss: 1.0481\n",
      "Epoch [12/40], Step [41/118], D_loss: 0.5102, G_loss: 1.7833\n",
      "Epoch [12/40], Step [42/118], D_loss: 0.6734, G_loss: 1.3221\n",
      "Epoch [12/40], Step [43/118], D_loss: 1.1856, G_loss: 0.8409\n",
      "Epoch [12/40], Step [44/118], D_loss: 2.2512, G_loss: 0.5605\n",
      "Epoch [12/40], Step [45/118], D_loss: 1.5034, G_loss: 1.0786\n",
      "Epoch [12/40], Step [46/118], D_loss: 0.9929, G_loss: 1.2400\n",
      "Epoch [12/40], Step [47/118], D_loss: 0.4303, G_loss: 1.5227\n",
      "Epoch [12/40], Step [48/118], D_loss: 0.9807, G_loss: 0.8512\n",
      "Epoch [12/40], Step [49/118], D_loss: 1.0683, G_loss: 1.0688\n",
      "Epoch [12/40], Step [50/118], D_loss: 1.2626, G_loss: 1.3018\n",
      "Epoch [12/40], Step [51/118], D_loss: 1.1076, G_loss: 2.2863\n",
      "Epoch [12/40], Step [52/118], D_loss: 1.3424, G_loss: 0.8572\n",
      "Epoch [12/40], Step [53/118], D_loss: 0.7852, G_loss: 1.8543\n",
      "Epoch [12/40], Step [54/118], D_loss: 0.5497, G_loss: 1.6285\n",
      "Epoch [12/40], Step [55/118], D_loss: 0.4015, G_loss: 1.4624\n",
      "Epoch [12/40], Step [56/118], D_loss: 0.7815, G_loss: 1.1730\n",
      "Epoch [12/40], Step [57/118], D_loss: 0.9008, G_loss: 1.2586\n",
      "Epoch [12/40], Step [58/118], D_loss: 0.7236, G_loss: 1.7438\n",
      "Epoch [12/40], Step [59/118], D_loss: 1.3337, G_loss: 1.9874\n",
      "Epoch [12/40], Step [60/118], D_loss: 1.0690, G_loss: 0.6001\n",
      "Epoch [12/40], Step [61/118], D_loss: 0.5133, G_loss: 1.4085\n",
      "Epoch [12/40], Step [62/118], D_loss: 1.4812, G_loss: 1.2719\n",
      "Epoch [12/40], Step [63/118], D_loss: 0.7039, G_loss: 2.1404\n",
      "Epoch [12/40], Step [64/118], D_loss: 0.9749, G_loss: 1.4770\n",
      "Epoch [12/40], Step [65/118], D_loss: 1.4437, G_loss: 0.5711\n",
      "Epoch [12/40], Step [66/118], D_loss: 0.6953, G_loss: 1.3761\n",
      "Epoch [12/40], Step [67/118], D_loss: 0.9939, G_loss: 1.3591\n",
      "Epoch [12/40], Step [68/118], D_loss: 0.9021, G_loss: 0.8596\n",
      "Epoch [12/40], Step [69/118], D_loss: 1.5545, G_loss: 0.6371\n",
      "Epoch [12/40], Step [70/118], D_loss: 0.9381, G_loss: 1.5037\n",
      "Epoch [12/40], Step [71/118], D_loss: 0.5036, G_loss: 2.3589\n",
      "Epoch [12/40], Step [72/118], D_loss: 0.7811, G_loss: 1.6942\n",
      "Epoch [12/40], Step [73/118], D_loss: 0.8695, G_loss: 0.8230\n",
      "Epoch [12/40], Step [74/118], D_loss: 1.0801, G_loss: 1.2026\n",
      "Epoch [12/40], Step [75/118], D_loss: 0.6716, G_loss: 1.8969\n",
      "Epoch [12/40], Step [76/118], D_loss: 1.2953, G_loss: 1.5230\n",
      "Epoch [12/40], Step [77/118], D_loss: 0.6003, G_loss: 0.9972\n",
      "Epoch [12/40], Step [78/118], D_loss: 1.4465, G_loss: 1.0620\n",
      "Epoch [12/40], Step [79/118], D_loss: 1.2917, G_loss: 1.2786\n",
      "Epoch [12/40], Step [80/118], D_loss: 0.6145, G_loss: 1.6167\n",
      "Epoch [12/40], Step [81/118], D_loss: 0.8588, G_loss: 0.9802\n",
      "Epoch [12/40], Step [82/118], D_loss: 0.9111, G_loss: 1.1845\n",
      "Epoch [12/40], Step [83/118], D_loss: 0.3780, G_loss: 2.0965\n",
      "Epoch [12/40], Step [84/118], D_loss: 1.0841, G_loss: 1.8693\n",
      "Epoch [12/40], Step [85/118], D_loss: 0.4246, G_loss: 1.8122\n",
      "Epoch [12/40], Step [86/118], D_loss: 1.1483, G_loss: 0.7896\n",
      "Epoch [12/40], Step [87/118], D_loss: 1.3405, G_loss: 1.0359\n",
      "Epoch [12/40], Step [88/118], D_loss: 0.6988, G_loss: 1.7163\n",
      "Epoch [12/40], Step [89/118], D_loss: 1.2883, G_loss: 0.8820\n",
      "Epoch [12/40], Step [90/118], D_loss: 0.9862, G_loss: 1.2010\n",
      "Epoch [12/40], Step [91/118], D_loss: 0.8923, G_loss: 1.5477\n",
      "Epoch [12/40], Step [92/118], D_loss: 0.7714, G_loss: 1.0250\n",
      "Epoch [12/40], Step [93/118], D_loss: 1.1035, G_loss: 0.7789\n",
      "Epoch [12/40], Step [94/118], D_loss: 0.6151, G_loss: 1.6766\n",
      "Epoch [12/40], Step [95/118], D_loss: 0.4064, G_loss: 2.3507\n",
      "Epoch [12/40], Step [96/118], D_loss: 1.3858, G_loss: 1.0534\n",
      "Epoch [12/40], Step [97/118], D_loss: 1.3107, G_loss: 1.4883\n",
      "Epoch [12/40], Step [98/118], D_loss: 0.3380, G_loss: 1.5949\n",
      "Epoch [12/40], Step [99/118], D_loss: 0.6819, G_loss: 1.2938\n",
      "Epoch [12/40], Step [100/118], D_loss: 1.1472, G_loss: 1.0277\n",
      "Epoch [12/40], Step [101/118], D_loss: 1.4132, G_loss: 1.0573\n",
      "Epoch [12/40], Step [102/118], D_loss: 1.5840, G_loss: 0.7039\n",
      "Epoch [12/40], Step [103/118], D_loss: 1.0464, G_loss: 1.2311\n",
      "Epoch [12/40], Step [104/118], D_loss: 1.1440, G_loss: 1.3788\n",
      "Epoch [12/40], Step [105/118], D_loss: 1.2650, G_loss: 0.7874\n",
      "Epoch [12/40], Step [106/118], D_loss: 0.5169, G_loss: 1.6592\n",
      "Epoch [12/40], Step [107/118], D_loss: 1.8052, G_loss: 0.6008\n",
      "Epoch [12/40], Step [108/118], D_loss: 0.3543, G_loss: 1.9782\n",
      "Epoch [12/40], Step [109/118], D_loss: 0.5944, G_loss: 1.7054\n",
      "Epoch [12/40], Step [110/118], D_loss: 0.8064, G_loss: 1.4961\n",
      "Epoch [12/40], Step [111/118], D_loss: 0.5680, G_loss: 1.8237\n",
      "Epoch [12/40], Step [112/118], D_loss: 0.5538, G_loss: 1.2697\n",
      "Epoch [12/40], Step [113/118], D_loss: 1.0814, G_loss: 1.2460\n",
      "Epoch [12/40], Step [114/118], D_loss: 1.2248, G_loss: 1.3632\n",
      "Epoch [12/40], Step [115/118], D_loss: 0.6325, G_loss: 2.2368\n",
      "Epoch [12/40], Step [116/118], D_loss: 1.0587, G_loss: 0.9260\n",
      "Epoch [12/40], Step [117/118], D_loss: 0.7255, G_loss: 1.8891\n",
      "Epoch [12/40], Step [118/118], D_loss: 1.3001, G_loss: 0.5966\n",
      "Epoch [13/40], Step [1/118], D_loss: 1.2467, G_loss: 0.8908\n",
      "Epoch [13/40], Step [2/118], D_loss: 1.5310, G_loss: 1.1833\n",
      "Epoch [13/40], Step [3/118], D_loss: 1.3611, G_loss: 1.3901\n",
      "Epoch [13/40], Step [4/118], D_loss: 1.2130, G_loss: 0.7066\n",
      "Epoch [13/40], Step [5/118], D_loss: 0.9762, G_loss: 0.8036\n",
      "Epoch [13/40], Step [6/118], D_loss: 1.4103, G_loss: 0.8660\n",
      "Epoch [13/40], Step [7/118], D_loss: 0.3978, G_loss: 2.2197\n",
      "Epoch [13/40], Step [8/118], D_loss: 0.8303, G_loss: 1.3323\n",
      "Epoch [13/40], Step [9/118], D_loss: 0.7425, G_loss: 1.1364\n",
      "Epoch [13/40], Step [10/118], D_loss: 0.7760, G_loss: 1.4060\n",
      "Epoch [13/40], Step [11/118], D_loss: 0.8306, G_loss: 1.4450\n",
      "Epoch [13/40], Step [12/118], D_loss: 1.2049, G_loss: 0.9919\n",
      "Epoch [13/40], Step [13/118], D_loss: 0.5877, G_loss: 1.2600\n",
      "Epoch [13/40], Step [14/118], D_loss: 1.2976, G_loss: 0.7168\n",
      "Epoch [13/40], Step [15/118], D_loss: 0.6042, G_loss: 1.6113\n",
      "Epoch [13/40], Step [16/118], D_loss: 1.5970, G_loss: 0.7542\n",
      "Epoch [13/40], Step [17/118], D_loss: 0.6952, G_loss: 1.1775\n",
      "Epoch [13/40], Step [18/118], D_loss: 0.6249, G_loss: 1.5324\n",
      "Epoch [13/40], Step [19/118], D_loss: 0.9937, G_loss: 1.2962\n",
      "Epoch [13/40], Step [20/118], D_loss: 0.7254, G_loss: 1.1888\n",
      "Epoch [13/40], Step [21/118], D_loss: 1.3338, G_loss: 0.9392\n",
      "Epoch [13/40], Step [22/118], D_loss: 0.7392, G_loss: 1.6371\n",
      "Epoch [13/40], Step [23/118], D_loss: 0.6665, G_loss: 1.5178\n",
      "Epoch [13/40], Step [24/118], D_loss: 0.5228, G_loss: 1.2719\n",
      "Epoch [13/40], Step [25/118], D_loss: 0.4128, G_loss: 1.7080\n",
      "Epoch [13/40], Step [26/118], D_loss: 2.3778, G_loss: 0.7872\n",
      "Epoch [13/40], Step [27/118], D_loss: 1.5834, G_loss: 2.5944\n",
      "Epoch [13/40], Step [28/118], D_loss: 0.7417, G_loss: 1.1971\n",
      "Epoch [13/40], Step [29/118], D_loss: 0.5815, G_loss: 1.0238\n",
      "Epoch [13/40], Step [30/118], D_loss: 0.3778, G_loss: 1.8931\n",
      "Epoch [13/40], Step [31/118], D_loss: 0.9761, G_loss: 1.0801\n",
      "Epoch [13/40], Step [32/118], D_loss: 1.0446, G_loss: 1.2792\n",
      "Epoch [13/40], Step [33/118], D_loss: 0.7136, G_loss: 1.6858\n",
      "Epoch [13/40], Step [34/118], D_loss: 2.5734, G_loss: 1.1119\n",
      "Epoch [13/40], Step [35/118], D_loss: 1.1539, G_loss: 0.4954\n",
      "Epoch [13/40], Step [36/118], D_loss: 1.9070, G_loss: 0.6711\n",
      "Epoch [13/40], Step [37/118], D_loss: 1.0332, G_loss: 1.5415\n",
      "Epoch [13/40], Step [38/118], D_loss: 0.9716, G_loss: 1.4075\n",
      "Epoch [13/40], Step [39/118], D_loss: 0.8353, G_loss: 1.2436\n",
      "Epoch [13/40], Step [40/118], D_loss: 1.3231, G_loss: 0.7641\n",
      "Epoch [13/40], Step [41/118], D_loss: 1.8030, G_loss: 0.7045\n",
      "Epoch [13/40], Step [42/118], D_loss: 0.7293, G_loss: 2.4051\n",
      "Epoch [13/40], Step [43/118], D_loss: 0.5638, G_loss: 1.2887\n",
      "Epoch [13/40], Step [44/118], D_loss: 1.6387, G_loss: 0.6733\n",
      "Epoch [13/40], Step [45/118], D_loss: 1.7877, G_loss: 0.7617\n",
      "Epoch [13/40], Step [46/118], D_loss: 1.0573, G_loss: 1.3723\n",
      "Epoch [13/40], Step [47/118], D_loss: 1.4038, G_loss: 0.8055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/40], Step [48/118], D_loss: 0.4687, G_loss: 1.3195\n",
      "Epoch [13/40], Step [49/118], D_loss: 1.1117, G_loss: 0.9528\n",
      "Epoch [13/40], Step [50/118], D_loss: 0.2575, G_loss: 2.1939\n",
      "Epoch [13/40], Step [51/118], D_loss: 0.6084, G_loss: 1.4093\n",
      "Epoch [13/40], Step [52/118], D_loss: 1.2477, G_loss: 0.9458\n",
      "Epoch [13/40], Step [53/118], D_loss: 0.7878, G_loss: 1.2451\n",
      "Epoch [13/40], Step [54/118], D_loss: 1.4498, G_loss: 0.7829\n",
      "Epoch [13/40], Step [55/118], D_loss: 1.0610, G_loss: 1.3346\n",
      "Epoch [13/40], Step [56/118], D_loss: 1.2044, G_loss: 1.8360\n",
      "Epoch [13/40], Step [57/118], D_loss: 0.3208, G_loss: 1.8002\n",
      "Epoch [13/40], Step [58/118], D_loss: 0.4004, G_loss: 1.5499\n",
      "Epoch [13/40], Step [59/118], D_loss: 0.5481, G_loss: 1.4959\n",
      "Epoch [13/40], Step [60/118], D_loss: 0.6470, G_loss: 1.4256\n",
      "Epoch [13/40], Step [61/118], D_loss: 0.9800, G_loss: 1.2129\n",
      "Epoch [13/40], Step [62/118], D_loss: 0.2739, G_loss: 2.8232\n",
      "Epoch [13/40], Step [63/118], D_loss: 0.8838, G_loss: 1.7463\n",
      "Epoch [13/40], Step [64/118], D_loss: 0.5639, G_loss: 1.1198\n",
      "Epoch [13/40], Step [65/118], D_loss: 0.5796, G_loss: 1.5279\n",
      "Epoch [13/40], Step [66/118], D_loss: 0.2311, G_loss: 2.9256\n",
      "Epoch [13/40], Step [67/118], D_loss: 0.8295, G_loss: 1.2390\n",
      "Epoch [13/40], Step [68/118], D_loss: 1.1270, G_loss: 0.9567\n",
      "Epoch [13/40], Step [69/118], D_loss: 1.5636, G_loss: 0.6913\n",
      "Epoch [13/40], Step [70/118], D_loss: 0.7383, G_loss: 1.4867\n",
      "Epoch [13/40], Step [71/118], D_loss: 1.1839, G_loss: 0.8837\n",
      "Epoch [13/40], Step [72/118], D_loss: 1.1972, G_loss: 0.8614\n",
      "Epoch [13/40], Step [73/118], D_loss: 0.5643, G_loss: 1.3387\n",
      "Epoch [13/40], Step [74/118], D_loss: 1.3954, G_loss: 0.9886\n",
      "Epoch [13/40], Step [75/118], D_loss: 0.7816, G_loss: 1.7471\n",
      "Epoch [13/40], Step [76/118], D_loss: 1.2159, G_loss: 0.8523\n",
      "Epoch [13/40], Step [77/118], D_loss: 0.6039, G_loss: 1.2703\n",
      "Epoch [13/40], Step [78/118], D_loss: 1.0640, G_loss: 1.0276\n",
      "Epoch [13/40], Step [79/118], D_loss: 0.5767, G_loss: 1.6897\n",
      "Epoch [13/40], Step [80/118], D_loss: 1.0075, G_loss: 1.1197\n",
      "Epoch [13/40], Step [81/118], D_loss: 0.6040, G_loss: 1.2097\n",
      "Epoch [13/40], Step [82/118], D_loss: 1.1676, G_loss: 0.8891\n",
      "Epoch [13/40], Step [83/118], D_loss: 0.3308, G_loss: 1.6175\n",
      "Epoch [13/40], Step [84/118], D_loss: 1.4218, G_loss: 0.8676\n",
      "Epoch [13/40], Step [85/118], D_loss: 0.5053, G_loss: 1.9585\n",
      "Epoch [13/40], Step [86/118], D_loss: 1.2322, G_loss: 1.3421\n",
      "Epoch [13/40], Step [87/118], D_loss: 1.2126, G_loss: 1.7601\n",
      "Epoch [13/40], Step [88/118], D_loss: 0.4602, G_loss: 1.4508\n",
      "Epoch [13/40], Step [89/118], D_loss: 0.5904, G_loss: 1.2731\n",
      "Epoch [13/40], Step [90/118], D_loss: 0.6911, G_loss: 1.3765\n",
      "Epoch [13/40], Step [91/118], D_loss: 1.0543, G_loss: 1.0455\n",
      "Epoch [13/40], Step [92/118], D_loss: 0.6674, G_loss: 1.2478\n",
      "Epoch [13/40], Step [93/118], D_loss: 0.3175, G_loss: 2.1887\n",
      "Epoch [13/40], Step [94/118], D_loss: 0.4213, G_loss: 1.8896\n",
      "Epoch [13/40], Step [95/118], D_loss: 0.1987, G_loss: 3.2064\n",
      "Epoch [13/40], Step [96/118], D_loss: 1.9993, G_loss: 0.6008\n",
      "Epoch [13/40], Step [97/118], D_loss: 0.2885, G_loss: 3.3158\n",
      "Epoch [13/40], Step [98/118], D_loss: 1.3866, G_loss: 1.0075\n",
      "Epoch [13/40], Step [99/118], D_loss: 0.7453, G_loss: 1.8246\n",
      "Epoch [13/40], Step [100/118], D_loss: 1.5551, G_loss: 1.3544\n",
      "Epoch [13/40], Step [101/118], D_loss: 0.7461, G_loss: 0.8144\n",
      "Epoch [13/40], Step [102/118], D_loss: 0.8777, G_loss: 1.1378\n",
      "Epoch [13/40], Step [103/118], D_loss: 1.3634, G_loss: 0.9562\n",
      "Epoch [13/40], Step [104/118], D_loss: 0.9755, G_loss: 1.2893\n",
      "Epoch [13/40], Step [105/118], D_loss: 0.8212, G_loss: 1.1673\n",
      "Epoch [13/40], Step [106/118], D_loss: 0.8763, G_loss: 1.1837\n",
      "Epoch [13/40], Step [107/118], D_loss: 0.5774, G_loss: 1.6258\n",
      "Epoch [13/40], Step [108/118], D_loss: 1.1331, G_loss: 0.8393\n",
      "Epoch [13/40], Step [109/118], D_loss: 0.7126, G_loss: 1.1581\n",
      "Epoch [13/40], Step [110/118], D_loss: 1.6761, G_loss: 0.9998\n",
      "Epoch [13/40], Step [111/118], D_loss: 0.8904, G_loss: 2.2667\n",
      "Epoch [13/40], Step [112/118], D_loss: 1.1838, G_loss: 0.6837\n",
      "Epoch [13/40], Step [113/118], D_loss: 0.4987, G_loss: 1.2359\n",
      "Epoch [13/40], Step [114/118], D_loss: 0.8959, G_loss: 1.3115\n",
      "Epoch [13/40], Step [115/118], D_loss: 0.2346, G_loss: 2.6260\n",
      "Epoch [13/40], Step [116/118], D_loss: 0.7852, G_loss: 1.6045\n",
      "Epoch [13/40], Step [117/118], D_loss: 0.8118, G_loss: 1.0897\n",
      "Epoch [13/40], Step [118/118], D_loss: 2.0228, G_loss: 0.7454\n",
      "Epoch [14/40], Step [1/118], D_loss: 2.6558, G_loss: 0.7766\n",
      "Epoch [14/40], Step [2/118], D_loss: 0.4110, G_loss: 1.6769\n",
      "Epoch [14/40], Step [3/118], D_loss: 0.5487, G_loss: 1.2623\n",
      "Epoch [14/40], Step [4/118], D_loss: 0.8431, G_loss: 1.2382\n",
      "Epoch [14/40], Step [5/118], D_loss: 0.7332, G_loss: 1.5208\n",
      "Epoch [14/40], Step [6/118], D_loss: 0.6127, G_loss: 1.5337\n",
      "Epoch [14/40], Step [7/118], D_loss: 0.8138, G_loss: 1.6437\n",
      "Epoch [14/40], Step [8/118], D_loss: 0.9401, G_loss: 0.9000\n",
      "Epoch [14/40], Step [9/118], D_loss: 0.3972, G_loss: 1.6726\n",
      "Epoch [14/40], Step [10/118], D_loss: 0.9174, G_loss: 1.0661\n",
      "Epoch [14/40], Step [11/118], D_loss: 1.3614, G_loss: 0.9750\n",
      "Epoch [14/40], Step [12/118], D_loss: 0.5455, G_loss: 2.4356\n",
      "Epoch [14/40], Step [13/118], D_loss: 0.2312, G_loss: 2.6046\n",
      "Epoch [14/40], Step [14/118], D_loss: 0.7789, G_loss: 1.1444\n",
      "Epoch [14/40], Step [15/118], D_loss: 1.1339, G_loss: 1.1972\n",
      "Epoch [14/40], Step [16/118], D_loss: 1.0908, G_loss: 1.3434\n",
      "Epoch [14/40], Step [17/118], D_loss: 0.3905, G_loss: 1.5514\n",
      "Epoch [14/40], Step [18/118], D_loss: 0.8640, G_loss: 1.0760\n",
      "Epoch [14/40], Step [19/118], D_loss: 0.3180, G_loss: 1.9689\n",
      "Epoch [14/40], Step [20/118], D_loss: 2.1043, G_loss: 0.4197\n",
      "Epoch [14/40], Step [21/118], D_loss: 0.7755, G_loss: 1.1885\n",
      "Epoch [14/40], Step [22/118], D_loss: 0.9807, G_loss: 1.1254\n",
      "Epoch [14/40], Step [23/118], D_loss: 1.3367, G_loss: 1.1091\n",
      "Epoch [14/40], Step [24/118], D_loss: 0.5927, G_loss: 2.0349\n",
      "Epoch [14/40], Step [25/118], D_loss: 0.8645, G_loss: 1.4963\n",
      "Epoch [14/40], Step [26/118], D_loss: 1.5547, G_loss: 0.5600\n",
      "Epoch [14/40], Step [27/118], D_loss: 0.5084, G_loss: 1.6358\n",
      "Epoch [14/40], Step [28/118], D_loss: 0.4193, G_loss: 1.9667\n",
      "Epoch [14/40], Step [29/118], D_loss: 0.8406, G_loss: 1.2605\n",
      "Epoch [14/40], Step [30/118], D_loss: 0.7517, G_loss: 1.2506\n",
      "Epoch [14/40], Step [31/118], D_loss: 0.6422, G_loss: 1.1228\n",
      "Epoch [14/40], Step [32/118], D_loss: 1.0025, G_loss: 1.0768\n",
      "Epoch [14/40], Step [33/118], D_loss: 0.5889, G_loss: 1.6606\n",
      "Epoch [14/40], Step [34/118], D_loss: 0.9527, G_loss: 1.4231\n",
      "Epoch [14/40], Step [35/118], D_loss: 1.0821, G_loss: 1.0564\n",
      "Epoch [14/40], Step [36/118], D_loss: 0.6772, G_loss: 1.7019\n",
      "Epoch [14/40], Step [37/118], D_loss: 1.2229, G_loss: 0.8887\n",
      "Epoch [14/40], Step [38/118], D_loss: 0.4240, G_loss: 1.3351\n",
      "Epoch [14/40], Step [39/118], D_loss: 0.8347, G_loss: 1.1023\n",
      "Epoch [14/40], Step [40/118], D_loss: 1.0333, G_loss: 1.1699\n",
      "Epoch [14/40], Step [41/118], D_loss: 0.2895, G_loss: 2.4356\n",
      "Epoch [14/40], Step [42/118], D_loss: 0.5498, G_loss: 2.1046\n",
      "Epoch [14/40], Step [43/118], D_loss: 0.6821, G_loss: 1.1911\n",
      "Epoch [14/40], Step [44/118], D_loss: 1.0973, G_loss: 0.9902\n",
      "Epoch [14/40], Step [45/118], D_loss: 0.5153, G_loss: 1.7937\n",
      "Epoch [14/40], Step [46/118], D_loss: 0.5120, G_loss: 1.9811\n",
      "Epoch [14/40], Step [47/118], D_loss: 0.4624, G_loss: 1.3540\n",
      "Epoch [14/40], Step [48/118], D_loss: 1.6144, G_loss: 1.0648\n",
      "Epoch [14/40], Step [49/118], D_loss: 0.6906, G_loss: 2.2124\n",
      "Epoch [14/40], Step [50/118], D_loss: 1.2693, G_loss: 1.9768\n",
      "Epoch [14/40], Step [51/118], D_loss: 0.5878, G_loss: 1.0701\n",
      "Epoch [14/40], Step [52/118], D_loss: 0.8487, G_loss: 0.8229\n",
      "Epoch [14/40], Step [53/118], D_loss: 1.0501, G_loss: 1.1691\n",
      "Epoch [14/40], Step [54/118], D_loss: 1.3911, G_loss: 1.2212\n",
      "Epoch [14/40], Step [55/118], D_loss: 0.6570, G_loss: 1.8625\n",
      "Epoch [14/40], Step [56/118], D_loss: 0.4803, G_loss: 2.2684\n",
      "Epoch [14/40], Step [57/118], D_loss: 0.3369, G_loss: 1.7902\n",
      "Epoch [14/40], Step [58/118], D_loss: 0.7531, G_loss: 1.1971\n",
      "Epoch [14/40], Step [59/118], D_loss: 0.2736, G_loss: 2.1829\n",
      "Epoch [14/40], Step [60/118], D_loss: 0.4227, G_loss: 1.6209\n",
      "Epoch [14/40], Step [61/118], D_loss: 1.4151, G_loss: 0.9528\n",
      "Epoch [14/40], Step [62/118], D_loss: 1.0911, G_loss: 1.2250\n",
      "Epoch [14/40], Step [63/118], D_loss: 1.1070, G_loss: 1.0827\n",
      "Epoch [14/40], Step [64/118], D_loss: 0.5501, G_loss: 1.9122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/40], Step [65/118], D_loss: 0.4038, G_loss: 1.9733\n",
      "Epoch [14/40], Step [66/118], D_loss: 0.8857, G_loss: 1.1861\n",
      "Epoch [14/40], Step [67/118], D_loss: 0.2822, G_loss: 2.1731\n",
      "Epoch [14/40], Step [68/118], D_loss: 1.2227, G_loss: 1.1983\n",
      "Epoch [14/40], Step [69/118], D_loss: 1.0871, G_loss: 1.5302\n",
      "Epoch [14/40], Step [70/118], D_loss: 0.9682, G_loss: 1.5405\n",
      "Epoch [14/40], Step [71/118], D_loss: 0.8528, G_loss: 0.8676\n",
      "Epoch [14/40], Step [72/118], D_loss: 0.5954, G_loss: 1.0469\n",
      "Epoch [14/40], Step [73/118], D_loss: 1.4878, G_loss: 1.3791\n",
      "Epoch [14/40], Step [74/118], D_loss: 0.5042, G_loss: 2.5433\n",
      "Epoch [14/40], Step [75/118], D_loss: 1.9316, G_loss: 0.7468\n",
      "Epoch [14/40], Step [76/118], D_loss: 1.0341, G_loss: 0.7334\n",
      "Epoch [14/40], Step [77/118], D_loss: 0.7315, G_loss: 1.2879\n",
      "Epoch [14/40], Step [78/118], D_loss: 0.4106, G_loss: 2.2521\n",
      "Epoch [14/40], Step [79/118], D_loss: 1.1333, G_loss: 0.7832\n",
      "Epoch [14/40], Step [80/118], D_loss: 2.5882, G_loss: 0.5530\n",
      "Epoch [14/40], Step [81/118], D_loss: 0.7234, G_loss: 2.2055\n",
      "Epoch [14/40], Step [82/118], D_loss: 1.3534, G_loss: 0.8225\n",
      "Epoch [14/40], Step [83/118], D_loss: 1.4685, G_loss: 0.6897\n",
      "Epoch [14/40], Step [84/118], D_loss: 0.6653, G_loss: 1.0773\n",
      "Epoch [14/40], Step [85/118], D_loss: 1.3311, G_loss: 0.7642\n",
      "Epoch [14/40], Step [86/118], D_loss: 0.2318, G_loss: 2.2065\n",
      "Epoch [14/40], Step [87/118], D_loss: 0.5019, G_loss: 1.9142\n",
      "Epoch [14/40], Step [88/118], D_loss: 0.4800, G_loss: 2.4655\n",
      "Epoch [14/40], Step [89/118], D_loss: 1.3407, G_loss: 0.6711\n",
      "Epoch [14/40], Step [90/118], D_loss: 0.7722, G_loss: 1.1330\n",
      "Epoch [14/40], Step [91/118], D_loss: 1.5108, G_loss: 0.8804\n",
      "Epoch [14/40], Step [92/118], D_loss: 0.9470, G_loss: 1.9309\n",
      "Epoch [14/40], Step [93/118], D_loss: 0.6691, G_loss: 2.9540\n",
      "Epoch [14/40], Step [94/118], D_loss: 0.9683, G_loss: 1.0892\n",
      "Epoch [14/40], Step [95/118], D_loss: 1.2915, G_loss: 0.9627\n",
      "Epoch [14/40], Step [96/118], D_loss: 0.9810, G_loss: 0.7605\n",
      "Epoch [14/40], Step [97/118], D_loss: 0.3974, G_loss: 1.3917\n",
      "Epoch [14/40], Step [98/118], D_loss: 0.4431, G_loss: 1.6700\n",
      "Epoch [14/40], Step [99/118], D_loss: 1.4095, G_loss: 1.1320\n",
      "Epoch [14/40], Step [100/118], D_loss: 0.5911, G_loss: 2.3610\n",
      "Epoch [14/40], Step [101/118], D_loss: 0.6347, G_loss: 1.4943\n",
      "Epoch [14/40], Step [102/118], D_loss: 1.4460, G_loss: 1.0299\n",
      "Epoch [14/40], Step [103/118], D_loss: 0.4829, G_loss: 1.1651\n",
      "Epoch [14/40], Step [104/118], D_loss: 1.2860, G_loss: 1.0872\n",
      "Epoch [14/40], Step [105/118], D_loss: 0.6309, G_loss: 2.5537\n",
      "Epoch [14/40], Step [106/118], D_loss: 0.7121, G_loss: 1.3966\n",
      "Epoch [14/40], Step [107/118], D_loss: 0.6546, G_loss: 1.6919\n",
      "Epoch [14/40], Step [108/118], D_loss: 1.0218, G_loss: 1.0783\n",
      "Epoch [14/40], Step [109/118], D_loss: 1.0022, G_loss: 0.7718\n",
      "Epoch [14/40], Step [110/118], D_loss: 0.3873, G_loss: 1.7622\n",
      "Epoch [14/40], Step [111/118], D_loss: 0.9889, G_loss: 1.1291\n",
      "Epoch [14/40], Step [112/118], D_loss: 0.7409, G_loss: 1.5439\n",
      "Epoch [14/40], Step [113/118], D_loss: 1.0576, G_loss: 1.1052\n",
      "Epoch [14/40], Step [114/118], D_loss: 0.8252, G_loss: 1.1094\n",
      "Epoch [14/40], Step [115/118], D_loss: 0.8133, G_loss: 1.0240\n",
      "Epoch [14/40], Step [116/118], D_loss: 0.1186, G_loss: 2.7082\n",
      "Epoch [14/40], Step [117/118], D_loss: 0.9530, G_loss: 1.2621\n",
      "Epoch [14/40], Step [118/118], D_loss: 0.5258, G_loss: 2.1786\n",
      "Epoch [15/40], Step [1/118], D_loss: 0.7595, G_loss: 1.7583\n",
      "Epoch [15/40], Step [2/118], D_loss: 0.9504, G_loss: 0.8538\n",
      "Epoch [15/40], Step [3/118], D_loss: 0.8799, G_loss: 1.0045\n",
      "Epoch [15/40], Step [4/118], D_loss: 1.3510, G_loss: 1.3788\n",
      "Epoch [15/40], Step [5/118], D_loss: 0.7521, G_loss: 3.3751\n",
      "Epoch [15/40], Step [6/118], D_loss: 1.0780, G_loss: 0.7628\n",
      "Epoch [15/40], Step [7/118], D_loss: 1.3980, G_loss: 0.5205\n",
      "Epoch [15/40], Step [8/118], D_loss: 1.2412, G_loss: 1.3548\n",
      "Epoch [15/40], Step [9/118], D_loss: 0.5522, G_loss: 2.2320\n",
      "Epoch [15/40], Step [10/118], D_loss: 1.4899, G_loss: 1.3935\n",
      "Epoch [15/40], Step [11/118], D_loss: 0.5189, G_loss: 1.0755\n",
      "Epoch [15/40], Step [12/118], D_loss: 0.4474, G_loss: 1.6879\n",
      "Epoch [15/40], Step [13/118], D_loss: 0.8555, G_loss: 1.3262\n",
      "Epoch [15/40], Step [14/118], D_loss: 0.2173, G_loss: 2.4752\n",
      "Epoch [15/40], Step [15/118], D_loss: 0.6777, G_loss: 1.9705\n",
      "Epoch [15/40], Step [16/118], D_loss: 0.7405, G_loss: 1.1916\n",
      "Epoch [15/40], Step [17/118], D_loss: 0.6085, G_loss: 1.6232\n",
      "Epoch [15/40], Step [18/118], D_loss: 1.1076, G_loss: 1.0842\n",
      "Epoch [15/40], Step [19/118], D_loss: 1.2214, G_loss: 0.6550\n",
      "Epoch [15/40], Step [20/118], D_loss: 1.0178, G_loss: 0.9418\n",
      "Epoch [15/40], Step [21/118], D_loss: 0.6692, G_loss: 1.4734\n",
      "Epoch [15/40], Step [22/118], D_loss: 0.2854, G_loss: 2.4104\n",
      "Epoch [15/40], Step [23/118], D_loss: 1.0399, G_loss: 1.8453\n",
      "Epoch [15/40], Step [24/118], D_loss: 1.1063, G_loss: 0.5912\n",
      "Epoch [15/40], Step [25/118], D_loss: 1.0761, G_loss: 1.3162\n",
      "Epoch [15/40], Step [26/118], D_loss: 0.8837, G_loss: 1.9480\n",
      "Epoch [15/40], Step [27/118], D_loss: 0.7378, G_loss: 3.1864\n",
      "Epoch [15/40], Step [28/118], D_loss: 1.0716, G_loss: 0.8176\n",
      "Epoch [15/40], Step [29/118], D_loss: 0.7746, G_loss: 1.1775\n",
      "Epoch [15/40], Step [30/118], D_loss: 1.4745, G_loss: 0.6955\n",
      "Epoch [15/40], Step [31/118], D_loss: 0.5826, G_loss: 1.8337\n",
      "Epoch [15/40], Step [32/118], D_loss: 0.4506, G_loss: 1.3230\n",
      "Epoch [15/40], Step [33/118], D_loss: 1.8530, G_loss: 0.7711\n",
      "Epoch [15/40], Step [34/118], D_loss: 0.7266, G_loss: 1.8864\n",
      "Epoch [15/40], Step [35/118], D_loss: 1.0245, G_loss: 1.3969\n",
      "Epoch [15/40], Step [36/118], D_loss: 1.2830, G_loss: 1.0868\n",
      "Epoch [15/40], Step [37/118], D_loss: 0.6583, G_loss: 0.9643\n",
      "Epoch [15/40], Step [38/118], D_loss: 1.2956, G_loss: 0.6640\n",
      "Epoch [15/40], Step [39/118], D_loss: 1.2118, G_loss: 1.0545\n",
      "Epoch [15/40], Step [40/118], D_loss: 0.5370, G_loss: 2.3589\n",
      "Epoch [15/40], Step [41/118], D_loss: 1.9070, G_loss: 1.5628\n",
      "Epoch [15/40], Step [42/118], D_loss: 1.3849, G_loss: 0.5935\n",
      "Epoch [15/40], Step [43/118], D_loss: 0.8703, G_loss: 1.3097\n",
      "Epoch [15/40], Step [44/118], D_loss: 1.3175, G_loss: 0.9147\n",
      "Epoch [15/40], Step [45/118], D_loss: 0.9534, G_loss: 1.0337\n",
      "Epoch [15/40], Step [46/118], D_loss: 0.2275, G_loss: 2.8732\n",
      "Epoch [15/40], Step [47/118], D_loss: 0.7327, G_loss: 1.2655\n",
      "Epoch [15/40], Step [48/118], D_loss: 1.4264, G_loss: 0.7707\n",
      "Epoch [15/40], Step [49/118], D_loss: 0.5155, G_loss: 1.7384\n",
      "Epoch [15/40], Step [50/118], D_loss: 0.5296, G_loss: 1.8418\n",
      "Epoch [15/40], Step [51/118], D_loss: 0.6964, G_loss: 1.5292\n",
      "Epoch [15/40], Step [52/118], D_loss: 0.4697, G_loss: 1.3215\n",
      "Epoch [15/40], Step [53/118], D_loss: 1.0527, G_loss: 1.3867\n",
      "Epoch [15/40], Step [54/118], D_loss: 1.4144, G_loss: 1.2875\n",
      "Epoch [15/40], Step [55/118], D_loss: 0.7300, G_loss: 0.9680\n",
      "Epoch [15/40], Step [56/118], D_loss: 0.6843, G_loss: 1.4930\n",
      "Epoch [15/40], Step [57/118], D_loss: 1.1216, G_loss: 1.4922\n",
      "Epoch [15/40], Step [58/118], D_loss: 0.7232, G_loss: 2.3098\n",
      "Epoch [15/40], Step [59/118], D_loss: 0.7062, G_loss: 1.1379\n",
      "Epoch [15/40], Step [60/118], D_loss: 0.4793, G_loss: 1.5027\n",
      "Epoch [15/40], Step [61/118], D_loss: 0.5232, G_loss: 1.5009\n",
      "Epoch [15/40], Step [62/118], D_loss: 1.0227, G_loss: 1.1744\n",
      "Epoch [15/40], Step [63/118], D_loss: 0.8069, G_loss: 1.4840\n",
      "Epoch [15/40], Step [64/118], D_loss: 0.8860, G_loss: 1.4566\n",
      "Epoch [15/40], Step [65/118], D_loss: 1.7287, G_loss: 0.4370\n",
      "Epoch [15/40], Step [66/118], D_loss: 0.4749, G_loss: 1.0139\n",
      "Epoch [15/40], Step [67/118], D_loss: 2.6506, G_loss: 0.5259\n",
      "Epoch [15/40], Step [68/118], D_loss: 1.0978, G_loss: 1.5639\n",
      "Epoch [15/40], Step [69/118], D_loss: 0.6750, G_loss: 2.2698\n",
      "Epoch [15/40], Step [70/118], D_loss: 0.8111, G_loss: 0.9977\n",
      "Epoch [15/40], Step [71/118], D_loss: 0.9935, G_loss: 0.9327\n",
      "Epoch [15/40], Step [72/118], D_loss: 0.5748, G_loss: 1.4814\n",
      "Epoch [15/40], Step [73/118], D_loss: 0.6805, G_loss: 1.2912\n",
      "Epoch [15/40], Step [74/118], D_loss: 0.7293, G_loss: 1.3211\n",
      "Epoch [15/40], Step [75/118], D_loss: 0.6223, G_loss: 1.4423\n",
      "Epoch [15/40], Step [76/118], D_loss: 0.4905, G_loss: 1.9082\n",
      "Epoch [15/40], Step [77/118], D_loss: 0.4390, G_loss: 1.8420\n",
      "Epoch [15/40], Step [78/118], D_loss: 0.7169, G_loss: 1.4102\n",
      "Epoch [15/40], Step [79/118], D_loss: 0.4679, G_loss: 1.6895\n",
      "Epoch [15/40], Step [80/118], D_loss: 1.0699, G_loss: 1.0271\n",
      "Epoch [15/40], Step [81/118], D_loss: 0.8786, G_loss: 0.9198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/40], Step [82/118], D_loss: 0.3638, G_loss: 1.5762\n",
      "Epoch [15/40], Step [83/118], D_loss: 0.6759, G_loss: 1.2878\n",
      "Epoch [15/40], Step [84/118], D_loss: 0.7297, G_loss: 1.6923\n",
      "Epoch [15/40], Step [85/118], D_loss: 1.3073, G_loss: 1.0544\n",
      "Epoch [15/40], Step [86/118], D_loss: 1.2673, G_loss: 0.9410\n",
      "Epoch [15/40], Step [87/118], D_loss: 0.4295, G_loss: 2.9325\n",
      "Epoch [15/40], Step [88/118], D_loss: 0.4350, G_loss: 1.8896\n",
      "Epoch [15/40], Step [89/118], D_loss: 1.4451, G_loss: 1.0395\n",
      "Epoch [15/40], Step [90/118], D_loss: 1.3289, G_loss: 1.1016\n",
      "Epoch [15/40], Step [91/118], D_loss: 0.7791, G_loss: 1.2336\n",
      "Epoch [15/40], Step [92/118], D_loss: 1.9181, G_loss: 0.5819\n",
      "Epoch [15/40], Step [93/118], D_loss: 1.8269, G_loss: 0.9806\n",
      "Epoch [15/40], Step [94/118], D_loss: 0.9857, G_loss: 1.9355\n",
      "Epoch [15/40], Step [95/118], D_loss: 1.4412, G_loss: 0.4432\n",
      "Epoch [15/40], Step [96/118], D_loss: 0.7260, G_loss: 1.2076\n",
      "Epoch [15/40], Step [97/118], D_loss: 0.3818, G_loss: 2.2445\n",
      "Epoch [15/40], Step [98/118], D_loss: 1.4545, G_loss: 1.1711\n",
      "Epoch [15/40], Step [99/118], D_loss: 0.8829, G_loss: 1.6405\n",
      "Epoch [15/40], Step [100/118], D_loss: 1.2751, G_loss: 0.8323\n",
      "Epoch [15/40], Step [101/118], D_loss: 2.5804, G_loss: 0.2810\n",
      "Epoch [15/40], Step [102/118], D_loss: 0.7045, G_loss: 1.5266\n",
      "Epoch [15/40], Step [103/118], D_loss: 1.1763, G_loss: 0.9293\n",
      "Epoch [15/40], Step [104/118], D_loss: 0.6043, G_loss: 1.3382\n",
      "Epoch [15/40], Step [105/118], D_loss: 1.2709, G_loss: 0.9178\n",
      "Epoch [15/40], Step [106/118], D_loss: 0.4475, G_loss: 1.9804\n",
      "Epoch [15/40], Step [107/118], D_loss: 1.9634, G_loss: 0.5879\n",
      "Epoch [15/40], Step [108/118], D_loss: 0.5653, G_loss: 1.5875\n",
      "Epoch [15/40], Step [109/118], D_loss: 0.5592, G_loss: 1.4722\n",
      "Epoch [15/40], Step [110/118], D_loss: 0.9576, G_loss: 1.4435\n",
      "Epoch [15/40], Step [111/118], D_loss: 0.6410, G_loss: 1.0681\n",
      "Epoch [15/40], Step [112/118], D_loss: 0.9400, G_loss: 1.6210\n",
      "Epoch [15/40], Step [113/118], D_loss: 0.4023, G_loss: 3.2045\n",
      "Epoch [15/40], Step [114/118], D_loss: 0.7555, G_loss: 1.7474\n",
      "Epoch [15/40], Step [115/118], D_loss: 0.8118, G_loss: 0.9711\n",
      "Epoch [15/40], Step [116/118], D_loss: 0.3883, G_loss: 1.9306\n",
      "Epoch [15/40], Step [117/118], D_loss: 0.7504, G_loss: 1.8645\n",
      "Epoch [15/40], Step [118/118], D_loss: 0.6850, G_loss: 1.0763\n",
      "Epoch [16/40], Step [1/118], D_loss: 0.7530, G_loss: 1.3621\n",
      "Epoch [16/40], Step [2/118], D_loss: 0.6052, G_loss: 1.8077\n",
      "Epoch [16/40], Step [3/118], D_loss: 0.4537, G_loss: 2.3204\n",
      "Epoch [16/40], Step [4/118], D_loss: 1.1379, G_loss: 0.8976\n",
      "Epoch [16/40], Step [5/118], D_loss: 1.1789, G_loss: 0.8477\n",
      "Epoch [16/40], Step [6/118], D_loss: 0.6263, G_loss: 1.2918\n",
      "Epoch [16/40], Step [7/118], D_loss: 0.7917, G_loss: 1.2130\n",
      "Epoch [16/40], Step [8/118], D_loss: 0.7499, G_loss: 1.5129\n",
      "Epoch [16/40], Step [9/118], D_loss: 1.6309, G_loss: 0.8674\n",
      "Epoch [16/40], Step [10/118], D_loss: 0.5283, G_loss: 2.1396\n",
      "Epoch [16/40], Step [11/118], D_loss: 0.9669, G_loss: 2.0420\n",
      "Epoch [16/40], Step [12/118], D_loss: 0.9645, G_loss: 0.7280\n",
      "Epoch [16/40], Step [13/118], D_loss: 0.7300, G_loss: 1.6607\n",
      "Epoch [16/40], Step [14/118], D_loss: 0.6627, G_loss: 1.9848\n",
      "Epoch [16/40], Step [15/118], D_loss: 1.2264, G_loss: 2.7802\n",
      "Epoch [16/40], Step [16/118], D_loss: 0.5379, G_loss: 1.0863\n",
      "Epoch [16/40], Step [17/118], D_loss: 2.5244, G_loss: 0.3837\n",
      "Epoch [16/40], Step [18/118], D_loss: 0.6381, G_loss: 1.5922\n",
      "Epoch [16/40], Step [19/118], D_loss: 0.8736, G_loss: 1.1847\n",
      "Epoch [16/40], Step [20/118], D_loss: 0.4638, G_loss: 1.9728\n",
      "Epoch [16/40], Step [21/118], D_loss: 0.2914, G_loss: 2.3619\n",
      "Epoch [16/40], Step [22/118], D_loss: 0.5707, G_loss: 1.4940\n",
      "Epoch [16/40], Step [23/118], D_loss: 0.1622, G_loss: 2.3670\n",
      "Epoch [16/40], Step [24/118], D_loss: 0.4532, G_loss: 1.7597\n",
      "Epoch [16/40], Step [25/118], D_loss: 0.6067, G_loss: 1.4237\n",
      "Epoch [16/40], Step [26/118], D_loss: 0.9968, G_loss: 1.2230\n",
      "Epoch [16/40], Step [27/118], D_loss: 0.5887, G_loss: 1.6440\n",
      "Epoch [16/40], Step [28/118], D_loss: 0.6224, G_loss: 1.5356\n",
      "Epoch [16/40], Step [29/118], D_loss: 0.5654, G_loss: 1.6956\n",
      "Epoch [16/40], Step [30/118], D_loss: 0.3753, G_loss: 2.0266\n",
      "Epoch [16/40], Step [31/118], D_loss: 1.1570, G_loss: 0.6294\n",
      "Epoch [16/40], Step [32/118], D_loss: 1.2269, G_loss: 0.9562\n",
      "Epoch [16/40], Step [33/118], D_loss: 0.7430, G_loss: 1.8959\n",
      "Epoch [16/40], Step [34/118], D_loss: 2.2726, G_loss: 2.1561\n",
      "Epoch [16/40], Step [35/118], D_loss: 1.7969, G_loss: 0.6218\n",
      "Epoch [16/40], Step [36/118], D_loss: 0.8252, G_loss: 1.5363\n",
      "Epoch [16/40], Step [37/118], D_loss: 0.6644, G_loss: 1.4102\n",
      "Epoch [16/40], Step [38/118], D_loss: 0.5367, G_loss: 1.6482\n",
      "Epoch [16/40], Step [39/118], D_loss: 0.9513, G_loss: 1.7110\n",
      "Epoch [16/40], Step [40/118], D_loss: 1.4165, G_loss: 1.3402\n",
      "Epoch [16/40], Step [41/118], D_loss: 0.3251, G_loss: 1.5562\n",
      "Epoch [16/40], Step [42/118], D_loss: 0.8157, G_loss: 1.0688\n",
      "Epoch [16/40], Step [43/118], D_loss: 0.3729, G_loss: 1.9495\n",
      "Epoch [16/40], Step [44/118], D_loss: 0.6231, G_loss: 1.4291\n",
      "Epoch [16/40], Step [45/118], D_loss: 2.2644, G_loss: 0.6617\n",
      "Epoch [16/40], Step [46/118], D_loss: 0.5019, G_loss: 2.2334\n",
      "Epoch [16/40], Step [47/118], D_loss: 1.6758, G_loss: 0.5931\n",
      "Epoch [16/40], Step [48/118], D_loss: 0.9908, G_loss: 0.6084\n",
      "Epoch [16/40], Step [49/118], D_loss: 0.8656, G_loss: 1.2820\n",
      "Epoch [16/40], Step [50/118], D_loss: 0.4567, G_loss: 2.1678\n",
      "Epoch [16/40], Step [51/118], D_loss: 0.6086, G_loss: 1.7425\n",
      "Epoch [16/40], Step [52/118], D_loss: 0.3195, G_loss: 3.1380\n",
      "Epoch [16/40], Step [53/118], D_loss: 0.9420, G_loss: 1.1870\n",
      "Epoch [16/40], Step [54/118], D_loss: 1.1620, G_loss: 0.9606\n",
      "Epoch [16/40], Step [55/118], D_loss: 0.6405, G_loss: 1.9158\n",
      "Epoch [16/40], Step [56/118], D_loss: 1.4507, G_loss: 0.5811\n",
      "Epoch [16/40], Step [57/118], D_loss: 0.5082, G_loss: 1.4891\n",
      "Epoch [16/40], Step [58/118], D_loss: 1.3145, G_loss: 1.6973\n",
      "Epoch [16/40], Step [59/118], D_loss: 0.6342, G_loss: 3.5184\n",
      "Epoch [16/40], Step [60/118], D_loss: 0.8708, G_loss: 1.0911\n",
      "Epoch [16/40], Step [61/118], D_loss: 0.3653, G_loss: 1.4641\n",
      "Epoch [16/40], Step [62/118], D_loss: 0.8062, G_loss: 1.0696\n",
      "Epoch [16/40], Step [63/118], D_loss: 1.6385, G_loss: 0.7863\n",
      "Epoch [16/40], Step [64/118], D_loss: 0.3667, G_loss: 2.2748\n",
      "Epoch [16/40], Step [65/118], D_loss: 1.5864, G_loss: 0.7364\n",
      "Epoch [16/40], Step [66/118], D_loss: 0.4838, G_loss: 1.5349\n",
      "Epoch [16/40], Step [67/118], D_loss: 1.4956, G_loss: 0.6698\n",
      "Epoch [16/40], Step [68/118], D_loss: 0.5417, G_loss: 1.6033\n",
      "Epoch [16/40], Step [69/118], D_loss: 0.8930, G_loss: 1.2270\n",
      "Epoch [16/40], Step [70/118], D_loss: 0.7398, G_loss: 1.3359\n",
      "Epoch [16/40], Step [71/118], D_loss: 0.9925, G_loss: 0.8672\n",
      "Epoch [16/40], Step [72/118], D_loss: 0.5741, G_loss: 1.7890\n",
      "Epoch [16/40], Step [73/118], D_loss: 0.8178, G_loss: 1.7682\n",
      "Epoch [16/40], Step [74/118], D_loss: 2.4442, G_loss: 0.6873\n",
      "Epoch [16/40], Step [75/118], D_loss: 0.4947, G_loss: 2.6725\n",
      "Epoch [16/40], Step [76/118], D_loss: 0.8620, G_loss: 1.6477\n",
      "Epoch [16/40], Step [77/118], D_loss: 1.5258, G_loss: 0.3918\n",
      "Epoch [16/40], Step [78/118], D_loss: 1.3575, G_loss: 0.8054\n",
      "Epoch [16/40], Step [79/118], D_loss: 0.4638, G_loss: 2.0676\n",
      "Epoch [16/40], Step [80/118], D_loss: 1.8039, G_loss: 0.7698\n",
      "Epoch [16/40], Step [81/118], D_loss: 0.9226, G_loss: 1.1528\n",
      "Epoch [16/40], Step [82/118], D_loss: 0.6038, G_loss: 1.8147\n",
      "Epoch [16/40], Step [83/118], D_loss: 0.7350, G_loss: 2.2398\n",
      "Epoch [16/40], Step [84/118], D_loss: 0.4148, G_loss: 1.3654\n",
      "Epoch [16/40], Step [85/118], D_loss: 1.0577, G_loss: 1.2514\n",
      "Epoch [16/40], Step [86/118], D_loss: 1.1860, G_loss: 1.1324\n",
      "Epoch [16/40], Step [87/118], D_loss: 0.4862, G_loss: 1.5749\n",
      "Epoch [16/40], Step [88/118], D_loss: 0.6229, G_loss: 1.4686\n",
      "Epoch [16/40], Step [89/118], D_loss: 0.4131, G_loss: 2.5047\n",
      "Epoch [16/40], Step [90/118], D_loss: 1.7670, G_loss: 0.8202\n",
      "Epoch [16/40], Step [91/118], D_loss: 0.9697, G_loss: 1.7856\n",
      "Epoch [16/40], Step [92/118], D_loss: 0.6715, G_loss: 2.6441\n",
      "Epoch [16/40], Step [93/118], D_loss: 0.5488, G_loss: 1.3265\n",
      "Epoch [16/40], Step [94/118], D_loss: 0.7823, G_loss: 1.1057\n",
      "Epoch [16/40], Step [95/118], D_loss: 0.4092, G_loss: 1.4331\n",
      "Epoch [16/40], Step [96/118], D_loss: 0.6791, G_loss: 1.6007\n",
      "Epoch [16/40], Step [97/118], D_loss: 0.7628, G_loss: 1.6661\n",
      "Epoch [16/40], Step [98/118], D_loss: 0.6835, G_loss: 2.1300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/40], Step [99/118], D_loss: 0.4144, G_loss: 1.4518\n",
      "Epoch [16/40], Step [100/118], D_loss: 0.7959, G_loss: 1.1016\n",
      "Epoch [16/40], Step [101/118], D_loss: 0.6561, G_loss: 1.2824\n",
      "Epoch [16/40], Step [102/118], D_loss: 0.5969, G_loss: 1.5455\n",
      "Epoch [16/40], Step [103/118], D_loss: 0.8620, G_loss: 0.8791\n",
      "Epoch [16/40], Step [104/118], D_loss: 0.7086, G_loss: 1.5033\n",
      "Epoch [16/40], Step [105/118], D_loss: 0.4202, G_loss: 2.2522\n",
      "Epoch [16/40], Step [106/118], D_loss: 1.6905, G_loss: 0.6233\n",
      "Epoch [16/40], Step [107/118], D_loss: 0.9601, G_loss: 0.8876\n",
      "Epoch [16/40], Step [108/118], D_loss: 0.4465, G_loss: 1.6181\n",
      "Epoch [16/40], Step [109/118], D_loss: 0.5497, G_loss: 1.6235\n",
      "Epoch [16/40], Step [110/118], D_loss: 0.3403, G_loss: 2.3974\n",
      "Epoch [16/40], Step [111/118], D_loss: 0.7924, G_loss: 1.4265\n",
      "Epoch [16/40], Step [112/118], D_loss: 0.5206, G_loss: 1.9862\n",
      "Epoch [16/40], Step [113/118], D_loss: 1.1597, G_loss: 0.7920\n",
      "Epoch [16/40], Step [114/118], D_loss: 0.9342, G_loss: 1.1267\n",
      "Epoch [16/40], Step [115/118], D_loss: 0.2429, G_loss: 2.6652\n",
      "Epoch [16/40], Step [116/118], D_loss: 0.7951, G_loss: 1.2596\n",
      "Epoch [16/40], Step [117/118], D_loss: 0.5913, G_loss: 1.2763\n",
      "Epoch [16/40], Step [118/118], D_loss: 4.9916, G_loss: 0.1011\n",
      "Epoch [17/40], Step [1/118], D_loss: 0.7036, G_loss: 2.4811\n",
      "Epoch [17/40], Step [2/118], D_loss: 0.5782, G_loss: 1.5536\n",
      "Epoch [17/40], Step [3/118], D_loss: 0.6302, G_loss: 1.2260\n",
      "Epoch [17/40], Step [4/118], D_loss: 0.8142, G_loss: 0.8553\n",
      "Epoch [17/40], Step [5/118], D_loss: 0.6732, G_loss: 1.2011\n",
      "Epoch [17/40], Step [6/118], D_loss: 0.4462, G_loss: 1.9346\n",
      "Epoch [17/40], Step [7/118], D_loss: 0.7053, G_loss: 1.9796\n",
      "Epoch [17/40], Step [8/118], D_loss: 0.8624, G_loss: 2.0629\n",
      "Epoch [17/40], Step [9/118], D_loss: 0.7149, G_loss: 1.0608\n",
      "Epoch [17/40], Step [10/118], D_loss: 0.4449, G_loss: 1.5956\n",
      "Epoch [17/40], Step [11/118], D_loss: 0.5714, G_loss: 1.6020\n",
      "Epoch [17/40], Step [12/118], D_loss: 0.6545, G_loss: 1.7199\n",
      "Epoch [17/40], Step [13/118], D_loss: 1.0308, G_loss: 1.3566\n",
      "Epoch [17/40], Step [14/118], D_loss: 0.8598, G_loss: 1.1161\n",
      "Epoch [17/40], Step [15/118], D_loss: 0.7879, G_loss: 1.9379\n",
      "Epoch [17/40], Step [16/118], D_loss: 0.5260, G_loss: 1.4521\n",
      "Epoch [17/40], Step [17/118], D_loss: 0.4548, G_loss: 1.9803\n",
      "Epoch [17/40], Step [18/118], D_loss: 0.8662, G_loss: 1.4237\n",
      "Epoch [17/40], Step [19/118], D_loss: 0.5109, G_loss: 1.3919\n",
      "Epoch [17/40], Step [20/118], D_loss: 1.3953, G_loss: 1.2009\n",
      "Epoch [17/40], Step [21/118], D_loss: 0.9083, G_loss: 2.1360\n",
      "Epoch [17/40], Step [22/118], D_loss: 0.5802, G_loss: 1.0926\n",
      "Epoch [17/40], Step [23/118], D_loss: 0.9528, G_loss: 1.3874\n",
      "Epoch [17/40], Step [24/118], D_loss: 0.7237, G_loss: 2.0080\n",
      "Epoch [17/40], Step [25/118], D_loss: 1.7296, G_loss: 0.4672\n",
      "Epoch [17/40], Step [26/118], D_loss: 0.6525, G_loss: 1.5609\n",
      "Epoch [17/40], Step [27/118], D_loss: 0.9891, G_loss: 1.7501\n",
      "Epoch [17/40], Step [28/118], D_loss: 3.6537, G_loss: 0.9846\n",
      "Epoch [17/40], Step [29/118], D_loss: 0.8228, G_loss: 0.6463\n",
      "Epoch [17/40], Step [30/118], D_loss: 0.7525, G_loss: 1.1250\n",
      "Epoch [17/40], Step [31/118], D_loss: 0.7429, G_loss: 1.7224\n",
      "Epoch [17/40], Step [32/118], D_loss: 1.0355, G_loss: 1.3694\n",
      "Epoch [17/40], Step [33/118], D_loss: 0.7620, G_loss: 1.3124\n",
      "Epoch [17/40], Step [34/118], D_loss: 0.5027, G_loss: 1.3729\n",
      "Epoch [17/40], Step [35/118], D_loss: 0.7320, G_loss: 1.6165\n",
      "Epoch [17/40], Step [36/118], D_loss: 1.5471, G_loss: 0.8084\n",
      "Epoch [17/40], Step [37/118], D_loss: 0.5875, G_loss: 1.6216\n",
      "Epoch [17/40], Step [38/118], D_loss: 0.5509, G_loss: 2.1518\n",
      "Epoch [17/40], Step [39/118], D_loss: 1.4939, G_loss: 0.5234\n",
      "Epoch [17/40], Step [40/118], D_loss: 1.0507, G_loss: 0.6664\n",
      "Epoch [17/40], Step [41/118], D_loss: 2.0764, G_loss: 0.8079\n",
      "Epoch [17/40], Step [42/118], D_loss: 0.4847, G_loss: 2.5034\n",
      "Epoch [17/40], Step [43/118], D_loss: 0.5947, G_loss: 2.5823\n",
      "Epoch [17/40], Step [44/118], D_loss: 2.0499, G_loss: 0.6207\n",
      "Epoch [17/40], Step [45/118], D_loss: 1.4882, G_loss: 0.6352\n",
      "Epoch [17/40], Step [46/118], D_loss: 0.6314, G_loss: 2.0554\n",
      "Epoch [17/40], Step [47/118], D_loss: 1.0063, G_loss: 1.5284\n",
      "Epoch [17/40], Step [48/118], D_loss: 1.8846, G_loss: 0.5038\n",
      "Epoch [17/40], Step [49/118], D_loss: 0.6118, G_loss: 1.4174\n",
      "Epoch [17/40], Step [50/118], D_loss: 0.6176, G_loss: 1.5247\n",
      "Epoch [17/40], Step [51/118], D_loss: 0.8402, G_loss: 1.1952\n",
      "Epoch [17/40], Step [52/118], D_loss: 0.6506, G_loss: 1.5727\n",
      "Epoch [17/40], Step [53/118], D_loss: 0.6265, G_loss: 1.5858\n",
      "Epoch [17/40], Step [54/118], D_loss: 0.6587, G_loss: 1.9368\n",
      "Epoch [17/40], Step [55/118], D_loss: 0.5384, G_loss: 1.6047\n",
      "Epoch [17/40], Step [56/118], D_loss: 1.4683, G_loss: 0.9216\n",
      "Epoch [17/40], Step [57/118], D_loss: 0.8140, G_loss: 1.7818\n",
      "Epoch [17/40], Step [58/118], D_loss: 1.1207, G_loss: 0.7663\n",
      "Epoch [17/40], Step [59/118], D_loss: 0.4089, G_loss: 1.4200\n",
      "Epoch [17/40], Step [60/118], D_loss: 0.8240, G_loss: 1.6501\n",
      "Epoch [17/40], Step [61/118], D_loss: 1.3385, G_loss: 1.4469\n",
      "Epoch [17/40], Step [62/118], D_loss: 0.5369, G_loss: 1.2836\n",
      "Epoch [17/40], Step [63/118], D_loss: 1.4709, G_loss: 0.6744\n",
      "Epoch [17/40], Step [64/118], D_loss: 2.0366, G_loss: 0.7531\n",
      "Epoch [17/40], Step [65/118], D_loss: 0.2505, G_loss: 3.0614\n",
      "Epoch [17/40], Step [66/118], D_loss: 1.1056, G_loss: 1.3038\n",
      "Epoch [17/40], Step [67/118], D_loss: 1.1665, G_loss: 0.9294\n",
      "Epoch [17/40], Step [68/118], D_loss: 0.2990, G_loss: 2.7307\n",
      "Epoch [17/40], Step [69/118], D_loss: 0.2362, G_loss: 2.1984\n",
      "Epoch [17/40], Step [70/118], D_loss: 0.3556, G_loss: 2.5836\n",
      "Epoch [17/40], Step [71/118], D_loss: 0.4757, G_loss: 1.5089\n",
      "Epoch [17/40], Step [72/118], D_loss: 0.3450, G_loss: 2.0430\n",
      "Epoch [17/40], Step [73/118], D_loss: 0.5472, G_loss: 1.5824\n",
      "Epoch [17/40], Step [74/118], D_loss: 0.7176, G_loss: 1.5898\n",
      "Epoch [17/40], Step [75/118], D_loss: 0.8504, G_loss: 0.9106\n",
      "Epoch [17/40], Step [76/118], D_loss: 1.0334, G_loss: 1.2820\n",
      "Epoch [17/40], Step [77/118], D_loss: 0.9590, G_loss: 1.3775\n",
      "Epoch [17/40], Step [78/118], D_loss: 0.6276, G_loss: 1.3627\n",
      "Epoch [17/40], Step [79/118], D_loss: 0.8019, G_loss: 0.9153\n",
      "Epoch [17/40], Step [80/118], D_loss: 0.6200, G_loss: 1.4680\n",
      "Epoch [17/40], Step [81/118], D_loss: 0.8721, G_loss: 1.7981\n",
      "Epoch [17/40], Step [82/118], D_loss: 1.2741, G_loss: 1.1804\n",
      "Epoch [17/40], Step [83/118], D_loss: 1.0513, G_loss: 0.9047\n",
      "Epoch [17/40], Step [84/118], D_loss: 0.9299, G_loss: 1.1033\n",
      "Epoch [17/40], Step [85/118], D_loss: 1.5082, G_loss: 0.9970\n",
      "Epoch [17/40], Step [86/118], D_loss: 1.0384, G_loss: 2.6207\n",
      "Epoch [17/40], Step [87/118], D_loss: 0.7570, G_loss: 0.8829\n",
      "Epoch [17/40], Step [88/118], D_loss: 0.8360, G_loss: 1.3050\n",
      "Epoch [17/40], Step [89/118], D_loss: 0.4523, G_loss: 2.0070\n",
      "Epoch [17/40], Step [90/118], D_loss: 0.1928, G_loss: 2.7699\n",
      "Epoch [17/40], Step [91/118], D_loss: 1.0744, G_loss: 1.0484\n",
      "Epoch [17/40], Step [92/118], D_loss: 0.9236, G_loss: 0.8199\n",
      "Epoch [17/40], Step [93/118], D_loss: 1.6127, G_loss: 1.1582\n",
      "Epoch [17/40], Step [94/118], D_loss: 0.6617, G_loss: 2.1101\n",
      "Epoch [17/40], Step [95/118], D_loss: 1.0210, G_loss: 1.1771\n",
      "Epoch [17/40], Step [96/118], D_loss: 0.3717, G_loss: 1.4211\n",
      "Epoch [17/40], Step [97/118], D_loss: 0.4882, G_loss: 1.5602\n",
      "Epoch [17/40], Step [98/118], D_loss: 1.1841, G_loss: 1.1429\n",
      "Epoch [17/40], Step [99/118], D_loss: 0.5031, G_loss: 2.1556\n",
      "Epoch [17/40], Step [100/118], D_loss: 1.9413, G_loss: 0.6604\n",
      "Epoch [17/40], Step [101/118], D_loss: 0.2603, G_loss: 1.3869\n",
      "Epoch [17/40], Step [102/118], D_loss: 1.5944, G_loss: 0.9183\n",
      "Epoch [17/40], Step [103/118], D_loss: 1.1881, G_loss: 2.1361\n",
      "Epoch [17/40], Step [104/118], D_loss: 2.1668, G_loss: 1.7010\n",
      "Epoch [17/40], Step [105/118], D_loss: 0.6663, G_loss: 1.0561\n",
      "Epoch [17/40], Step [106/118], D_loss: 0.4110, G_loss: 1.4584\n",
      "Epoch [17/40], Step [107/118], D_loss: 1.1537, G_loss: 1.1170\n",
      "Epoch [17/40], Step [108/118], D_loss: 0.9316, G_loss: 1.8116\n",
      "Epoch [17/40], Step [109/118], D_loss: 0.5646, G_loss: 1.1958\n",
      "Epoch [17/40], Step [110/118], D_loss: 1.5154, G_loss: 1.1159\n",
      "Epoch [17/40], Step [111/118], D_loss: 0.6920, G_loss: 2.1399\n",
      "Epoch [17/40], Step [112/118], D_loss: 0.4591, G_loss: 2.0039\n",
      "Epoch [17/40], Step [113/118], D_loss: 1.0435, G_loss: 1.0914\n",
      "Epoch [17/40], Step [114/118], D_loss: 1.0530, G_loss: 0.5566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/40], Step [115/118], D_loss: 1.1307, G_loss: 0.9364\n",
      "Epoch [17/40], Step [116/118], D_loss: 0.6609, G_loss: 2.0362\n",
      "Epoch [17/40], Step [117/118], D_loss: 0.3867, G_loss: 2.9747\n",
      "Epoch [17/40], Step [118/118], D_loss: 2.4464, G_loss: 0.6729\n",
      "Epoch [18/40], Step [1/118], D_loss: 2.0077, G_loss: 1.1613\n",
      "Epoch [18/40], Step [2/118], D_loss: 0.6391, G_loss: 1.5263\n",
      "Epoch [18/40], Step [3/118], D_loss: 0.4809, G_loss: 1.1446\n",
      "Epoch [18/40], Step [4/118], D_loss: 2.2430, G_loss: 0.6140\n",
      "Epoch [18/40], Step [5/118], D_loss: 0.6472, G_loss: 2.1147\n",
      "Epoch [18/40], Step [6/118], D_loss: 0.4699, G_loss: 2.3776\n",
      "Epoch [18/40], Step [7/118], D_loss: 0.7069, G_loss: 1.3440\n",
      "Epoch [18/40], Step [8/118], D_loss: 0.8048, G_loss: 1.2177\n",
      "Epoch [18/40], Step [9/118], D_loss: 1.3543, G_loss: 0.9855\n",
      "Epoch [18/40], Step [10/118], D_loss: 0.6374, G_loss: 0.8809\n",
      "Epoch [18/40], Step [11/118], D_loss: 1.0195, G_loss: 1.0728\n",
      "Epoch [18/40], Step [12/118], D_loss: 0.8404, G_loss: 1.6212\n",
      "Epoch [18/40], Step [13/118], D_loss: 0.8903, G_loss: 1.5274\n",
      "Epoch [18/40], Step [14/118], D_loss: 0.4564, G_loss: 1.6434\n",
      "Epoch [18/40], Step [15/118], D_loss: 1.6135, G_loss: 1.0451\n",
      "Epoch [18/40], Step [16/118], D_loss: 0.7855, G_loss: 2.8495\n",
      "Epoch [18/40], Step [17/118], D_loss: 2.1900, G_loss: 0.7700\n",
      "Epoch [18/40], Step [18/118], D_loss: 1.1831, G_loss: 0.6444\n",
      "Epoch [18/40], Step [19/118], D_loss: 0.9861, G_loss: 1.6271\n",
      "Epoch [18/40], Step [20/118], D_loss: 0.6870, G_loss: 2.0250\n",
      "Epoch [18/40], Step [21/118], D_loss: 0.7546, G_loss: 1.5167\n",
      "Epoch [18/40], Step [22/118], D_loss: 0.4969, G_loss: 2.1435\n",
      "Epoch [18/40], Step [23/118], D_loss: 0.7016, G_loss: 2.3156\n",
      "Epoch [18/40], Step [24/118], D_loss: 0.3231, G_loss: 1.3507\n",
      "Epoch [18/40], Step [25/118], D_loss: 1.0878, G_loss: 1.0484\n",
      "Epoch [18/40], Step [26/118], D_loss: 0.7312, G_loss: 1.5431\n",
      "Epoch [18/40], Step [27/118], D_loss: 0.5813, G_loss: 1.6928\n",
      "Epoch [18/40], Step [28/118], D_loss: 0.8799, G_loss: 2.8792\n",
      "Epoch [18/40], Step [29/118], D_loss: 0.5555, G_loss: 1.3497\n",
      "Epoch [18/40], Step [30/118], D_loss: 0.7795, G_loss: 1.0416\n",
      "Epoch [18/40], Step [31/118], D_loss: 0.5146, G_loss: 1.8309\n",
      "Epoch [18/40], Step [32/118], D_loss: 0.5292, G_loss: 1.9116\n",
      "Epoch [18/40], Step [33/118], D_loss: 2.5005, G_loss: 0.6131\n",
      "Epoch [18/40], Step [34/118], D_loss: 1.8172, G_loss: 0.5108\n",
      "Epoch [18/40], Step [35/118], D_loss: 0.7505, G_loss: 1.5140\n",
      "Epoch [18/40], Step [36/118], D_loss: 1.0399, G_loss: 1.0150\n",
      "Epoch [18/40], Step [37/118], D_loss: 1.6474, G_loss: 0.7278\n",
      "Epoch [18/40], Step [38/118], D_loss: 0.5963, G_loss: 2.0152\n",
      "Epoch [18/40], Step [39/118], D_loss: 1.4953, G_loss: 1.1006\n",
      "Epoch [18/40], Step [40/118], D_loss: 1.8226, G_loss: 0.6164\n",
      "Epoch [18/40], Step [41/118], D_loss: 0.7650, G_loss: 0.8235\n",
      "Epoch [18/40], Step [42/118], D_loss: 1.0175, G_loss: 1.4295\n",
      "Epoch [18/40], Step [43/118], D_loss: 1.2562, G_loss: 1.2888\n",
      "Epoch [18/40], Step [44/118], D_loss: 0.4816, G_loss: 1.8153\n",
      "Epoch [18/40], Step [45/118], D_loss: 0.9261, G_loss: 1.2904\n",
      "Epoch [18/40], Step [46/118], D_loss: 1.4826, G_loss: 1.2662\n",
      "Epoch [18/40], Step [47/118], D_loss: 0.8962, G_loss: 0.8014\n",
      "Epoch [18/40], Step [48/118], D_loss: 0.3887, G_loss: 1.6576\n",
      "Epoch [18/40], Step [49/118], D_loss: 1.2377, G_loss: 0.9869\n",
      "Epoch [18/40], Step [50/118], D_loss: 1.5206, G_loss: 0.9784\n",
      "Epoch [18/40], Step [51/118], D_loss: 0.7694, G_loss: 1.4047\n",
      "Epoch [18/40], Step [52/118], D_loss: 1.6497, G_loss: 1.4067\n",
      "Epoch [18/40], Step [53/118], D_loss: 0.7480, G_loss: 0.8289\n",
      "Epoch [18/40], Step [54/118], D_loss: 0.6366, G_loss: 1.5261\n",
      "Epoch [18/40], Step [55/118], D_loss: 0.5274, G_loss: 2.5703\n",
      "Epoch [18/40], Step [56/118], D_loss: 0.6524, G_loss: 1.3733\n",
      "Epoch [18/40], Step [57/118], D_loss: 0.7770, G_loss: 1.7049\n",
      "Epoch [18/40], Step [58/118], D_loss: 0.8405, G_loss: 1.8622\n",
      "Epoch [18/40], Step [59/118], D_loss: 0.7583, G_loss: 1.5500\n",
      "Epoch [18/40], Step [60/118], D_loss: 1.3242, G_loss: 0.7152\n",
      "Epoch [18/40], Step [61/118], D_loss: 0.5144, G_loss: 1.8286\n",
      "Epoch [18/40], Step [62/118], D_loss: 1.4242, G_loss: 0.9380\n",
      "Epoch [18/40], Step [63/118], D_loss: 1.3840, G_loss: 0.5389\n",
      "Epoch [18/40], Step [64/118], D_loss: 0.6160, G_loss: 1.0034\n",
      "Epoch [18/40], Step [65/118], D_loss: 1.1515, G_loss: 1.3619\n",
      "Epoch [18/40], Step [66/118], D_loss: 0.9246, G_loss: 1.6641\n",
      "Epoch [18/40], Step [67/118], D_loss: 1.0857, G_loss: 1.1173\n",
      "Epoch [18/40], Step [68/118], D_loss: 0.7897, G_loss: 1.0713\n",
      "Epoch [18/40], Step [69/118], D_loss: 0.9912, G_loss: 1.1172\n",
      "Epoch [18/40], Step [70/118], D_loss: 0.4732, G_loss: 2.5704\n",
      "Epoch [18/40], Step [71/118], D_loss: 1.4307, G_loss: 0.6914\n",
      "Epoch [18/40], Step [72/118], D_loss: 0.5203, G_loss: 1.4373\n",
      "Epoch [18/40], Step [73/118], D_loss: 0.9856, G_loss: 1.0460\n",
      "Epoch [18/40], Step [74/118], D_loss: 0.4944, G_loss: 2.0209\n",
      "Epoch [18/40], Step [75/118], D_loss: 0.4435, G_loss: 2.2329\n",
      "Epoch [18/40], Step [76/118], D_loss: 0.5379, G_loss: 2.8131\n",
      "Epoch [18/40], Step [77/118], D_loss: 0.9677, G_loss: 1.4749\n",
      "Epoch [18/40], Step [78/118], D_loss: 1.0291, G_loss: 0.6334\n",
      "Epoch [18/40], Step [79/118], D_loss: 0.7861, G_loss: 1.4173\n",
      "Epoch [18/40], Step [80/118], D_loss: 0.5757, G_loss: 2.0017\n",
      "Epoch [18/40], Step [81/118], D_loss: 0.4926, G_loss: 2.0494\n",
      "Epoch [18/40], Step [82/118], D_loss: 1.4368, G_loss: 0.8066\n",
      "Epoch [18/40], Step [83/118], D_loss: 0.9152, G_loss: 1.1665\n",
      "Epoch [18/40], Step [84/118], D_loss: 0.9610, G_loss: 1.2221\n",
      "Epoch [18/40], Step [85/118], D_loss: 0.7691, G_loss: 2.0885\n",
      "Epoch [18/40], Step [86/118], D_loss: 0.3119, G_loss: 1.7628\n",
      "Epoch [18/40], Step [87/118], D_loss: 0.8348, G_loss: 1.5102\n",
      "Epoch [18/40], Step [88/118], D_loss: 0.5539, G_loss: 2.1052\n",
      "Epoch [18/40], Step [89/118], D_loss: 1.1914, G_loss: 1.1628\n",
      "Epoch [18/40], Step [90/118], D_loss: 0.4112, G_loss: 1.5396\n",
      "Epoch [18/40], Step [91/118], D_loss: 0.7528, G_loss: 1.1025\n",
      "Epoch [18/40], Step [92/118], D_loss: 0.7565, G_loss: 1.1426\n",
      "Epoch [18/40], Step [93/118], D_loss: 0.5350, G_loss: 1.5980\n",
      "Epoch [18/40], Step [94/118], D_loss: 0.8081, G_loss: 1.3653\n",
      "Epoch [18/40], Step [95/118], D_loss: 1.2725, G_loss: 0.9742\n",
      "Epoch [18/40], Step [96/118], D_loss: 0.2734, G_loss: 2.1669\n",
      "Epoch [18/40], Step [97/118], D_loss: 0.5383, G_loss: 1.4198\n",
      "Epoch [18/40], Step [98/118], D_loss: 0.3049, G_loss: 1.9479\n",
      "Epoch [18/40], Step [99/118], D_loss: 2.7535, G_loss: 0.3725\n",
      "Epoch [18/40], Step [100/118], D_loss: 0.7439, G_loss: 1.1672\n",
      "Epoch [18/40], Step [101/118], D_loss: 0.4733, G_loss: 1.6291\n",
      "Epoch [18/40], Step [102/118], D_loss: 0.3741, G_loss: 2.1052\n",
      "Epoch [18/40], Step [103/118], D_loss: 1.6152, G_loss: 0.7863\n",
      "Epoch [18/40], Step [104/118], D_loss: 0.4549, G_loss: 2.9603\n",
      "Epoch [18/40], Step [105/118], D_loss: 1.2651, G_loss: 0.6533\n",
      "Epoch [18/40], Step [106/118], D_loss: 0.7215, G_loss: 0.9786\n",
      "Epoch [18/40], Step [107/118], D_loss: 1.6300, G_loss: 1.2092\n",
      "Epoch [18/40], Step [108/118], D_loss: 0.5807, G_loss: 2.4110\n",
      "Epoch [18/40], Step [109/118], D_loss: 1.3808, G_loss: 1.7884\n",
      "Epoch [18/40], Step [110/118], D_loss: 1.0905, G_loss: 1.4223\n",
      "Epoch [18/40], Step [111/118], D_loss: 1.0467, G_loss: 0.7952\n",
      "Epoch [18/40], Step [112/118], D_loss: 0.8503, G_loss: 1.6902\n",
      "Epoch [18/40], Step [113/118], D_loss: 1.5222, G_loss: 1.1027\n",
      "Epoch [18/40], Step [114/118], D_loss: 0.7922, G_loss: 1.2326\n",
      "Epoch [18/40], Step [115/118], D_loss: 0.8734, G_loss: 3.1628\n",
      "Epoch [18/40], Step [116/118], D_loss: 0.5864, G_loss: 1.0717\n",
      "Epoch [18/40], Step [117/118], D_loss: 0.7192, G_loss: 1.4385\n",
      "Epoch [18/40], Step [118/118], D_loss: 1.7479, G_loss: 0.9471\n",
      "Epoch [19/40], Step [1/118], D_loss: 0.9967, G_loss: 1.4269\n",
      "Epoch [19/40], Step [2/118], D_loss: 1.4628, G_loss: 0.6471\n",
      "Epoch [19/40], Step [3/118], D_loss: 1.4048, G_loss: 0.7452\n",
      "Epoch [19/40], Step [4/118], D_loss: 1.1256, G_loss: 1.2411\n",
      "Epoch [19/40], Step [5/118], D_loss: 0.5800, G_loss: 2.4076\n",
      "Epoch [19/40], Step [6/118], D_loss: 0.8936, G_loss: 0.9324\n",
      "Epoch [19/40], Step [7/118], D_loss: 0.4888, G_loss: 1.5591\n",
      "Epoch [19/40], Step [8/118], D_loss: 1.7107, G_loss: 0.8873\n",
      "Epoch [19/40], Step [9/118], D_loss: 1.5904, G_loss: 1.0466\n",
      "Epoch [19/40], Step [10/118], D_loss: 1.8661, G_loss: 1.2723\n",
      "Epoch [19/40], Step [11/118], D_loss: 0.6016, G_loss: 0.9224\n",
      "Epoch [19/40], Step [12/118], D_loss: 0.8843, G_loss: 1.2609\n",
      "Epoch [19/40], Step [13/118], D_loss: 0.8823, G_loss: 1.5413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/40], Step [14/118], D_loss: 0.6054, G_loss: 1.3971\n",
      "Epoch [19/40], Step [15/118], D_loss: 1.0229, G_loss: 0.9475\n",
      "Epoch [19/40], Step [16/118], D_loss: 1.4692, G_loss: 0.6698\n",
      "Epoch [19/40], Step [17/118], D_loss: 1.5239, G_loss: 0.9982\n",
      "Epoch [19/40], Step [18/118], D_loss: 0.6529, G_loss: 1.8240\n",
      "Epoch [19/40], Step [19/118], D_loss: 0.6034, G_loss: 1.8185\n",
      "Epoch [19/40], Step [20/118], D_loss: 1.1892, G_loss: 0.8965\n",
      "Epoch [19/40], Step [21/118], D_loss: 0.3897, G_loss: 1.4641\n",
      "Epoch [19/40], Step [22/118], D_loss: 0.3444, G_loss: 1.6307\n",
      "Epoch [19/40], Step [23/118], D_loss: 0.5254, G_loss: 1.5502\n",
      "Epoch [19/40], Step [24/118], D_loss: 1.0251, G_loss: 1.4088\n",
      "Epoch [19/40], Step [25/118], D_loss: 1.1929, G_loss: 1.4367\n",
      "Epoch [19/40], Step [26/118], D_loss: 0.2701, G_loss: 1.8792\n",
      "Epoch [19/40], Step [27/118], D_loss: 0.6334, G_loss: 1.4571\n",
      "Epoch [19/40], Step [28/118], D_loss: 1.4023, G_loss: 1.1607\n",
      "Epoch [19/40], Step [29/118], D_loss: 1.9175, G_loss: 0.6074\n",
      "Epoch [19/40], Step [30/118], D_loss: 1.7413, G_loss: 0.8711\n",
      "Epoch [19/40], Step [31/118], D_loss: 0.4619, G_loss: 2.0065\n",
      "Epoch [19/40], Step [32/118], D_loss: 0.5412, G_loss: 1.5351\n",
      "Epoch [19/40], Step [33/118], D_loss: 1.3158, G_loss: 0.9035\n",
      "Epoch [19/40], Step [34/118], D_loss: 1.2518, G_loss: 1.1303\n",
      "Epoch [19/40], Step [35/118], D_loss: 0.2957, G_loss: 2.1651\n",
      "Epoch [19/40], Step [36/118], D_loss: 1.1493, G_loss: 0.9425\n",
      "Epoch [19/40], Step [37/118], D_loss: 0.2748, G_loss: 2.7456\n",
      "Epoch [19/40], Step [38/118], D_loss: 1.2582, G_loss: 0.7836\n",
      "Epoch [19/40], Step [39/118], D_loss: 0.9909, G_loss: 0.9766\n",
      "Epoch [19/40], Step [40/118], D_loss: 0.3575, G_loss: 2.2706\n",
      "Epoch [19/40], Step [41/118], D_loss: 0.4336, G_loss: 1.9887\n",
      "Epoch [19/40], Step [42/118], D_loss: 0.7573, G_loss: 1.2522\n",
      "Epoch [19/40], Step [43/118], D_loss: 0.7430, G_loss: 1.3624\n",
      "Epoch [19/40], Step [44/118], D_loss: 1.4478, G_loss: 1.0607\n",
      "Epoch [19/40], Step [45/118], D_loss: 1.5940, G_loss: 0.4886\n",
      "Epoch [19/40], Step [46/118], D_loss: 0.6841, G_loss: 1.5752\n",
      "Epoch [19/40], Step [47/118], D_loss: 0.5540, G_loss: 1.9193\n",
      "Epoch [19/40], Step [48/118], D_loss: 0.4222, G_loss: 2.9264\n",
      "Epoch [19/40], Step [49/118], D_loss: 0.8711, G_loss: 1.0685\n",
      "Epoch [19/40], Step [50/118], D_loss: 0.6581, G_loss: 1.0800\n",
      "Epoch [19/40], Step [51/118], D_loss: 0.3979, G_loss: 1.8786\n",
      "Epoch [19/40], Step [52/118], D_loss: 0.5705, G_loss: 2.0764\n",
      "Epoch [19/40], Step [53/118], D_loss: 0.6100, G_loss: 1.3322\n",
      "Epoch [19/40], Step [54/118], D_loss: 0.7142, G_loss: 1.4394\n",
      "Epoch [19/40], Step [55/118], D_loss: 0.6357, G_loss: 1.7881\n",
      "Epoch [19/40], Step [56/118], D_loss: 1.1357, G_loss: 0.9176\n",
      "Epoch [19/40], Step [57/118], D_loss: 0.4686, G_loss: 1.3997\n",
      "Epoch [19/40], Step [58/118], D_loss: 0.4059, G_loss: 1.7155\n",
      "Epoch [19/40], Step [59/118], D_loss: 0.6175, G_loss: 1.5122\n",
      "Epoch [19/40], Step [60/118], D_loss: 0.3987, G_loss: 2.4052\n",
      "Epoch [19/40], Step [61/118], D_loss: 0.6683, G_loss: 1.3118\n",
      "Epoch [19/40], Step [62/118], D_loss: 0.3762, G_loss: 2.1794\n",
      "Epoch [19/40], Step [63/118], D_loss: 1.0303, G_loss: 1.0298\n",
      "Epoch [19/40], Step [64/118], D_loss: 0.6313, G_loss: 1.6258\n",
      "Epoch [19/40], Step [65/118], D_loss: 0.5591, G_loss: 3.3145\n",
      "Epoch [19/40], Step [66/118], D_loss: 0.8852, G_loss: 0.8997\n",
      "Epoch [19/40], Step [67/118], D_loss: 0.7311, G_loss: 0.9897\n",
      "Epoch [19/40], Step [68/118], D_loss: 1.7587, G_loss: 0.9775\n",
      "Epoch [19/40], Step [69/118], D_loss: 0.4750, G_loss: 2.3790\n",
      "Epoch [19/40], Step [70/118], D_loss: 1.2174, G_loss: 2.1749\n",
      "Epoch [19/40], Step [71/118], D_loss: 0.9766, G_loss: 0.6641\n",
      "Epoch [19/40], Step [72/118], D_loss: 0.3686, G_loss: 1.3690\n",
      "Epoch [19/40], Step [73/118], D_loss: 1.7091, G_loss: 0.9999\n",
      "Epoch [19/40], Step [74/118], D_loss: 0.5790, G_loss: 2.5138\n",
      "Epoch [19/40], Step [75/118], D_loss: 0.4329, G_loss: 2.4876\n",
      "Epoch [19/40], Step [76/118], D_loss: 0.6050, G_loss: 1.2088\n",
      "Epoch [19/40], Step [77/118], D_loss: 0.8393, G_loss: 0.9756\n",
      "Epoch [19/40], Step [78/118], D_loss: 1.4571, G_loss: 0.7591\n",
      "Epoch [19/40], Step [79/118], D_loss: 0.8787, G_loss: 1.6461\n",
      "Epoch [19/40], Step [80/118], D_loss: 1.3651, G_loss: 1.6900\n",
      "Epoch [19/40], Step [81/118], D_loss: 0.7768, G_loss: 0.9567\n",
      "Epoch [19/40], Step [82/118], D_loss: 1.2346, G_loss: 0.9404\n",
      "Epoch [19/40], Step [83/118], D_loss: 0.1246, G_loss: 3.0114\n",
      "Epoch [19/40], Step [84/118], D_loss: 0.9929, G_loss: 1.0773\n",
      "Epoch [19/40], Step [85/118], D_loss: 1.2502, G_loss: 1.0042\n",
      "Epoch [19/40], Step [86/118], D_loss: 1.3870, G_loss: 1.0897\n",
      "Epoch [19/40], Step [87/118], D_loss: 0.9060, G_loss: 2.2193\n",
      "Epoch [19/40], Step [88/118], D_loss: 0.5343, G_loss: 1.1226\n",
      "Epoch [19/40], Step [89/118], D_loss: 0.3487, G_loss: 1.6147\n",
      "Epoch [19/40], Step [90/118], D_loss: 2.1495, G_loss: 1.0598\n",
      "Epoch [19/40], Step [91/118], D_loss: 0.8394, G_loss: 3.0584\n",
      "Epoch [19/40], Step [92/118], D_loss: 0.6704, G_loss: 2.4942\n",
      "Epoch [19/40], Step [93/118], D_loss: 0.4970, G_loss: 1.1434\n",
      "Epoch [19/40], Step [94/118], D_loss: 1.3594, G_loss: 0.6568\n",
      "Epoch [19/40], Step [95/118], D_loss: 0.2532, G_loss: 2.0674\n",
      "Epoch [19/40], Step [96/118], D_loss: 0.3575, G_loss: 1.9937\n",
      "Epoch [19/40], Step [97/118], D_loss: 0.4426, G_loss: 1.8556\n",
      "Epoch [19/40], Step [98/118], D_loss: 0.4093, G_loss: 2.0938\n",
      "Epoch [19/40], Step [99/118], D_loss: 1.2101, G_loss: 1.1902\n",
      "Epoch [19/40], Step [100/118], D_loss: 0.4369, G_loss: 2.2923\n",
      "Epoch [19/40], Step [101/118], D_loss: 1.3196, G_loss: 1.5761\n",
      "Epoch [19/40], Step [102/118], D_loss: 0.4471, G_loss: 2.0019\n",
      "Epoch [19/40], Step [103/118], D_loss: 1.0693, G_loss: 0.8648\n",
      "Epoch [19/40], Step [104/118], D_loss: 0.4921, G_loss: 1.9760\n",
      "Epoch [19/40], Step [105/118], D_loss: 0.6151, G_loss: 2.6266\n",
      "Epoch [19/40], Step [106/118], D_loss: 0.9425, G_loss: 0.8831\n",
      "Epoch [19/40], Step [107/118], D_loss: 0.9338, G_loss: 1.2556\n",
      "Epoch [19/40], Step [108/118], D_loss: 0.4050, G_loss: 2.3966\n",
      "Epoch [19/40], Step [109/118], D_loss: 0.2232, G_loss: 2.6087\n",
      "Epoch [19/40], Step [110/118], D_loss: 1.0983, G_loss: 1.7840\n",
      "Epoch [19/40], Step [111/118], D_loss: 1.0430, G_loss: 0.8712\n",
      "Epoch [19/40], Step [112/118], D_loss: 0.5636, G_loss: 1.9521\n",
      "Epoch [19/40], Step [113/118], D_loss: 1.0219, G_loss: 1.4751\n",
      "Epoch [19/40], Step [114/118], D_loss: 0.1721, G_loss: 3.1170\n",
      "Epoch [19/40], Step [115/118], D_loss: 0.2755, G_loss: 2.6203\n",
      "Epoch [19/40], Step [116/118], D_loss: 1.3692, G_loss: 0.7441\n",
      "Epoch [19/40], Step [117/118], D_loss: 0.9256, G_loss: 1.0326\n",
      "Epoch [19/40], Step [118/118], D_loss: 0.3536, G_loss: 4.5520\n",
      "Epoch [20/40], Step [1/118], D_loss: 0.4554, G_loss: 1.2764\n",
      "Epoch [20/40], Step [2/118], D_loss: 1.4897, G_loss: 0.7944\n",
      "Epoch [20/40], Step [3/118], D_loss: 0.3972, G_loss: 2.0714\n",
      "Epoch [20/40], Step [4/118], D_loss: 0.1425, G_loss: 3.0304\n",
      "Epoch [20/40], Step [5/118], D_loss: 0.1872, G_loss: 2.5967\n",
      "Epoch [20/40], Step [6/118], D_loss: 0.5888, G_loss: 1.5231\n",
      "Epoch [20/40], Step [7/118], D_loss: 0.6108, G_loss: 1.5768\n",
      "Epoch [20/40], Step [8/118], D_loss: 2.5774, G_loss: 1.3466\n",
      "Epoch [20/40], Step [9/118], D_loss: 0.9040, G_loss: 0.6093\n",
      "Epoch [20/40], Step [10/118], D_loss: 1.1297, G_loss: 1.2414\n",
      "Epoch [20/40], Step [11/118], D_loss: 0.4452, G_loss: 2.3970\n",
      "Epoch [20/40], Step [12/118], D_loss: 0.8546, G_loss: 1.5687\n",
      "Epoch [20/40], Step [13/118], D_loss: 1.0710, G_loss: 1.0755\n",
      "Epoch [20/40], Step [14/118], D_loss: 1.3249, G_loss: 0.5337\n",
      "Epoch [20/40], Step [15/118], D_loss: 1.3052, G_loss: 0.9242\n",
      "Epoch [20/40], Step [16/118], D_loss: 0.3226, G_loss: 2.4231\n",
      "Epoch [20/40], Step [17/118], D_loss: 0.5690, G_loss: 1.9484\n",
      "Epoch [20/40], Step [18/118], D_loss: 0.2740, G_loss: 3.0810\n",
      "Epoch [20/40], Step [19/118], D_loss: 0.4698, G_loss: 1.5698\n",
      "Epoch [20/40], Step [20/118], D_loss: 0.7727, G_loss: 1.2471\n",
      "Epoch [20/40], Step [21/118], D_loss: 0.6722, G_loss: 2.5507\n",
      "Epoch [20/40], Step [22/118], D_loss: 0.2128, G_loss: 1.8290\n",
      "Epoch [20/40], Step [23/118], D_loss: 1.1094, G_loss: 0.8708\n",
      "Epoch [20/40], Step [24/118], D_loss: 1.1845, G_loss: 1.3303\n",
      "Epoch [20/40], Step [25/118], D_loss: 0.5177, G_loss: 2.2922\n",
      "Epoch [20/40], Step [26/118], D_loss: 1.0707, G_loss: 1.6311\n",
      "Epoch [20/40], Step [27/118], D_loss: 0.3130, G_loss: 1.5687\n",
      "Epoch [20/40], Step [28/118], D_loss: 0.5968, G_loss: 1.4455\n",
      "Epoch [20/40], Step [29/118], D_loss: 1.4938, G_loss: 1.4346\n",
      "Epoch [20/40], Step [30/118], D_loss: 0.9950, G_loss: 3.0062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/40], Step [31/118], D_loss: 0.6988, G_loss: 1.3294\n",
      "Epoch [20/40], Step [32/118], D_loss: 0.4645, G_loss: 1.2813\n",
      "Epoch [20/40], Step [33/118], D_loss: 1.0215, G_loss: 1.0293\n",
      "Epoch [20/40], Step [34/118], D_loss: 0.6719, G_loss: 1.5143\n",
      "Epoch [20/40], Step [35/118], D_loss: 0.5301, G_loss: 1.6645\n",
      "Epoch [20/40], Step [36/118], D_loss: 0.6022, G_loss: 1.3353\n",
      "Epoch [20/40], Step [37/118], D_loss: 0.5857, G_loss: 1.1574\n",
      "Epoch [20/40], Step [38/118], D_loss: 0.8715, G_loss: 1.5894\n",
      "Epoch [20/40], Step [39/118], D_loss: 1.0141, G_loss: 1.6641\n",
      "Epoch [20/40], Step [40/118], D_loss: 0.6810, G_loss: 2.5514\n",
      "Epoch [20/40], Step [41/118], D_loss: 1.1896, G_loss: 1.1849\n",
      "Epoch [20/40], Step [42/118], D_loss: 0.8311, G_loss: 1.7675\n",
      "Epoch [20/40], Step [43/118], D_loss: 0.2572, G_loss: 2.4850\n",
      "Epoch [20/40], Step [44/118], D_loss: 0.4070, G_loss: 1.6558\n",
      "Epoch [20/40], Step [45/118], D_loss: 0.6988, G_loss: 1.4022\n",
      "Epoch [20/40], Step [46/118], D_loss: 1.6126, G_loss: 0.7202\n",
      "Epoch [20/40], Step [47/118], D_loss: 0.5754, G_loss: 2.1467\n",
      "Epoch [20/40], Step [48/118], D_loss: 1.0641, G_loss: 1.0729\n",
      "Epoch [20/40], Step [49/118], D_loss: 0.6590, G_loss: 1.7516\n",
      "Epoch [20/40], Step [50/118], D_loss: 1.1229, G_loss: 0.9341\n",
      "Epoch [20/40], Step [51/118], D_loss: 0.9073, G_loss: 1.0262\n",
      "Epoch [20/40], Step [52/118], D_loss: 0.8509, G_loss: 1.1854\n",
      "Epoch [20/40], Step [53/118], D_loss: 0.7161, G_loss: 1.2213\n",
      "Epoch [20/40], Step [54/118], D_loss: 0.2151, G_loss: 1.9254\n",
      "Epoch [20/40], Step [55/118], D_loss: 2.3726, G_loss: 0.9010\n",
      "Epoch [20/40], Step [56/118], D_loss: 0.5537, G_loss: 2.7687\n",
      "Epoch [20/40], Step [57/118], D_loss: 0.8039, G_loss: 2.3666\n",
      "Epoch [20/40], Step [58/118], D_loss: 0.9306, G_loss: 0.7243\n",
      "Epoch [20/40], Step [59/118], D_loss: 0.2357, G_loss: 1.7253\n",
      "Epoch [20/40], Step [60/118], D_loss: 0.9928, G_loss: 1.2228\n",
      "Epoch [20/40], Step [61/118], D_loss: 0.4622, G_loss: 2.3538\n",
      "Epoch [20/40], Step [62/118], D_loss: 1.0123, G_loss: 1.5138\n",
      "Epoch [20/40], Step [63/118], D_loss: 0.3594, G_loss: 2.5777\n",
      "Epoch [20/40], Step [64/118], D_loss: 0.9804, G_loss: 1.3489\n",
      "Epoch [20/40], Step [65/118], D_loss: 2.3442, G_loss: 0.4498\n",
      "Epoch [20/40], Step [66/118], D_loss: 0.5424, G_loss: 2.1173\n",
      "Epoch [20/40], Step [67/118], D_loss: 0.4580, G_loss: 2.6711\n",
      "Epoch [20/40], Step [68/118], D_loss: 0.4642, G_loss: 1.5636\n",
      "Epoch [20/40], Step [69/118], D_loss: 1.3680, G_loss: 0.6062\n",
      "Epoch [20/40], Step [70/118], D_loss: 0.8195, G_loss: 0.8660\n",
      "Epoch [20/40], Step [71/118], D_loss: 0.7914, G_loss: 1.5126\n",
      "Epoch [20/40], Step [72/118], D_loss: 0.3813, G_loss: 2.4447\n",
      "Epoch [20/40], Step [73/118], D_loss: 0.2832, G_loss: 2.4223\n",
      "Epoch [20/40], Step [74/118], D_loss: 1.3350, G_loss: 1.0176\n",
      "Epoch [20/40], Step [75/118], D_loss: 0.1675, G_loss: 2.9182\n",
      "Epoch [20/40], Step [76/118], D_loss: 0.7291, G_loss: 2.0721\n",
      "Epoch [20/40], Step [77/118], D_loss: 1.0455, G_loss: 0.8845\n",
      "Epoch [20/40], Step [78/118], D_loss: 0.9373, G_loss: 0.9921\n",
      "Epoch [20/40], Step [79/118], D_loss: 0.3980, G_loss: 2.2112\n",
      "Epoch [20/40], Step [80/118], D_loss: 0.6760, G_loss: 2.0736\n",
      "Epoch [20/40], Step [81/118], D_loss: 2.5788, G_loss: 1.8366\n",
      "Epoch [20/40], Step [82/118], D_loss: 0.6373, G_loss: 1.0069\n",
      "Epoch [20/40], Step [83/118], D_loss: 0.7237, G_loss: 1.3407\n",
      "Epoch [20/40], Step [84/118], D_loss: 0.8604, G_loss: 1.2878\n",
      "Epoch [20/40], Step [85/118], D_loss: 2.3424, G_loss: 0.4808\n",
      "Epoch [20/40], Step [86/118], D_loss: 1.6706, G_loss: 0.5465\n",
      "Epoch [20/40], Step [87/118], D_loss: 1.1951, G_loss: 1.2028\n",
      "Epoch [20/40], Step [88/118], D_loss: 0.3941, G_loss: 2.3412\n",
      "Epoch [20/40], Step [89/118], D_loss: 1.2545, G_loss: 1.1280\n",
      "Epoch [20/40], Step [90/118], D_loss: 0.3824, G_loss: 1.4871\n",
      "Epoch [20/40], Step [91/118], D_loss: 1.6773, G_loss: 0.7815\n",
      "Epoch [20/40], Step [92/118], D_loss: 1.1397, G_loss: 0.9218\n",
      "Epoch [20/40], Step [93/118], D_loss: 0.5116, G_loss: 2.0868\n",
      "Epoch [20/40], Step [94/118], D_loss: 0.8188, G_loss: 1.8725\n",
      "Epoch [20/40], Step [95/118], D_loss: 1.1456, G_loss: 0.8627\n",
      "Epoch [20/40], Step [96/118], D_loss: 2.5323, G_loss: 0.3689\n",
      "Epoch [20/40], Step [97/118], D_loss: 0.4630, G_loss: 2.2142\n",
      "Epoch [20/40], Step [98/118], D_loss: 0.5504, G_loss: 1.8080\n",
      "Epoch [20/40], Step [99/118], D_loss: 1.2285, G_loss: 1.0574\n",
      "Epoch [20/40], Step [100/118], D_loss: 0.6519, G_loss: 1.7685\n",
      "Epoch [20/40], Step [101/118], D_loss: 1.0705, G_loss: 1.0511\n",
      "Epoch [20/40], Step [102/118], D_loss: 0.7982, G_loss: 0.9053\n",
      "Epoch [20/40], Step [103/118], D_loss: 0.7853, G_loss: 1.3972\n",
      "Epoch [20/40], Step [104/118], D_loss: 1.4560, G_loss: 1.0005\n",
      "Epoch [20/40], Step [105/118], D_loss: 0.6762, G_loss: 1.9881\n",
      "Epoch [20/40], Step [106/118], D_loss: 0.1900, G_loss: 2.2506\n",
      "Epoch [20/40], Step [107/118], D_loss: 1.3477, G_loss: 0.8385\n",
      "Epoch [20/40], Step [108/118], D_loss: 0.8717, G_loss: 1.3474\n",
      "Epoch [20/40], Step [109/118], D_loss: 0.8035, G_loss: 1.3046\n",
      "Epoch [20/40], Step [110/118], D_loss: 0.4934, G_loss: 1.6794\n",
      "Epoch [20/40], Step [111/118], D_loss: 0.9090, G_loss: 1.0522\n",
      "Epoch [20/40], Step [112/118], D_loss: 0.4143, G_loss: 1.3740\n",
      "Epoch [20/40], Step [113/118], D_loss: 0.9347, G_loss: 1.2540\n",
      "Epoch [20/40], Step [114/118], D_loss: 0.2262, G_loss: 2.7566\n",
      "Epoch [20/40], Step [115/118], D_loss: 0.9480, G_loss: 1.1189\n",
      "Epoch [20/40], Step [116/118], D_loss: 0.4918, G_loss: 1.3469\n",
      "Epoch [20/40], Step [117/118], D_loss: 0.8870, G_loss: 1.1425\n",
      "Epoch [20/40], Step [118/118], D_loss: 3.7159, G_loss: 0.3900\n",
      "Epoch [21/40], Step [1/118], D_loss: 5.0107, G_loss: 0.0309\n",
      "Epoch [21/40], Step [2/118], D_loss: 1.5232, G_loss: 1.8435\n",
      "Epoch [21/40], Step [3/118], D_loss: 0.8989, G_loss: 2.5436\n",
      "Epoch [21/40], Step [4/118], D_loss: 1.0732, G_loss: 2.1819\n",
      "Epoch [21/40], Step [5/118], D_loss: 1.1377, G_loss: 1.6769\n",
      "Epoch [21/40], Step [6/118], D_loss: 0.8473, G_loss: 0.8754\n",
      "Epoch [21/40], Step [7/118], D_loss: 1.0128, G_loss: 1.0634\n",
      "Epoch [21/40], Step [8/118], D_loss: 0.5149, G_loss: 1.4235\n",
      "Epoch [21/40], Step [9/118], D_loss: 0.8527, G_loss: 1.3695\n",
      "Epoch [21/40], Step [10/118], D_loss: 1.3163, G_loss: 1.2322\n",
      "Epoch [21/40], Step [11/118], D_loss: 1.2862, G_loss: 1.4741\n",
      "Epoch [21/40], Step [12/118], D_loss: 0.5833, G_loss: 1.1925\n",
      "Epoch [21/40], Step [13/118], D_loss: 0.6218, G_loss: 1.3361\n",
      "Epoch [21/40], Step [14/118], D_loss: 0.4312, G_loss: 1.9686\n",
      "Epoch [21/40], Step [15/118], D_loss: 0.8772, G_loss: 1.0794\n",
      "Epoch [21/40], Step [16/118], D_loss: 0.2224, G_loss: 2.2305\n",
      "Epoch [21/40], Step [17/118], D_loss: 0.6727, G_loss: 1.5836\n",
      "Epoch [21/40], Step [18/118], D_loss: 0.5734, G_loss: 1.2877\n",
      "Epoch [21/40], Step [19/118], D_loss: 0.7628, G_loss: 1.3047\n",
      "Epoch [21/40], Step [20/118], D_loss: 0.3815, G_loss: 1.8547\n",
      "Epoch [21/40], Step [21/118], D_loss: 0.6540, G_loss: 1.4977\n",
      "Epoch [21/40], Step [22/118], D_loss: 0.8415, G_loss: 1.4880\n",
      "Epoch [21/40], Step [23/118], D_loss: 0.9965, G_loss: 1.2332\n",
      "Epoch [21/40], Step [24/118], D_loss: 0.5901, G_loss: 2.3259\n",
      "Epoch [21/40], Step [25/118], D_loss: 0.4757, G_loss: 1.4450\n",
      "Epoch [21/40], Step [26/118], D_loss: 1.2504, G_loss: 0.7401\n",
      "Epoch [21/40], Step [27/118], D_loss: 1.6959, G_loss: 1.0805\n",
      "Epoch [21/40], Step [28/118], D_loss: 0.4393, G_loss: 5.0567\n",
      "Epoch [21/40], Step [29/118], D_loss: 0.4983, G_loss: 1.6745\n",
      "Epoch [21/40], Step [30/118], D_loss: 1.4472, G_loss: 0.7924\n",
      "Epoch [21/40], Step [31/118], D_loss: 3.5548, G_loss: 0.1316\n",
      "Epoch [21/40], Step [32/118], D_loss: 0.6078, G_loss: 1.8628\n",
      "Epoch [21/40], Step [33/118], D_loss: 1.1888, G_loss: 1.1126\n",
      "Epoch [21/40], Step [34/118], D_loss: 1.1025, G_loss: 0.9585\n",
      "Epoch [21/40], Step [35/118], D_loss: 1.0804, G_loss: 1.2462\n",
      "Epoch [21/40], Step [36/118], D_loss: 0.8951, G_loss: 1.0850\n",
      "Epoch [21/40], Step [37/118], D_loss: 1.0894, G_loss: 0.9939\n",
      "Epoch [21/40], Step [38/118], D_loss: 0.5431, G_loss: 1.9598\n",
      "Epoch [21/40], Step [39/118], D_loss: 0.8684, G_loss: 1.3427\n",
      "Epoch [21/40], Step [40/118], D_loss: 1.0659, G_loss: 0.8193\n",
      "Epoch [21/40], Step [41/118], D_loss: 0.3141, G_loss: 1.6932\n",
      "Epoch [21/40], Step [42/118], D_loss: 1.6459, G_loss: 0.9596\n",
      "Epoch [21/40], Step [43/118], D_loss: 0.7537, G_loss: 2.7084\n",
      "Epoch [21/40], Step [44/118], D_loss: 0.6447, G_loss: 1.1987\n",
      "Epoch [21/40], Step [45/118], D_loss: 1.2682, G_loss: 0.9506\n",
      "Epoch [21/40], Step [46/118], D_loss: 1.9460, G_loss: 0.4635\n",
      "Epoch [21/40], Step [47/118], D_loss: 0.8101, G_loss: 1.6180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/40], Step [48/118], D_loss: 0.4725, G_loss: 3.2421\n",
      "Epoch [21/40], Step [49/118], D_loss: 0.4789, G_loss: 3.2196\n",
      "Epoch [21/40], Step [50/118], D_loss: 0.7199, G_loss: 1.1596\n",
      "Epoch [21/40], Step [51/118], D_loss: 0.2643, G_loss: 2.3470\n",
      "Epoch [21/40], Step [52/118], D_loss: 1.0647, G_loss: 1.4622\n",
      "Epoch [21/40], Step [53/118], D_loss: 1.3835, G_loss: 2.7662\n",
      "Epoch [21/40], Step [54/118], D_loss: 0.7730, G_loss: 1.0107\n",
      "Epoch [21/40], Step [55/118], D_loss: 0.2427, G_loss: 1.5875\n",
      "Epoch [21/40], Step [56/118], D_loss: 1.3925, G_loss: 1.0381\n",
      "Epoch [21/40], Step [57/118], D_loss: 0.4406, G_loss: 2.6539\n",
      "Epoch [21/40], Step [58/118], D_loss: 0.7230, G_loss: 1.4443\n",
      "Epoch [21/40], Step [59/118], D_loss: 2.4109, G_loss: 0.7838\n",
      "Epoch [21/40], Step [60/118], D_loss: 0.8518, G_loss: 0.6677\n",
      "Epoch [21/40], Step [61/118], D_loss: 0.9468, G_loss: 1.0435\n",
      "Epoch [21/40], Step [62/118], D_loss: 0.8403, G_loss: 1.5524\n",
      "Epoch [21/40], Step [63/118], D_loss: 1.0283, G_loss: 1.2734\n",
      "Epoch [21/40], Step [64/118], D_loss: 1.7033, G_loss: 1.6082\n",
      "Epoch [21/40], Step [65/118], D_loss: 0.8513, G_loss: 0.9846\n",
      "Epoch [21/40], Step [66/118], D_loss: 1.0053, G_loss: 1.3264\n",
      "Epoch [21/40], Step [67/118], D_loss: 1.0547, G_loss: 1.8533\n",
      "Epoch [21/40], Step [68/118], D_loss: 1.2737, G_loss: 0.6212\n",
      "Epoch [21/40], Step [69/118], D_loss: 1.2574, G_loss: 0.8268\n",
      "Epoch [21/40], Step [70/118], D_loss: 1.7944, G_loss: 0.7203\n",
      "Epoch [21/40], Step [71/118], D_loss: 2.4263, G_loss: 0.4222\n",
      "Epoch [21/40], Step [72/118], D_loss: 0.5402, G_loss: 2.0697\n",
      "Epoch [21/40], Step [73/118], D_loss: 0.4851, G_loss: 2.9124\n",
      "Epoch [21/40], Step [74/118], D_loss: 0.7654, G_loss: 1.0933\n",
      "Epoch [21/40], Step [75/118], D_loss: 0.5983, G_loss: 1.3132\n",
      "Epoch [21/40], Step [76/118], D_loss: 0.7874, G_loss: 1.3650\n",
      "Epoch [21/40], Step [77/118], D_loss: 1.1395, G_loss: 1.0337\n",
      "Epoch [21/40], Step [78/118], D_loss: 0.5148, G_loss: 2.7779\n",
      "Epoch [21/40], Step [79/118], D_loss: 1.0891, G_loss: 1.1387\n",
      "Epoch [21/40], Step [80/118], D_loss: 1.7638, G_loss: 0.6188\n",
      "Epoch [21/40], Step [81/118], D_loss: 1.1110, G_loss: 1.3594\n",
      "Epoch [21/40], Step [82/118], D_loss: 0.4669, G_loss: 1.7490\n",
      "Epoch [21/40], Step [83/118], D_loss: 0.7830, G_loss: 1.2615\n",
      "Epoch [21/40], Step [84/118], D_loss: 1.1670, G_loss: 1.1554\n",
      "Epoch [21/40], Step [85/118], D_loss: 1.0910, G_loss: 0.8294\n",
      "Epoch [21/40], Step [86/118], D_loss: 0.6895, G_loss: 1.7062\n",
      "Epoch [21/40], Step [87/118], D_loss: 2.2201, G_loss: 0.4593\n",
      "Epoch [21/40], Step [88/118], D_loss: 0.5337, G_loss: 2.1488\n",
      "Epoch [21/40], Step [89/118], D_loss: 0.7906, G_loss: 1.7924\n",
      "Epoch [21/40], Step [90/118], D_loss: 1.0017, G_loss: 1.0858\n",
      "Epoch [21/40], Step [91/118], D_loss: 0.3312, G_loss: 1.6560\n",
      "Epoch [21/40], Step [92/118], D_loss: 0.5919, G_loss: 1.4428\n",
      "Epoch [21/40], Step [93/118], D_loss: 0.6789, G_loss: 1.2089\n",
      "Epoch [21/40], Step [94/118], D_loss: 0.4277, G_loss: 1.8253\n",
      "Epoch [21/40], Step [95/118], D_loss: 0.4578, G_loss: 1.8469\n",
      "Epoch [21/40], Step [96/118], D_loss: 0.6312, G_loss: 1.4421\n",
      "Epoch [21/40], Step [97/118], D_loss: 0.8804, G_loss: 1.9038\n",
      "Epoch [21/40], Step [98/118], D_loss: 0.6641, G_loss: 1.0681\n",
      "Epoch [21/40], Step [99/118], D_loss: 0.8589, G_loss: 1.2849\n",
      "Epoch [21/40], Step [100/118], D_loss: 0.2262, G_loss: 2.6147\n",
      "Epoch [21/40], Step [101/118], D_loss: 0.8519, G_loss: 1.4113\n",
      "Epoch [21/40], Step [102/118], D_loss: 1.3297, G_loss: 1.3414\n",
      "Epoch [21/40], Step [103/118], D_loss: 0.7242, G_loss: 0.9264\n",
      "Epoch [21/40], Step [104/118], D_loss: 0.5121, G_loss: 1.3996\n",
      "Epoch [21/40], Step [105/118], D_loss: 0.4905, G_loss: 1.7095\n",
      "Epoch [21/40], Step [106/118], D_loss: 0.9458, G_loss: 1.4346\n",
      "Epoch [21/40], Step [107/118], D_loss: 1.2564, G_loss: 0.9312\n",
      "Epoch [21/40], Step [108/118], D_loss: 1.0643, G_loss: 1.3083\n",
      "Epoch [21/40], Step [109/118], D_loss: 1.3352, G_loss: 1.4689\n",
      "Epoch [21/40], Step [110/118], D_loss: 0.8170, G_loss: 0.8288\n",
      "Epoch [21/40], Step [111/118], D_loss: 2.0507, G_loss: 1.0384\n",
      "Epoch [21/40], Step [112/118], D_loss: 0.2840, G_loss: 3.6714\n",
      "Epoch [21/40], Step [113/118], D_loss: 1.5646, G_loss: 1.2773\n",
      "Epoch [21/40], Step [114/118], D_loss: 0.6674, G_loss: 1.0181\n",
      "Epoch [21/40], Step [115/118], D_loss: 1.1727, G_loss: 0.8928\n",
      "Epoch [21/40], Step [116/118], D_loss: 0.7439, G_loss: 1.4058\n",
      "Epoch [21/40], Step [117/118], D_loss: 0.6980, G_loss: 1.9988\n",
      "Epoch [21/40], Step [118/118], D_loss: 0.1240, G_loss: 2.2966\n",
      "Epoch [22/40], Step [1/118], D_loss: 0.8317, G_loss: 1.1575\n",
      "Epoch [22/40], Step [2/118], D_loss: 0.7490, G_loss: 1.4821\n",
      "Epoch [22/40], Step [3/118], D_loss: 0.5904, G_loss: 1.6839\n",
      "Epoch [22/40], Step [4/118], D_loss: 0.7217, G_loss: 1.6820\n",
      "Epoch [22/40], Step [5/118], D_loss: 1.4068, G_loss: 1.7641\n",
      "Epoch [22/40], Step [6/118], D_loss: 1.3624, G_loss: 0.6471\n",
      "Epoch [22/40], Step [7/118], D_loss: 0.5605, G_loss: 1.8814\n",
      "Epoch [22/40], Step [8/118], D_loss: 0.7651, G_loss: 1.2542\n",
      "Epoch [22/40], Step [9/118], D_loss: 1.0306, G_loss: 1.6948\n",
      "Epoch [22/40], Step [10/118], D_loss: 0.5399, G_loss: 1.2257\n",
      "Epoch [22/40], Step [11/118], D_loss: 0.8010, G_loss: 1.1661\n",
      "Epoch [22/40], Step [12/118], D_loss: 1.1179, G_loss: 1.1266\n",
      "Epoch [22/40], Step [13/118], D_loss: 0.4305, G_loss: 1.8846\n",
      "Epoch [22/40], Step [14/118], D_loss: 2.6215, G_loss: 0.4498\n",
      "Epoch [22/40], Step [15/118], D_loss: 0.3890, G_loss: 1.9340\n",
      "Epoch [22/40], Step [16/118], D_loss: 0.2264, G_loss: 2.1636\n",
      "Epoch [22/40], Step [17/118], D_loss: 1.1851, G_loss: 1.0694\n",
      "Epoch [22/40], Step [18/118], D_loss: 0.4285, G_loss: 2.1479\n",
      "Epoch [22/40], Step [19/118], D_loss: 1.8079, G_loss: 1.5995\n",
      "Epoch [22/40], Step [20/118], D_loss: 1.0661, G_loss: 0.6855\n",
      "Epoch [22/40], Step [21/118], D_loss: 1.2981, G_loss: 0.9976\n",
      "Epoch [22/40], Step [22/118], D_loss: 0.6164, G_loss: 1.9349\n",
      "Epoch [22/40], Step [23/118], D_loss: 1.5726, G_loss: 1.0899\n",
      "Epoch [22/40], Step [24/118], D_loss: 0.3502, G_loss: 2.1411\n",
      "Epoch [22/40], Step [25/118], D_loss: 1.5462, G_loss: 0.5518\n",
      "Epoch [22/40], Step [26/118], D_loss: 0.7970, G_loss: 1.2259\n",
      "Epoch [22/40], Step [27/118], D_loss: 1.1247, G_loss: 1.1300\n",
      "Epoch [22/40], Step [28/118], D_loss: 1.3662, G_loss: 0.9378\n",
      "Epoch [22/40], Step [29/118], D_loss: 0.2456, G_loss: 2.5127\n",
      "Epoch [22/40], Step [30/118], D_loss: 1.2557, G_loss: 1.2927\n",
      "Epoch [22/40], Step [31/118], D_loss: 2.2101, G_loss: 0.4532\n",
      "Epoch [22/40], Step [32/118], D_loss: 0.8081, G_loss: 3.3663\n",
      "Epoch [22/40], Step [33/118], D_loss: 0.9294, G_loss: 1.0580\n",
      "Epoch [22/40], Step [34/118], D_loss: 0.4521, G_loss: 1.9816\n",
      "Epoch [22/40], Step [35/118], D_loss: 0.6742, G_loss: 1.5528\n",
      "Epoch [22/40], Step [36/118], D_loss: 0.5842, G_loss: 2.1956\n",
      "Epoch [22/40], Step [37/118], D_loss: 0.8080, G_loss: 0.9369\n",
      "Epoch [22/40], Step [38/118], D_loss: 0.9656, G_loss: 1.1075\n",
      "Epoch [22/40], Step [39/118], D_loss: 0.8847, G_loss: 1.3705\n",
      "Epoch [22/40], Step [40/118], D_loss: 0.5694, G_loss: 1.7263\n",
      "Epoch [22/40], Step [41/118], D_loss: 0.6627, G_loss: 1.8379\n",
      "Epoch [22/40], Step [42/118], D_loss: 0.6544, G_loss: 1.2218\n",
      "Epoch [22/40], Step [43/118], D_loss: 0.6519, G_loss: 1.3588\n",
      "Epoch [22/40], Step [44/118], D_loss: 0.2448, G_loss: 2.1575\n",
      "Epoch [22/40], Step [45/118], D_loss: 0.4526, G_loss: 1.9175\n",
      "Epoch [22/40], Step [46/118], D_loss: 1.9600, G_loss: 0.4496\n",
      "Epoch [22/40], Step [47/118], D_loss: 2.0394, G_loss: 0.3570\n",
      "Epoch [22/40], Step [48/118], D_loss: 0.7033, G_loss: 1.2294\n",
      "Epoch [22/40], Step [49/118], D_loss: 0.9675, G_loss: 1.1880\n",
      "Epoch [22/40], Step [50/118], D_loss: 0.7845, G_loss: 1.6400\n",
      "Epoch [22/40], Step [51/118], D_loss: 0.2644, G_loss: 2.1429\n",
      "Epoch [22/40], Step [52/118], D_loss: 0.4956, G_loss: 1.5500\n",
      "Epoch [22/40], Step [53/118], D_loss: 0.2110, G_loss: 3.0398\n",
      "Epoch [22/40], Step [54/118], D_loss: 2.2775, G_loss: 0.6431\n",
      "Epoch [22/40], Step [55/118], D_loss: 1.1357, G_loss: 1.5797\n",
      "Epoch [22/40], Step [56/118], D_loss: 1.4465, G_loss: 0.5857\n",
      "Epoch [22/40], Step [57/118], D_loss: 1.0834, G_loss: 0.9855\n",
      "Epoch [22/40], Step [58/118], D_loss: 0.2701, G_loss: 3.0175\n",
      "Epoch [22/40], Step [59/118], D_loss: 1.1690, G_loss: 0.9973\n",
      "Epoch [22/40], Step [60/118], D_loss: 1.0779, G_loss: 0.9537\n",
      "Epoch [22/40], Step [61/118], D_loss: 0.6744, G_loss: 2.0424\n",
      "Epoch [22/40], Step [62/118], D_loss: 0.7219, G_loss: 1.2006\n",
      "Epoch [22/40], Step [63/118], D_loss: 0.7261, G_loss: 1.0472\n",
      "Epoch [22/40], Step [64/118], D_loss: 0.6628, G_loss: 1.4080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/40], Step [65/118], D_loss: 0.5485, G_loss: 1.8165\n",
      "Epoch [22/40], Step [66/118], D_loss: 0.7188, G_loss: 2.1408\n",
      "Epoch [22/40], Step [67/118], D_loss: 0.9245, G_loss: 0.9439\n",
      "Epoch [22/40], Step [68/118], D_loss: 0.3898, G_loss: 2.0897\n",
      "Epoch [22/40], Step [69/118], D_loss: 0.3883, G_loss: 1.7779\n",
      "Epoch [22/40], Step [70/118], D_loss: 0.9692, G_loss: 1.0695\n",
      "Epoch [22/40], Step [71/118], D_loss: 0.6032, G_loss: 1.6345\n",
      "Epoch [22/40], Step [72/118], D_loss: 0.6427, G_loss: 1.4082\n",
      "Epoch [22/40], Step [73/118], D_loss: 1.1097, G_loss: 1.0425\n",
      "Epoch [22/40], Step [74/118], D_loss: 0.3939, G_loss: 2.7592\n",
      "Epoch [22/40], Step [75/118], D_loss: 0.6859, G_loss: 1.1558\n",
      "Epoch [22/40], Step [76/118], D_loss: 0.9712, G_loss: 0.8990\n",
      "Epoch [22/40], Step [77/118], D_loss: 1.0696, G_loss: 0.8195\n",
      "Epoch [22/40], Step [78/118], D_loss: 0.8422, G_loss: 1.7391\n",
      "Epoch [22/40], Step [79/118], D_loss: 0.2647, G_loss: 2.8657\n",
      "Epoch [22/40], Step [80/118], D_loss: 1.7169, G_loss: 0.8006\n",
      "Epoch [22/40], Step [81/118], D_loss: 1.2556, G_loss: 0.8529\n",
      "Epoch [22/40], Step [82/118], D_loss: 0.6225, G_loss: 1.7462\n",
      "Epoch [22/40], Step [83/118], D_loss: 0.3819, G_loss: 1.8249\n",
      "Epoch [22/40], Step [84/118], D_loss: 0.9846, G_loss: 0.9468\n",
      "Epoch [22/40], Step [85/118], D_loss: 0.2479, G_loss: 2.1597\n",
      "Epoch [22/40], Step [86/118], D_loss: 0.8321, G_loss: 0.9745\n",
      "Epoch [22/40], Step [87/118], D_loss: 0.4405, G_loss: 1.5718\n",
      "Epoch [22/40], Step [88/118], D_loss: 0.4320, G_loss: 2.0950\n",
      "Epoch [22/40], Step [89/118], D_loss: 1.9402, G_loss: 0.9332\n",
      "Epoch [22/40], Step [90/118], D_loss: 1.3333, G_loss: 1.3259\n",
      "Epoch [22/40], Step [91/118], D_loss: 0.2123, G_loss: 2.6623\n",
      "Epoch [22/40], Step [92/118], D_loss: 1.1934, G_loss: 0.9804\n",
      "Epoch [22/40], Step [93/118], D_loss: 0.9388, G_loss: 1.6304\n",
      "Epoch [22/40], Step [94/118], D_loss: 1.3797, G_loss: 1.8656\n",
      "Epoch [22/40], Step [95/118], D_loss: 0.4426, G_loss: 1.2675\n",
      "Epoch [22/40], Step [96/118], D_loss: 0.5715, G_loss: 1.3480\n",
      "Epoch [22/40], Step [97/118], D_loss: 0.2231, G_loss: 2.4045\n",
      "Epoch [22/40], Step [98/118], D_loss: 1.3565, G_loss: 0.8637\n",
      "Epoch [22/40], Step [99/118], D_loss: 0.5266, G_loss: 1.4146\n",
      "Epoch [22/40], Step [100/118], D_loss: 1.9267, G_loss: 0.7212\n",
      "Epoch [22/40], Step [101/118], D_loss: 0.5720, G_loss: 2.3134\n",
      "Epoch [22/40], Step [102/118], D_loss: 0.6746, G_loss: 1.5127\n",
      "Epoch [22/40], Step [103/118], D_loss: 1.7539, G_loss: 0.7316\n",
      "Epoch [22/40], Step [104/118], D_loss: 0.3765, G_loss: 1.4060\n",
      "Epoch [22/40], Step [105/118], D_loss: 0.4316, G_loss: 1.7729\n",
      "Epoch [22/40], Step [106/118], D_loss: 0.6266, G_loss: 1.7082\n",
      "Epoch [22/40], Step [107/118], D_loss: 0.8079, G_loss: 1.3876\n",
      "Epoch [22/40], Step [108/118], D_loss: 0.4785, G_loss: 1.6895\n",
      "Epoch [22/40], Step [109/118], D_loss: 1.2451, G_loss: 0.8275\n",
      "Epoch [22/40], Step [110/118], D_loss: 0.6806, G_loss: 1.4835\n",
      "Epoch [22/40], Step [111/118], D_loss: 0.3617, G_loss: 2.6292\n",
      "Epoch [22/40], Step [112/118], D_loss: 0.9291, G_loss: 1.1024\n",
      "Epoch [22/40], Step [113/118], D_loss: 0.8929, G_loss: 0.9129\n",
      "Epoch [22/40], Step [114/118], D_loss: 0.7974, G_loss: 1.6428\n",
      "Epoch [22/40], Step [115/118], D_loss: 1.0160, G_loss: 1.9436\n",
      "Epoch [22/40], Step [116/118], D_loss: 1.4221, G_loss: 0.7587\n",
      "Epoch [22/40], Step [117/118], D_loss: 0.9732, G_loss: 0.8835\n",
      "Epoch [22/40], Step [118/118], D_loss: 2.3108, G_loss: 1.1889\n",
      "Epoch [23/40], Step [1/118], D_loss: 0.3052, G_loss: 3.3312\n",
      "Epoch [23/40], Step [2/118], D_loss: 1.6943, G_loss: 1.9107\n",
      "Epoch [23/40], Step [3/118], D_loss: 1.6357, G_loss: 0.6108\n",
      "Epoch [23/40], Step [4/118], D_loss: 1.1302, G_loss: 0.6393\n",
      "Epoch [23/40], Step [5/118], D_loss: 1.5763, G_loss: 1.0179\n",
      "Epoch [23/40], Step [6/118], D_loss: 1.0097, G_loss: 1.5298\n",
      "Epoch [23/40], Step [7/118], D_loss: 1.9186, G_loss: 0.7898\n",
      "Epoch [23/40], Step [8/118], D_loss: 0.9305, G_loss: 0.8866\n",
      "Epoch [23/40], Step [9/118], D_loss: 0.7187, G_loss: 1.2844\n",
      "Epoch [23/40], Step [10/118], D_loss: 1.2777, G_loss: 0.7787\n",
      "Epoch [23/40], Step [11/118], D_loss: 0.9084, G_loss: 1.3955\n",
      "Epoch [23/40], Step [12/118], D_loss: 1.6331, G_loss: 1.2358\n",
      "Epoch [23/40], Step [13/118], D_loss: 0.6044, G_loss: 1.0995\n",
      "Epoch [23/40], Step [14/118], D_loss: 1.2907, G_loss: 1.0774\n",
      "Epoch [23/40], Step [15/118], D_loss: 1.1268, G_loss: 1.4147\n",
      "Epoch [23/40], Step [16/118], D_loss: 0.4533, G_loss: 1.6204\n",
      "Epoch [23/40], Step [17/118], D_loss: 1.0080, G_loss: 0.8906\n",
      "Epoch [23/40], Step [18/118], D_loss: 0.3533, G_loss: 1.6919\n",
      "Epoch [23/40], Step [19/118], D_loss: 0.7122, G_loss: 1.3056\n",
      "Epoch [23/40], Step [20/118], D_loss: 0.9378, G_loss: 1.1077\n",
      "Epoch [23/40], Step [21/118], D_loss: 0.9691, G_loss: 1.4081\n",
      "Epoch [23/40], Step [22/118], D_loss: 1.6161, G_loss: 2.4022\n",
      "Epoch [23/40], Step [23/118], D_loss: 0.7788, G_loss: 0.8126\n",
      "Epoch [23/40], Step [24/118], D_loss: 1.3381, G_loss: 0.9361\n",
      "Epoch [23/40], Step [25/118], D_loss: 0.9648, G_loss: 1.7573\n",
      "Epoch [23/40], Step [26/118], D_loss: 0.8289, G_loss: 1.0686\n",
      "Epoch [23/40], Step [27/118], D_loss: 0.2261, G_loss: 2.8082\n",
      "Epoch [23/40], Step [28/118], D_loss: 0.4641, G_loss: 2.2841\n",
      "Epoch [23/40], Step [29/118], D_loss: 0.8370, G_loss: 1.2163\n",
      "Epoch [23/40], Step [30/118], D_loss: 1.2526, G_loss: 1.0795\n",
      "Epoch [23/40], Step [31/118], D_loss: 1.5135, G_loss: 0.7854\n",
      "Epoch [23/40], Step [32/118], D_loss: 1.6367, G_loss: 0.7125\n",
      "Epoch [23/40], Step [33/118], D_loss: 0.5615, G_loss: 1.9450\n",
      "Epoch [23/40], Step [34/118], D_loss: 0.5927, G_loss: 1.6804\n",
      "Epoch [23/40], Step [35/118], D_loss: 0.5671, G_loss: 2.2071\n",
      "Epoch [23/40], Step [36/118], D_loss: 1.6754, G_loss: 0.6356\n",
      "Epoch [23/40], Step [37/118], D_loss: 0.3310, G_loss: 2.4182\n",
      "Epoch [23/40], Step [38/118], D_loss: 1.3786, G_loss: 0.9767\n",
      "Epoch [23/40], Step [39/118], D_loss: 0.7553, G_loss: 1.0329\n",
      "Epoch [23/40], Step [40/118], D_loss: 0.5044, G_loss: 1.7499\n",
      "Epoch [23/40], Step [41/118], D_loss: 0.8203, G_loss: 1.5435\n",
      "Epoch [23/40], Step [42/118], D_loss: 0.3772, G_loss: 1.3896\n",
      "Epoch [23/40], Step [43/118], D_loss: 0.6276, G_loss: 1.4142\n",
      "Epoch [23/40], Step [44/118], D_loss: 1.0579, G_loss: 1.5831\n",
      "Epoch [23/40], Step [45/118], D_loss: 0.6070, G_loss: 2.2450\n",
      "Epoch [23/40], Step [46/118], D_loss: 1.1411, G_loss: 1.1139\n",
      "Epoch [23/40], Step [47/118], D_loss: 0.8224, G_loss: 0.9648\n",
      "Epoch [23/40], Step [48/118], D_loss: 0.6290, G_loss: 1.1652\n",
      "Epoch [23/40], Step [49/118], D_loss: 1.0857, G_loss: 0.9652\n",
      "Epoch [23/40], Step [50/118], D_loss: 0.9719, G_loss: 1.0484\n",
      "Epoch [23/40], Step [51/118], D_loss: 0.2187, G_loss: 2.6470\n",
      "Epoch [23/40], Step [52/118], D_loss: 0.4481, G_loss: 1.7702\n",
      "Epoch [23/40], Step [53/118], D_loss: 1.6006, G_loss: 0.6354\n",
      "Epoch [23/40], Step [54/118], D_loss: 1.0160, G_loss: 1.0229\n",
      "Epoch [23/40], Step [55/118], D_loss: 0.8520, G_loss: 1.4077\n",
      "Epoch [23/40], Step [56/118], D_loss: 0.7590, G_loss: 1.3192\n",
      "Epoch [23/40], Step [57/118], D_loss: 0.7005, G_loss: 1.5953\n",
      "Epoch [23/40], Step [58/118], D_loss: 0.5749, G_loss: 1.6397\n",
      "Epoch [23/40], Step [59/118], D_loss: 1.9894, G_loss: 0.6160\n",
      "Epoch [23/40], Step [60/118], D_loss: 0.7523, G_loss: 1.0627\n",
      "Epoch [23/40], Step [61/118], D_loss: 0.4131, G_loss: 1.8583\n",
      "Epoch [23/40], Step [62/118], D_loss: 1.4420, G_loss: 0.6499\n",
      "Epoch [23/40], Step [63/118], D_loss: 0.5975, G_loss: 1.5085\n",
      "Epoch [23/40], Step [64/118], D_loss: 0.9609, G_loss: 1.2843\n",
      "Epoch [23/40], Step [65/118], D_loss: 0.6492, G_loss: 1.5773\n",
      "Epoch [23/40], Step [66/118], D_loss: 0.3469, G_loss: 1.8699\n",
      "Epoch [23/40], Step [67/118], D_loss: 0.5326, G_loss: 1.9691\n",
      "Epoch [23/40], Step [68/118], D_loss: 0.5765, G_loss: 1.1694\n",
      "Epoch [23/40], Step [69/118], D_loss: 0.8627, G_loss: 1.2679\n",
      "Epoch [23/40], Step [70/118], D_loss: 0.7188, G_loss: 1.8610\n",
      "Epoch [23/40], Step [71/118], D_loss: 0.2065, G_loss: 3.2749\n",
      "Epoch [23/40], Step [72/118], D_loss: 0.9051, G_loss: 1.2628\n",
      "Epoch [23/40], Step [73/118], D_loss: 0.7985, G_loss: 0.9162\n",
      "Epoch [23/40], Step [74/118], D_loss: 0.9886, G_loss: 1.0556\n",
      "Epoch [23/40], Step [75/118], D_loss: 0.4993, G_loss: 1.9252\n",
      "Epoch [23/40], Step [76/118], D_loss: 1.6672, G_loss: 0.7948\n",
      "Epoch [23/40], Step [77/118], D_loss: 1.6733, G_loss: 0.7272\n",
      "Epoch [23/40], Step [78/118], D_loss: 0.8145, G_loss: 1.9875\n",
      "Epoch [23/40], Step [79/118], D_loss: 0.4378, G_loss: 1.5320\n",
      "Epoch [23/40], Step [80/118], D_loss: 0.7176, G_loss: 1.4489\n",
      "Epoch [23/40], Step [81/118], D_loss: 0.7358, G_loss: 1.9723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/40], Step [82/118], D_loss: 0.5190, G_loss: 1.4253\n",
      "Epoch [23/40], Step [83/118], D_loss: 0.4481, G_loss: 1.8239\n",
      "Epoch [23/40], Step [84/118], D_loss: 0.8908, G_loss: 1.2747\n",
      "Epoch [23/40], Step [85/118], D_loss: 0.5450, G_loss: 1.5851\n",
      "Epoch [23/40], Step [86/118], D_loss: 0.4627, G_loss: 2.5020\n",
      "Epoch [23/40], Step [87/118], D_loss: 0.8377, G_loss: 1.2474\n",
      "Epoch [23/40], Step [88/118], D_loss: 0.9766, G_loss: 1.6846\n",
      "Epoch [23/40], Step [89/118], D_loss: 1.2431, G_loss: 0.5688\n",
      "Epoch [23/40], Step [90/118], D_loss: 1.0456, G_loss: 0.8186\n",
      "Epoch [23/40], Step [91/118], D_loss: 0.5048, G_loss: 1.6503\n",
      "Epoch [23/40], Step [92/118], D_loss: 1.2208, G_loss: 0.9581\n",
      "Epoch [23/40], Step [93/118], D_loss: 1.1505, G_loss: 0.9342\n",
      "Epoch [23/40], Step [94/118], D_loss: 0.3074, G_loss: 2.1827\n",
      "Epoch [23/40], Step [95/118], D_loss: 0.4189, G_loss: 1.8511\n",
      "Epoch [23/40], Step [96/118], D_loss: 0.4537, G_loss: 1.8120\n",
      "Epoch [23/40], Step [97/118], D_loss: 1.0221, G_loss: 1.1955\n",
      "Epoch [23/40], Step [98/118], D_loss: 0.3960, G_loss: 1.5816\n",
      "Epoch [23/40], Step [99/118], D_loss: 1.2319, G_loss: 1.0448\n",
      "Epoch [23/40], Step [100/118], D_loss: 0.9031, G_loss: 1.9394\n",
      "Epoch [23/40], Step [101/118], D_loss: 2.7954, G_loss: 1.3061\n",
      "Epoch [23/40], Step [102/118], D_loss: 1.2149, G_loss: 0.6487\n",
      "Epoch [23/40], Step [103/118], D_loss: 0.4435, G_loss: 1.7851\n",
      "Epoch [23/40], Step [104/118], D_loss: 1.2148, G_loss: 1.0665\n",
      "Epoch [23/40], Step [105/118], D_loss: 0.9476, G_loss: 1.5445\n",
      "Epoch [23/40], Step [106/118], D_loss: 1.0965, G_loss: 0.6920\n",
      "Epoch [23/40], Step [107/118], D_loss: 1.9818, G_loss: 0.7726\n",
      "Epoch [23/40], Step [108/118], D_loss: 0.6945, G_loss: 2.4947\n",
      "Epoch [23/40], Step [109/118], D_loss: 0.2232, G_loss: 2.7835\n",
      "Epoch [23/40], Step [110/118], D_loss: 0.6197, G_loss: 1.2693\n",
      "Epoch [23/40], Step [111/118], D_loss: 0.5833, G_loss: 1.5338\n",
      "Epoch [23/40], Step [112/118], D_loss: 1.7312, G_loss: 0.8473\n",
      "Epoch [23/40], Step [113/118], D_loss: 0.2314, G_loss: 2.9826\n",
      "Epoch [23/40], Step [114/118], D_loss: 1.2946, G_loss: 1.6793\n",
      "Epoch [23/40], Step [115/118], D_loss: 0.6085, G_loss: 1.0095\n",
      "Epoch [23/40], Step [116/118], D_loss: 1.1308, G_loss: 0.9530\n",
      "Epoch [23/40], Step [117/118], D_loss: 0.6682, G_loss: 1.5161\n",
      "Epoch [23/40], Step [118/118], D_loss: 2.1315, G_loss: 1.0740\n",
      "Epoch [24/40], Step [1/118], D_loss: 1.4260, G_loss: 2.5112\n",
      "Epoch [24/40], Step [2/118], D_loss: 0.7438, G_loss: 1.2774\n",
      "Epoch [24/40], Step [3/118], D_loss: 0.4633, G_loss: 1.7291\n",
      "Epoch [24/40], Step [4/118], D_loss: 2.0957, G_loss: 0.6048\n",
      "Epoch [24/40], Step [5/118], D_loss: 0.7406, G_loss: 2.2139\n",
      "Epoch [24/40], Step [6/118], D_loss: 0.6381, G_loss: 2.7610\n",
      "Epoch [24/40], Step [7/118], D_loss: 0.5248, G_loss: 2.9435\n",
      "Epoch [24/40], Step [8/118], D_loss: 1.6057, G_loss: 0.5732\n",
      "Epoch [24/40], Step [9/118], D_loss: 0.6850, G_loss: 0.9371\n",
      "Epoch [24/40], Step [10/118], D_loss: 1.1436, G_loss: 0.8950\n",
      "Epoch [24/40], Step [11/118], D_loss: 0.4384, G_loss: 1.7655\n",
      "Epoch [24/40], Step [12/118], D_loss: 1.2133, G_loss: 1.4241\n",
      "Epoch [24/40], Step [13/118], D_loss: 1.1169, G_loss: 1.7093\n",
      "Epoch [24/40], Step [14/118], D_loss: 0.8065, G_loss: 1.7516\n",
      "Epoch [24/40], Step [15/118], D_loss: 1.2743, G_loss: 0.7301\n",
      "Epoch [24/40], Step [16/118], D_loss: 1.2564, G_loss: 1.1097\n",
      "Epoch [24/40], Step [17/118], D_loss: 0.4512, G_loss: 2.3208\n",
      "Epoch [24/40], Step [18/118], D_loss: 0.4370, G_loss: 1.9749\n",
      "Epoch [24/40], Step [19/118], D_loss: 0.3564, G_loss: 1.7149\n",
      "Epoch [24/40], Step [20/118], D_loss: 1.3275, G_loss: 0.8562\n",
      "Epoch [24/40], Step [21/118], D_loss: 1.0285, G_loss: 1.0321\n",
      "Epoch [24/40], Step [22/118], D_loss: 1.7278, G_loss: 0.5027\n",
      "Epoch [24/40], Step [23/118], D_loss: 0.6922, G_loss: 1.3456\n",
      "Epoch [24/40], Step [24/118], D_loss: 0.8090, G_loss: 1.6870\n",
      "Epoch [24/40], Step [25/118], D_loss: 0.3849, G_loss: 1.6586\n",
      "Epoch [24/40], Step [26/118], D_loss: 0.7481, G_loss: 1.2878\n",
      "Epoch [24/40], Step [27/118], D_loss: 0.9390, G_loss: 1.2646\n",
      "Epoch [24/40], Step [28/118], D_loss: 1.2493, G_loss: 1.1220\n",
      "Epoch [24/40], Step [29/118], D_loss: 1.0124, G_loss: 0.7967\n",
      "Epoch [24/40], Step [30/118], D_loss: 0.8480, G_loss: 1.3276\n",
      "Epoch [24/40], Step [31/118], D_loss: 0.4091, G_loss: 2.4620\n",
      "Epoch [24/40], Step [32/118], D_loss: 0.7639, G_loss: 1.3065\n",
      "Epoch [24/40], Step [33/118], D_loss: 1.1050, G_loss: 0.8428\n",
      "Epoch [24/40], Step [34/118], D_loss: 0.3269, G_loss: 1.9760\n",
      "Epoch [24/40], Step [35/118], D_loss: 0.7594, G_loss: 1.6336\n",
      "Epoch [24/40], Step [36/118], D_loss: 0.5730, G_loss: 1.2172\n",
      "Epoch [24/40], Step [37/118], D_loss: 1.4002, G_loss: 0.9806\n",
      "Epoch [24/40], Step [38/118], D_loss: 0.8399, G_loss: 1.5003\n",
      "Epoch [24/40], Step [39/118], D_loss: 0.9289, G_loss: 1.2227\n",
      "Epoch [24/40], Step [40/118], D_loss: 0.7164, G_loss: 1.1530\n",
      "Epoch [24/40], Step [41/118], D_loss: 0.7552, G_loss: 1.6785\n",
      "Epoch [24/40], Step [42/118], D_loss: 1.1813, G_loss: 1.3190\n",
      "Epoch [24/40], Step [43/118], D_loss: 0.6007, G_loss: 1.2396\n",
      "Epoch [24/40], Step [44/118], D_loss: 1.0559, G_loss: 1.0167\n",
      "Epoch [24/40], Step [45/118], D_loss: 0.4614, G_loss: 1.9215\n",
      "Epoch [24/40], Step [46/118], D_loss: 0.9907, G_loss: 1.5041\n",
      "Epoch [24/40], Step [47/118], D_loss: 0.9639, G_loss: 0.6738\n",
      "Epoch [24/40], Step [48/118], D_loss: 0.8041, G_loss: 1.2141\n",
      "Epoch [24/40], Step [49/118], D_loss: 0.3498, G_loss: 2.1556\n",
      "Epoch [24/40], Step [50/118], D_loss: 0.6929, G_loss: 1.5965\n",
      "Epoch [24/40], Step [51/118], D_loss: 1.5052, G_loss: 0.6009\n",
      "Epoch [24/40], Step [52/118], D_loss: 0.5386, G_loss: 1.2827\n",
      "Epoch [24/40], Step [53/118], D_loss: 0.4363, G_loss: 1.8621\n",
      "Epoch [24/40], Step [54/118], D_loss: 1.1316, G_loss: 1.3612\n",
      "Epoch [24/40], Step [55/118], D_loss: 2.1645, G_loss: 0.7660\n",
      "Epoch [24/40], Step [56/118], D_loss: 0.4871, G_loss: 1.6719\n",
      "Epoch [24/40], Step [57/118], D_loss: 1.6360, G_loss: 1.4769\n",
      "Epoch [24/40], Step [58/118], D_loss: 0.6714, G_loss: 0.8983\n",
      "Epoch [24/40], Step [59/118], D_loss: 1.7127, G_loss: 1.2320\n",
      "Epoch [24/40], Step [60/118], D_loss: 0.8990, G_loss: 2.8301\n",
      "Epoch [24/40], Step [61/118], D_loss: 0.6917, G_loss: 1.3000\n",
      "Epoch [24/40], Step [62/118], D_loss: 0.3824, G_loss: 2.2300\n",
      "Epoch [24/40], Step [63/118], D_loss: 0.6812, G_loss: 1.3911\n",
      "Epoch [24/40], Step [64/118], D_loss: 0.6567, G_loss: 1.1399\n",
      "Epoch [24/40], Step [65/118], D_loss: 1.0525, G_loss: 0.7608\n",
      "Epoch [24/40], Step [66/118], D_loss: 0.4640, G_loss: 1.4879\n",
      "Epoch [24/40], Step [67/118], D_loss: 0.7774, G_loss: 1.3657\n",
      "Epoch [24/40], Step [68/118], D_loss: 0.8176, G_loss: 1.5286\n",
      "Epoch [24/40], Step [69/118], D_loss: 0.6982, G_loss: 1.7213\n",
      "Epoch [24/40], Step [70/118], D_loss: 1.0900, G_loss: 1.1498\n",
      "Epoch [24/40], Step [71/118], D_loss: 0.5519, G_loss: 1.2588\n",
      "Epoch [24/40], Step [72/118], D_loss: 0.5021, G_loss: 1.4776\n",
      "Epoch [24/40], Step [73/118], D_loss: 0.4881, G_loss: 1.4813\n",
      "Epoch [24/40], Step [74/118], D_loss: 1.4178, G_loss: 0.8515\n",
      "Epoch [24/40], Step [75/118], D_loss: 0.2854, G_loss: 2.1437\n",
      "Epoch [24/40], Step [76/118], D_loss: 1.2387, G_loss: 0.9231\n",
      "Epoch [24/40], Step [77/118], D_loss: 0.6301, G_loss: 1.6268\n",
      "Epoch [24/40], Step [78/118], D_loss: 0.4790, G_loss: 1.9998\n",
      "Epoch [24/40], Step [79/118], D_loss: 0.2783, G_loss: 2.3478\n",
      "Epoch [24/40], Step [80/118], D_loss: 1.0358, G_loss: 0.9280\n",
      "Epoch [24/40], Step [81/118], D_loss: 0.7729, G_loss: 1.1370\n",
      "Epoch [24/40], Step [82/118], D_loss: 0.6072, G_loss: 1.3773\n",
      "Epoch [24/40], Step [83/118], D_loss: 3.1542, G_loss: 0.2842\n",
      "Epoch [24/40], Step [84/118], D_loss: 0.5093, G_loss: 1.3484\n",
      "Epoch [24/40], Step [85/118], D_loss: 0.4754, G_loss: 1.6048\n",
      "Epoch [24/40], Step [86/118], D_loss: 0.9502, G_loss: 1.2506\n",
      "Epoch [24/40], Step [87/118], D_loss: 0.4093, G_loss: 2.3036\n",
      "Epoch [24/40], Step [88/118], D_loss: 1.6078, G_loss: 0.8546\n",
      "Epoch [24/40], Step [89/118], D_loss: 0.3637, G_loss: 2.4106\n",
      "Epoch [24/40], Step [90/118], D_loss: 1.6975, G_loss: 0.6771\n",
      "Epoch [24/40], Step [91/118], D_loss: 0.6158, G_loss: 0.9353\n",
      "Epoch [24/40], Step [92/118], D_loss: 1.0777, G_loss: 0.7216\n",
      "Epoch [24/40], Step [93/118], D_loss: 1.8208, G_loss: 1.0606\n",
      "Epoch [24/40], Step [94/118], D_loss: 0.8421, G_loss: 2.0134\n",
      "Epoch [24/40], Step [95/118], D_loss: 0.4557, G_loss: 3.7922\n",
      "Epoch [24/40], Step [96/118], D_loss: 0.9471, G_loss: 2.6892\n",
      "Epoch [24/40], Step [97/118], D_loss: 0.2865, G_loss: 1.3164\n",
      "Epoch [24/40], Step [98/118], D_loss: 1.0193, G_loss: 1.1759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/40], Step [99/118], D_loss: 0.6028, G_loss: 2.1274\n",
      "Epoch [24/40], Step [100/118], D_loss: 1.3011, G_loss: 1.8305\n",
      "Epoch [24/40], Step [101/118], D_loss: 2.0159, G_loss: 0.3363\n",
      "Epoch [24/40], Step [102/118], D_loss: 1.6703, G_loss: 0.5371\n",
      "Epoch [24/40], Step [103/118], D_loss: 0.3787, G_loss: 2.1440\n",
      "Epoch [24/40], Step [104/118], D_loss: 0.7372, G_loss: 1.7942\n",
      "Epoch [24/40], Step [105/118], D_loss: 1.7121, G_loss: 1.1318\n",
      "Epoch [24/40], Step [106/118], D_loss: 1.7845, G_loss: 0.4085\n",
      "Epoch [24/40], Step [107/118], D_loss: 1.0769, G_loss: 1.0926\n",
      "Epoch [24/40], Step [108/118], D_loss: 0.6926, G_loss: 1.8868\n",
      "Epoch [24/40], Step [109/118], D_loss: 0.4033, G_loss: 1.6148\n",
      "Epoch [24/40], Step [110/118], D_loss: 1.7561, G_loss: 0.8449\n",
      "Epoch [24/40], Step [111/118], D_loss: 0.8864, G_loss: 2.2844\n",
      "Epoch [24/40], Step [112/118], D_loss: 0.4102, G_loss: 2.3076\n",
      "Epoch [24/40], Step [113/118], D_loss: 0.5816, G_loss: 1.1297\n",
      "Epoch [24/40], Step [114/118], D_loss: 0.4266, G_loss: 1.7326\n",
      "Epoch [24/40], Step [115/118], D_loss: 0.3925, G_loss: 2.3849\n",
      "Epoch [24/40], Step [116/118], D_loss: 0.6022, G_loss: 1.6363\n",
      "Epoch [24/40], Step [117/118], D_loss: 2.0101, G_loss: 1.6189\n",
      "Epoch [24/40], Step [118/118], D_loss: 2.6027, G_loss: 0.4245\n",
      "Epoch [25/40], Step [1/118], D_loss: 0.9740, G_loss: 1.7032\n",
      "Epoch [25/40], Step [2/118], D_loss: 0.7513, G_loss: 1.3619\n",
      "Epoch [25/40], Step [3/118], D_loss: 1.1542, G_loss: 0.8758\n",
      "Epoch [25/40], Step [4/118], D_loss: 0.9169, G_loss: 1.4164\n",
      "Epoch [25/40], Step [5/118], D_loss: 0.8173, G_loss: 1.3032\n",
      "Epoch [25/40], Step [6/118], D_loss: 1.1709, G_loss: 1.3492\n",
      "Epoch [25/40], Step [7/118], D_loss: 0.7724, G_loss: 2.7904\n",
      "Epoch [25/40], Step [8/118], D_loss: 0.4016, G_loss: 2.1313\n",
      "Epoch [25/40], Step [9/118], D_loss: 0.8478, G_loss: 0.9795\n",
      "Epoch [25/40], Step [10/118], D_loss: 2.0354, G_loss: 0.6350\n",
      "Epoch [25/40], Step [11/118], D_loss: 0.9308, G_loss: 1.6998\n",
      "Epoch [25/40], Step [12/118], D_loss: 0.5312, G_loss: 2.4112\n",
      "Epoch [25/40], Step [13/118], D_loss: 1.0100, G_loss: 1.0694\n",
      "Epoch [25/40], Step [14/118], D_loss: 1.2088, G_loss: 1.0144\n",
      "Epoch [25/40], Step [15/118], D_loss: 0.2063, G_loss: 2.7251\n",
      "Epoch [25/40], Step [16/118], D_loss: 0.7670, G_loss: 1.9832\n",
      "Epoch [25/40], Step [17/118], D_loss: 0.3745, G_loss: 1.7004\n",
      "Epoch [25/40], Step [18/118], D_loss: 1.6274, G_loss: 0.8544\n",
      "Epoch [25/40], Step [19/118], D_loss: 0.8497, G_loss: 1.6958\n",
      "Epoch [25/40], Step [20/118], D_loss: 0.6591, G_loss: 2.3697\n",
      "Epoch [25/40], Step [21/118], D_loss: 1.1565, G_loss: 0.7252\n",
      "Epoch [25/40], Step [22/118], D_loss: 0.3825, G_loss: 1.4422\n",
      "Epoch [25/40], Step [23/118], D_loss: 0.3689, G_loss: 1.9385\n",
      "Epoch [25/40], Step [24/118], D_loss: 0.5388, G_loss: 1.8640\n",
      "Epoch [25/40], Step [25/118], D_loss: 0.9634, G_loss: 1.4468\n",
      "Epoch [25/40], Step [26/118], D_loss: 0.7263, G_loss: 1.7108\n",
      "Epoch [25/40], Step [27/118], D_loss: 2.2001, G_loss: 0.3715\n",
      "Epoch [25/40], Step [28/118], D_loss: 1.3721, G_loss: 0.5928\n",
      "Epoch [25/40], Step [29/118], D_loss: 0.8411, G_loss: 1.1862\n",
      "Epoch [25/40], Step [30/118], D_loss: 1.0901, G_loss: 1.4257\n",
      "Epoch [25/40], Step [31/118], D_loss: 0.4231, G_loss: 2.5232\n",
      "Epoch [25/40], Step [32/118], D_loss: 0.2736, G_loss: 2.3007\n",
      "Epoch [25/40], Step [33/118], D_loss: 0.7720, G_loss: 1.4149\n",
      "Epoch [25/40], Step [34/118], D_loss: 0.2863, G_loss: 1.8313\n",
      "Epoch [25/40], Step [35/118], D_loss: 2.1066, G_loss: 0.4200\n",
      "Epoch [25/40], Step [36/118], D_loss: 0.2053, G_loss: 1.8731\n",
      "Epoch [25/40], Step [37/118], D_loss: 1.2784, G_loss: 1.1903\n",
      "Epoch [25/40], Step [38/118], D_loss: 0.4441, G_loss: 2.4047\n",
      "Epoch [25/40], Step [39/118], D_loss: 1.5398, G_loss: 0.8058\n",
      "Epoch [25/40], Step [40/118], D_loss: 0.5144, G_loss: 1.2817\n",
      "Epoch [25/40], Step [41/118], D_loss: 0.8741, G_loss: 1.0964\n",
      "Epoch [25/40], Step [42/118], D_loss: 0.4850, G_loss: 1.4209\n",
      "Epoch [25/40], Step [43/118], D_loss: 0.9043, G_loss: 1.4401\n",
      "Epoch [25/40], Step [44/118], D_loss: 0.8212, G_loss: 2.5005\n",
      "Epoch [25/40], Step [45/118], D_loss: 0.6047, G_loss: 1.2294\n",
      "Epoch [25/40], Step [46/118], D_loss: 0.3101, G_loss: 2.0296\n",
      "Epoch [25/40], Step [47/118], D_loss: 0.5628, G_loss: 1.7451\n",
      "Epoch [25/40], Step [48/118], D_loss: 0.5478, G_loss: 1.5170\n",
      "Epoch [25/40], Step [49/118], D_loss: 0.2592, G_loss: 2.8314\n",
      "Epoch [25/40], Step [50/118], D_loss: 1.7032, G_loss: 0.5514\n",
      "Epoch [25/40], Step [51/118], D_loss: 0.8187, G_loss: 1.0422\n",
      "Epoch [25/40], Step [52/118], D_loss: 1.7989, G_loss: 0.8291\n",
      "Epoch [25/40], Step [53/118], D_loss: 1.8330, G_loss: 1.8080\n",
      "Epoch [25/40], Step [54/118], D_loss: 0.6992, G_loss: 1.0012\n",
      "Epoch [25/40], Step [55/118], D_loss: 0.9858, G_loss: 1.2656\n",
      "Epoch [25/40], Step [56/118], D_loss: 1.5576, G_loss: 0.7977\n",
      "Epoch [25/40], Step [57/118], D_loss: 0.6772, G_loss: 1.9722\n",
      "Epoch [25/40], Step [58/118], D_loss: 1.2951, G_loss: 1.1641\n",
      "Epoch [25/40], Step [59/118], D_loss: 0.5082, G_loss: 2.2014\n",
      "Epoch [25/40], Step [60/118], D_loss: 0.6707, G_loss: 1.1342\n",
      "Epoch [25/40], Step [61/118], D_loss: 0.9621, G_loss: 0.9594\n",
      "Epoch [25/40], Step [62/118], D_loss: 0.4687, G_loss: 1.9708\n",
      "Epoch [25/40], Step [63/118], D_loss: 0.9389, G_loss: 1.3945\n",
      "Epoch [25/40], Step [64/118], D_loss: 1.1206, G_loss: 1.0290\n",
      "Epoch [25/40], Step [65/118], D_loss: 1.1208, G_loss: 0.7495\n",
      "Epoch [25/40], Step [66/118], D_loss: 2.9189, G_loss: 0.2610\n",
      "Epoch [25/40], Step [67/118], D_loss: 0.6481, G_loss: 1.9763\n",
      "Epoch [25/40], Step [68/118], D_loss: 0.4499, G_loss: 2.0976\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4fc5c063c2a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mc_fill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mD_fake_decision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_fill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mG_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_fake_decision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    431\u001b[0m         return F.binary_cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m    432\u001b[0m                                       \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                                       reduce=self.reduce)\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1483\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# num_test_samples = 10*10\n",
    "temp_noise = torch.randn(label_dim, G_input_dim)\n",
    "fixed_noise = temp_noise\n",
    "fixed_c = torch.zeros(label_dim, 1)\n",
    "for i in range(9):\n",
    "    fixed_noise = torch.cat([fixed_noise, temp_noise], 0)\n",
    "    temp = torch.ones(label_dim, 1) + i\n",
    "    fixed_c = torch.cat([fixed_c, temp], 0)\n",
    "\n",
    "fixed_noise = fixed_noise.view(-1, G_input_dim, 1, 1)\n",
    "fixed_label = torch.zeros(G_input_dim, label_dim)\n",
    "fixed_label.scatter_(1, fixed_c.type(torch.LongTensor), 1)\n",
    "fixed_label = fixed_label.view(-1, label_dim, 1, 1)\n",
    "\n",
    "# label preprocess\n",
    "onehot = torch.zeros(label_dim, label_dim)\n",
    "onehot = onehot.scatter_(1, torch.LongTensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).view(label_dim, 1), 1).view(label_dim, label_dim, 1, 1)\n",
    "fill = torch.zeros([label_dim, label_dim, img_size, img_size])\n",
    "for i in range(label_dim):\n",
    "    fill[i, i, :, :] = 1\n",
    "\n",
    "D_avg_losses = []\n",
    "G_avg_losses = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs +1):\n",
    "    D_losses = []\n",
    "    G_losses = []\n",
    "    epoch_start_time = time.time()\n",
    "    generator.train()\n",
    "    if epoch == 5 or epoch == 10:\n",
    "        opt_Gen.param_groups[0]['lr'] /=2\n",
    "        opt_Disc.param_groups[0]['lr'] /=2\n",
    "        \n",
    "    \n",
    "    for i, (real_images,real_labels) in enumerate(data_loader):\n",
    "        minibatch = real_images.size()[0]\n",
    "        real_images = Variable(real_images.cuda())\n",
    "        \n",
    "        #labels\n",
    "        y_real = Variable(torch.ones(minibatch).cuda())\n",
    "        y_fake = Variable(torch.zeros(minibatch).cuda())\n",
    "        c_fill = Variable(fill[real_labels].cuda())\n",
    "        z_ = torch.randn(minibatch, G_input_dim).view(-1, G_input_dim, 1, 1)\n",
    "        z_ = Variable(z_.cuda())\n",
    "        ## Train Discriminator             \n",
    "        # first with real data\n",
    "        \n",
    "        D_real_decision = discriminator(real_images, c_fill).squeeze()\n",
    "        D_real_loss = criterion(D_real_decision, y_real)\n",
    "        \n",
    "        # Then with fake data\n",
    "        \n",
    "        c_ = (torch.rand(minibatch, 1) * label_dim).type(torch.LongTensor).squeeze()\n",
    "        c_onehot_ = Variable(onehot[c_].cuda())\n",
    "        c_fill_ = Variable(fill[c_].cuda())\n",
    "        \n",
    "        generator_image = generator(z_, c_onehot_) \n",
    "        D_fake_decision = discriminator(generator_image,c_fill_).squeeze()\n",
    "        D_fake_loss = criterion(D_fake_decision, y_fake)\n",
    "        \n",
    "        # Optimization\n",
    "        discriminator.zero_grad()\n",
    "        D_loss = D_fake_loss + D_real_loss\n",
    "        D_loss.backward()\n",
    "        opt_Disc.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        \n",
    "        generator_image = generator(z_, c_onehot_)\n",
    "        c_fill = Variable(fill[c_].cuda())\n",
    "        D_fake_decision = discriminator(generator_image,c_fill).squeeze()\n",
    "        G_loss = criterion(D_fake_decision, y_real)\n",
    "        \n",
    "        # Optimization\n",
    "        generator.zero_grad()\n",
    "        G_loss.backward()\n",
    "        opt_Gen.step()\n",
    "        \n",
    "        D_losses.append(D_loss.data[0])\n",
    "        G_losses.append(G_loss.data[0])\n",
    "        \n",
    "        print('Epoch [%d/%d], Step [%d/%d], D_loss: %.4f, G_loss: %.4f'\n",
    "            % (epoch+1, num_epochs, i+1, len(data_loader), D_loss.data[0], G_loss.data[0]))\n",
    "        \n",
    "    torch.save(generator.state_dict(), '/home/abdullah/Documents/ai/models/Own implementation of Gan/Conditional Gan/model_weights/generator_param.pkl')\n",
    "    torch.save(discriminator.state_dict(), '/home/abdullah/Documents/ai/models/Own implementation of Gan/Conditional Gan/model_weights/discriminator_param.pkl')\n",
    "    \n",
    "    D_avg_loss = torch.mean(torch.FloatTensor(D_losses))\n",
    "    G_avg_loss = torch.mean(torch.FloatTensor(G_losses))\n",
    "\n",
    "    # avg loss values for plot\n",
    "    D_avg_losses.append(D_avg_loss)\n",
    "    G_avg_losses.append(G_avg_loss)\n",
    "\n",
    "    plot_loss(D_avg_losses, G_avg_losses, epoch, save=True)\n",
    "\n",
    "    # Show result for fixed noise\n",
    "    plot_result(generator, fixed_noise, fixed_label, epoch, save=True)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make gif\n",
    "loss_plots = []\n",
    "gen_image_plots = []\n",
    "for epoch in range(2,24):\n",
    "    # plot for generating gif\n",
    "    save_fn1 = 'Model resultsMNIST_cDCGAN_losses_epoch_{:d}'.format(epoch + 1) + '.png'\n",
    "    loss_plots.append(imageio.imread(save_fn1))\n",
    "\n",
    "    save_fn2 = 'Model resultsMNIST_cDCGAN_epoch_{:d}'.format(epoch + 1) + '.png'\n",
    "    gen_image_plots.append(imageio.imread(save_fn2))\n",
    "\n",
    "imageio.mimsave('MNIST_cDCGAN_losses_epochs_{:d}'.format(num_epochs) + '.gif', loss_plots, fps=5)\n",
    "imageio.mimsave('MNIST_cDCGAN_epochs_{:d}'.format(num_epochs) + '.gif', gen_image_plots, fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
