{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as to\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.datasets as data\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configuration:\n",
    "    learning_rate = 0.0002\n",
    "    noise_dim = 100 #Input Noise dimension\n",
    "    img_size = 64\n",
    "    img_c = 1 #Input Image channels\n",
    "    gen_c = 128 #generator channels\n",
    "    disc_c = 128 #Discriminator channels\n",
    "    beta = .5\n",
    "    batch_size = 512\n",
    "    number_epochs = 100\n",
    "    workers = 0 # Number of processors to be used by program\n",
    "    gpu = True\n",
    "    \n",
    "opt = Configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([transforms.Resize(opt.img_size)\n",
    "                                ,transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "datasets= data.MNIST(root='mnist/',train = True , transform = transforms , download = True)\n",
    "dataloader = to.utils.data.DataLoader(datasets,opt.batch_size,shuffle = True, num_workers = opt.workers)\n",
    "\n",
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = nn.Sequential(\n",
    "    nn.ConvTranspose2d(100,opt.gen_c*8,4,1,0),\n",
    "    nn.BatchNorm2d(opt.gen_c*8),\n",
    "    nn.ReLU(True),\n",
    "    \n",
    "    nn.ConvTranspose2d(opt.gen_c*8,opt.gen_c*4,4,2,1),\n",
    "    nn.BatchNorm2d(opt.gen_c*4),\n",
    "    nn.ReLU(True),\n",
    "    \n",
    "    nn.ConvTranspose2d(opt.gen_c*4,opt.gen_c*2,4,2,1),\n",
    "    nn.BatchNorm2d(opt.gen_c*2),\n",
    "    nn.ReLU(True),\n",
    "    \n",
    "    nn.ConvTranspose2d(opt.gen_c*2,opt.gen_c,4,2,1),\n",
    "    nn.BatchNorm2d(opt.gen_c),\n",
    "    nn.ReLU(True),\n",
    "    \n",
    "    nn.ConvTranspose2d(opt.gen_c,1,4,2,1),\n",
    "    nn.Tanh()\n",
    "    )\n",
    "\n",
    "# Discriminator\n",
    "discriminator = nn.Sequential(\n",
    "    nn.Conv2d(1,opt.disc_c,4,2,1),\n",
    "    nn.LeakyReLU(0.2,inplace = True),\n",
    "    \n",
    "    nn.Conv2d(opt.disc_c,opt.disc_c*2,4,2,1),\n",
    "    nn.BatchNorm2d(opt.gen_c*2),\n",
    "    nn.LeakyReLU(0.2,inplace = True),\n",
    "    \n",
    "    nn.Conv2d(opt.disc_c*2,opt.disc_c*4,4,2,1),\n",
    "    nn.BatchNorm2d(opt.gen_c*4),\n",
    "    nn.LeakyReLU(0.2,inplace = True),\n",
    "    \n",
    "    nn.Conv2d(opt.disc_c*4,opt.disc_c*8,4,2,1),\n",
    "    nn.BatchNorm2d(opt.gen_c*8),\n",
    "    nn.LeakyReLU(0.2,inplace = True),\n",
    "    \n",
    "    nn.Conv2d(opt.disc_c*8,1,4,1,0)\n",
    ")\n",
    "\n",
    "def weight_init(m):\n",
    "    class_name=m.__class__.__name__\n",
    "    if class_name.find('Conv')!=-1:\n",
    "        m.weight.data.normal_(0,0.02)\n",
    "    elif class_name.find('Norm')!=-1:\n",
    "        m.weight.data.normal_(1.0,0.02)\n",
    "\n",
    "\n",
    "generator.apply(weight_init)\n",
    "discriminator.apply(weight_init)\n",
    "generator.cuda()\n",
    "discriminator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(d_losses, g_losses, num_epoch, save=False, save_dir='MNIST_DCGAN_results/', show=False):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0, num_epoch)\n",
    "    ax.set_ylim(0, max(np.max(g_losses), np.max(d_losses))*1.1)\n",
    "    plt.xlabel('Epoch {0}'.format(num_epoch + 1))\n",
    "    plt.ylabel('Loss values')\n",
    "    plt.plot(d_losses, label='Discriminator')\n",
    "    plt.plot(g_losses, label='Generator')\n",
    "    plt.legend()\n",
    "\n",
    "    # save figure\n",
    "    if save:\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        save_fn = save_dir + 'MNIST_DCGAN_losses_epoch_{:d}'.format(num_epoch + 1) + '.png'\n",
    "        plt.savefig(save_fn)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "def plot_result(generator, noise, num_epoch, save=False, save_dir='MNIST_DCGAN_results/', show=False, fig_size=(5, 5)):\n",
    "    generator.eval()\n",
    "\n",
    "    noise = Variable(noise.cuda())\n",
    "    gen_image = generator(noise)\n",
    "    gen_image = denorm(gen_image)\n",
    "\n",
    "    generator.train()\n",
    "\n",
    "    n_rows = np.sqrt(noise.size()[0]).astype(np.int32)\n",
    "    n_cols = np.sqrt(noise.size()[0]).astype(np.int32)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=fig_size)\n",
    "    for ax, img in zip(axes.flatten(), gen_image):\n",
    "        ax.axis('off')\n",
    "        ax.set_adjustable('box-forced')\n",
    "        ax.imshow(img.cpu().data.view(opt.img_size, opt.img_size).numpy(), cmap='gray', aspect='equal')\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    title = 'Epoch {0}'.format(num_epoch+1)\n",
    "    fig.text(0.5, 0.04, title, ha='center')\n",
    "\n",
    "    # save figure\n",
    "    if save:\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        save_fn = save_dir + 'MNIST_DCGAN_epoch_{:d}'.format(num_epoch+1) + '.png'\n",
    "        plt.savefig(save_fn)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "opt_Gen = Adam(generator.parameters(),lr = opt.learning_rate, betas = (0.5, 0.999))\n",
    "opt_Disc = Adam(discriminator.parameters(),lr = opt.learning_rate, betas = (0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D_avg_losses = []\n",
    "G_avg_losses = []\n",
    "num_test_samples = 5*5\n",
    "fixed_noise = to.randn(num_test_samples, 100).view(-1,100, 1, 1)\n",
    "for epoch in range(opt.number_epochs):\n",
    "    D_losses = []\n",
    "    G_losses = []\n",
    "    for i,(images, labels) in enumerate(dataloader):\n",
    "        minibatch = images.size()[0]\n",
    "        real_images = Variable(images.cuda()) \n",
    "        real_labels = Variable(to.ones(minibatch).cuda())\n",
    "        fake_labels = Variable(to.zeros(minibatch).cuda())\n",
    "        ##Train discriminator\n",
    "        #First with real data \n",
    "        D_real_Decision = discriminator(real_images).squeeze()   \n",
    "        D_real_loss = criterion(D_real_Decision,real_labels)        \n",
    "        #with fake data        \n",
    "        z_ = to.randn(minibatch,100 ).view(-1, 100, 1, 1)\n",
    "        z_ = Variable(z_.cuda())\n",
    "        gen_images = generator(z_)        \n",
    "        D_fake_decision = discriminator(gen_images).squeeze()\n",
    "        D_fake_loss = criterion(D_fake_decision,fake_labels)\n",
    "        \n",
    "        ## back propagation\n",
    "        \n",
    "        D_loss = D_real_loss + D_fake_loss\n",
    "        discriminator.zero_grad()\n",
    "        D_loss.backward()\n",
    "        opt_Disc.step()\n",
    "        \n",
    "        # train generator\n",
    "        z_ = to.randn(minibatch,100 ).view(-1, 100, 1, 1)\n",
    "        z_ = Variable(z_.cuda())\n",
    "        gen_images = generator(z_)\n",
    "        \n",
    "        D_fake_decisions = discriminator(gen_images).squeeze()\n",
    "        G_loss = criterion(D_fake_decisions,real_labels)\n",
    "        \n",
    "        discriminator.zero_grad()\n",
    "        generator.zero_grad()\n",
    "        G_loss.backward()\n",
    "        opt_Gen.step()\n",
    "        \n",
    "        #loss values\n",
    "        D_losses.append(D_loss.data[0])\n",
    "        G_losses.append(G_loss.data[0])\n",
    "        \n",
    "        print('Epoch [%d/%d], Step [%d/%d], D_loss: %.4f, G_loss: %.4f'\n",
    "              % (epoch+1, opt.number_epochs, i+1, len(dataloader), D_loss.data[0], G_loss.data[0]))\n",
    "\n",
    "    \n",
    "    D_avg_loss = to.mean(to.FloatTensor(D_losses))\n",
    "    G_avg_loss = to.mean(to.FloatTensor(G_losses))\n",
    "    D_avg_losses.append(D_avg_loss)\n",
    "    G_avg_losses.append(G_avg_loss)\n",
    "    \n",
    "    plot_loss(D_avg_losses, G_avg_losses, epoch, save=True)\n",
    "    \n",
    "    plot_result(generator, fixed_noise, epoch, save=True, fig_size=(5, 5))\n",
    "\n",
    "    \n",
    "    \n",
    "to.save(discriminator.state_dict(),'epoch_wnet_discriminator.pth')\n",
    "to.save(generator.state_dict(),'epoch_wnet_generator.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
